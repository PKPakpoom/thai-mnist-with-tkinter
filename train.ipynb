{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def leaky_ReLU(x):\n",
    "    return np.maximum(x, 0.1 * x)\n",
    "\n",
    "def leaky_ReLU_derivative(x):\n",
    "    return np.where(x > 0, 1, 0.1)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Prevent overflow\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# Mean Squared Error Loss\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.shape[0]\n",
    "\n",
    "def categorical_crossentropy(y, y_pred):\n",
    "    return np.sum(-np.log(y_pred) * y)\n",
    "\n",
    "def categorical_crossentropy_derivative(y_true, y_pred):\n",
    "    return y_pred - y_true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image_path):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    binary_image = image_array < 127\n",
    "    \n",
    "    coords = np.column_stack(np.where(binary_image))\n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    \n",
    "    cropped = image.crop((x_min, y_min, x_max + 1, y_max + 1))\n",
    "    cropped = cropped.resize((28, 28))\n",
    "\n",
    "    return cropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "0    1100\n",
      "1    1048\n",
      "2     966\n",
      "3     910\n",
      "4     809\n",
      "5     790\n",
      "6     718\n",
      "7     691\n",
      "9     612\n",
      "8     611\n",
      "Name: count, dtype: int64\n",
      "Train set size: (4880, 2)\n",
      "Test set size: (1230, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load label mapping\n",
    "label_map = pd.read_csv('./datasets/mnist.train.map.csv')\n",
    "\n",
    "# Count samples per category\n",
    "count = label_map[\"category\"].value_counts()\n",
    "min_samples = count.min()  # Ensure equal distribution\n",
    "print(count)\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "# Manually split each category\n",
    "for label in label_map[\"category\"].unique():\n",
    "    subset = label_map[label_map[\"category\"] == label]\n",
    "    \n",
    "    # Downsample to min_samples to ensure balance\n",
    "    subset = subset.sample(n=min_samples, random_state=42)\n",
    "\n",
    "    # Shuffle before splitting\n",
    "    subset = subset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Calculate split point (80% train, 20% test)\n",
    "    split_idx = int(len(subset) * 0.8)\n",
    "    \n",
    "    train_list.append(subset[:split_idx])  # First 80% for training\n",
    "    test_list.append(subset[split_idx:])   # Last 20% for testing\n",
    "\n",
    "# Merge data back together\n",
    "train_map = pd.concat(train_list).reset_index(drop=True)\n",
    "test_map = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "# **Shuffle the entire train & test datasets**\n",
    "train_map = train_map.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_map = test_map.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Read images into DataFrame\n",
    "train_datasets = train_map.copy()\n",
    "train_datasets[\"id\"] = train_datasets[\"id\"].apply(lambda x: f\"datasets/train/{x}\")\n",
    "train_datasets.rename(columns={\"id\": \"image_path\", \"category\": \"label\"}, inplace=True)\n",
    "\n",
    "test_datasets = test_map.copy()\n",
    "test_datasets[\"id\"] = test_datasets[\"id\"].apply(lambda x: f\"datasets/train/{x}\")\n",
    "test_datasets.rename(columns={\"id\": \"image_path\", \"category\": \"label\"}, inplace=True)\n",
    "\n",
    "print(\"Train set size:\", train_datasets.shape)\n",
    "print(\"Test set size:\", test_datasets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "3    488\n",
      "1    488\n",
      "8    488\n",
      "9    488\n",
      "5    488\n",
      "7    488\n",
      "2    488\n",
      "4    488\n",
      "6    488\n",
      "0    488\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "9    123\n",
      "6    123\n",
      "1    123\n",
      "7    123\n",
      "5    123\n",
      "3    123\n",
      "8    123\n",
      "4    123\n",
      "0    123\n",
      "2    123\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count label from train datasets\n",
    "train_count = train_datasets[\"label\"].value_counts()\n",
    "print(train_count)\n",
    "\n",
    "# count label from test datasets\n",
    "test_count = test_datasets[\"label\"].value_counts()\n",
    "print(test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model size\n",
    " - 784 Input which is 28x28 pixels\n",
    " - 4 Hidden layers each layer contain 64, 32, 32, 16 nodes in order\n",
    " - Output split out 0-9\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_layer_size = 784, hidden_layers = [64, 32, 32, 16], output_layer_size = 10):\n",
    "        np.random.seed(1234)\n",
    "        self.input_layer_size = input_layer_size  # 28 * 28 pixels\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.activation_function = leaky_ReLU\n",
    "        self.activation_derivative = leaky_ReLU_derivative\n",
    "\n",
    "        # Initialize weights and biases dynamically\n",
    "        layer_sizes = [self.input_layer_size] + self.hidden_layers + [self.output_layer_size]\n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(1 / layer_sizes[i])\n",
    "                        for i in range(len(layer_sizes) - 1)]\n",
    "        self.biases = [np.zeros(layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)]  # Initialize biases to zero\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a = [X]  # Store activations for backward pass\n",
    "        self.z = []  # Store weighted sums for backward pass\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):  \n",
    "            self.z.append(np.dot(self.a[-1], self.weights[i]) + self.biases[i])\n",
    "            self.a.append(self.activation_function(self.z[-1]))  # Use activation function here\n",
    "\n",
    "        # Output layer (no activation here, we apply softmax in loss)\n",
    "        self.z.append(np.dot(self.a[-1], self.weights[-1]) + self.biases[-1])\n",
    "        self.a.append(softmax(self.z[-1]))  # Raw output without softmax\n",
    "        \n",
    "        # Apply softmax to the output layer\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, y_true, learning_rate):\n",
    "        # Calculate the loss derivative w.r.t output (using categorical cross-entropy)\n",
    "        dL_da = categorical_crossentropy_derivative(y_true, self.a[-1])\n",
    "        \n",
    "        dW = []\n",
    "        dB = []\n",
    "        \n",
    "        # Backpropagation loop\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            if i == len(self.weights) - 1:\n",
    "                dL_dz = dL_da  # Output layer gradient (softmax + MSE derivative)\n",
    "            else:\n",
    "                dL_dz = np.dot(dL_da, self.weights[i + 1].T) * self.activation_derivative(self.z[i])\n",
    "            \n",
    "            dW.insert(0, np.dot(self.a[i].T, dL_dz))\n",
    "            dB.insert(0, np.sum(dL_dz, axis=0))\n",
    "            dL_da = dL_dz\n",
    "        \n",
    "        # Gradient descent update\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= (learning_rate * dW[i]) / y_true.shape[0]\n",
    "            self.biases[i] -= (learning_rate * dB[i]) / y_true.shape[0]\n",
    "\n",
    "    def train(self, X, y, epochs = 10, min_learning_rate = 1e-6, max_learning_rate = 1e-2):\n",
    "        for epoch in range(1, epochs+1):\n",
    "            current_learning_rate = max_learning_rate - (max_learning_rate - min_learning_rate) * epoch / epochs\n",
    "            y_pred = self.forward(X)\n",
    "            loss = categorical_crossentropy(y, y_pred) / y.shape[0]\n",
    "            self.backward(y, current_learning_rate)\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(f\"Epoch {epoch}, learning rate: {current_learning_rate}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 4880\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for index, row in train_datasets.iterrows():\n",
    "    img = crop_image(row[\"image_path\"]).convert('L')\n",
    "    img = np.array(img).reshape(784) / 255.0  # Flatten & normalize\n",
    "    x_train.append(img)\n",
    "    y_train.append(np.eye(10)[row[\"label\"]])  # One-hot encoding\n",
    "    # if index % 100 == 0:\n",
    "    #     print(f\"Image {index} processed\")\n",
    "\n",
    "print(\"Image count:\", len(x_train))\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count:  1230\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for index, row in test_datasets.iterrows():\n",
    "    img = crop_image(row[\"image_path\"]).convert('L')\n",
    "    img = np.array(img).reshape(784) / 255.0  # Flatten & normalize\n",
    "    x_test.append(img)\n",
    "    y_test.append(np.eye(10)[row[\"label\"]])  # One-hot encoding\n",
    "\n",
    "print(\"Image count: \", len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5477117797096405, learning rate: 0.00019998000000000002\n",
      "Epoch 2, Loss: 0.5477100803799471, learning rate: 0.00019996\n",
      "Epoch 3, Loss: 0.5477083736603029, learning rate: 0.00019994\n",
      "Epoch 4, Loss: 0.5477066678296478, learning rate: 0.00019992\n",
      "Epoch 5, Loss: 0.5477049672696254, learning rate: 0.0001999\n",
      "Epoch 6, Loss: 0.5477032614500666, learning rate: 0.00019988000000000002\n",
      "Epoch 7, Loss: 0.5477015594057393, learning rate: 0.00019986\n",
      "Epoch 8, Loss: 0.5476998554491362, learning rate: 0.00019984\n",
      "Epoch 9, Loss: 0.5476981530167844, learning rate: 0.00019982000000000002\n",
      "Epoch 10, Loss: 0.5476964486108549, learning rate: 0.0001998\n",
      "Epoch 11, Loss: 0.5476947483931751, learning rate: 0.00019978000000000002\n",
      "Epoch 12, Loss: 0.5476930417244412, learning rate: 0.00019976\n",
      "Epoch 13, Loss: 0.5476913374341121, learning rate: 0.00019974\n",
      "Epoch 14, Loss: 0.5476896406575764, learning rate: 0.00019972000000000002\n",
      "Epoch 15, Loss: 0.5476879338953852, learning rate: 0.0001997\n",
      "Epoch 16, Loss: 0.5476862345692763, learning rate: 0.00019968\n",
      "Epoch 17, Loss: 0.5476845283097799, learning rate: 0.00019966000000000002\n",
      "Epoch 18, Loss: 0.5476828276633524, learning rate: 0.00019964\n",
      "Epoch 19, Loss: 0.5476811228747132, learning rate: 0.00019962000000000002\n",
      "Epoch 20, Loss: 0.5476794224177419, learning rate: 0.0001996\n",
      "Epoch 21, Loss: 0.5476777244917195, learning rate: 0.00019958\n",
      "Epoch 22, Loss: 0.5476760231896242, learning rate: 0.00019956000000000002\n",
      "Epoch 23, Loss: 0.5476743220216336, learning rate: 0.00019954\n",
      "Epoch 24, Loss: 0.5476726225261543, learning rate: 0.00019952000000000001\n",
      "Epoch 25, Loss: 0.5476709205513607, learning rate: 0.0001995\n",
      "Epoch 26, Loss: 0.5476692216607647, learning rate: 0.00019948\n",
      "Epoch 27, Loss: 0.5476675216136027, learning rate: 0.00019946000000000002\n",
      "Epoch 28, Loss: 0.5476658249255079, learning rate: 0.00019944\n",
      "Epoch 29, Loss: 0.5476641244601262, learning rate: 0.00019942\n",
      "Epoch 30, Loss: 0.5476624283485945, learning rate: 0.00019940000000000002\n",
      "Epoch 31, Loss: 0.5476607276391843, learning rate: 0.00019938\n",
      "Epoch 32, Loss: 0.5476590300079465, learning rate: 0.00019936000000000002\n",
      "Epoch 33, Loss: 0.5476573309441829, learning rate: 0.00019934\n",
      "Epoch 34, Loss: 0.5476556340057076, learning rate: 0.00019932\n",
      "Epoch 35, Loss: 0.5476539371263179, learning rate: 0.00019930000000000002\n",
      "Epoch 36, Loss: 0.5476522381373877, learning rate: 0.00019928\n",
      "Epoch 37, Loss: 0.5476505453117436, learning rate: 0.00019926\n",
      "Epoch 38, Loss: 0.5476488467246444, learning rate: 0.00019924\n",
      "Epoch 39, Loss: 0.5476471509987124, learning rate: 0.00019922\n",
      "Epoch 40, Loss: 0.5476454546820262, learning rate: 0.00019920000000000002\n",
      "Epoch 41, Loss: 0.5476437598085531, learning rate: 0.00019918\n",
      "Epoch 42, Loss: 0.5476420635530115, learning rate: 0.00019916\n",
      "Epoch 43, Loss: 0.547640371341224, learning rate: 0.00019914000000000002\n",
      "Epoch 44, Loss: 0.5476386742711576, learning rate: 0.00019912\n",
      "Epoch 45, Loss: 0.5476369798420503, learning rate: 0.00019910000000000001\n",
      "Epoch 46, Loss: 0.5476352847106137, learning rate: 0.00019908\n",
      "Epoch 47, Loss: 0.547633592118431, learning rate: 0.00019906\n",
      "Epoch 48, Loss: 0.5476318981482092, learning rate: 0.00019904000000000002\n",
      "Epoch 49, Loss: 0.5476302057246315, learning rate: 0.00019902\n",
      "Epoch 50, Loss: 0.5476285094163391, learning rate: 0.000199\n",
      "Epoch 51, Loss: 0.5476268172717946, learning rate: 0.00019898000000000002\n",
      "Epoch 52, Loss: 0.5476251241061865, learning rate: 0.00019896\n",
      "Epoch 53, Loss: 0.5476234299647769, learning rate: 0.00019894000000000002\n",
      "Epoch 54, Loss: 0.5476217378239083, learning rate: 0.00019892\n",
      "Epoch 55, Loss: 0.5476200425762907, learning rate: 0.0001989\n",
      "Epoch 56, Loss: 0.5476183522628079, learning rate: 0.00019888000000000002\n",
      "Epoch 57, Loss: 0.5476166590903805, learning rate: 0.00019886\n",
      "Epoch 58, Loss: 0.5476149677854764, learning rate: 0.00019884000000000001\n",
      "Epoch 59, Loss: 0.5476132770464351, learning rate: 0.00019882\n",
      "Epoch 60, Loss: 0.5476115830450172, learning rate: 0.0001988\n",
      "Epoch 61, Loss: 0.5476098928091436, learning rate: 0.00019878000000000002\n",
      "Epoch 62, Loss: 0.547608200140983, learning rate: 0.00019876\n",
      "Epoch 63, Loss: 0.5476065129645987, learning rate: 0.00019874\n",
      "Epoch 64, Loss: 0.5476048204864038, learning rate: 0.00019872000000000002\n",
      "Epoch 65, Loss: 0.5476031299710973, learning rate: 0.0001987\n",
      "Epoch 66, Loss: 0.5476014421900934, learning rate: 0.00019868000000000002\n",
      "Epoch 67, Loss: 0.547599747441104, learning rate: 0.00019866\n",
      "Epoch 68, Loss: 0.547598062305829, learning rate: 0.00019864\n",
      "Epoch 69, Loss: 0.54759636689176, learning rate: 0.00019862000000000002\n",
      "Epoch 70, Loss: 0.5475946793784766, learning rate: 0.0001986\n",
      "Epoch 71, Loss: 0.547592987475947, learning rate: 0.00019858\n",
      "Epoch 72, Loss: 0.5475912997104275, learning rate: 0.00019856\n",
      "Epoch 73, Loss: 0.5475896104843906, learning rate: 0.00019854\n",
      "Epoch 74, Loss: 0.5475879204150607, learning rate: 0.00019852000000000002\n",
      "Epoch 75, Loss: 0.5475862364036125, learning rate: 0.0001985\n",
      "Epoch 76, Loss: 0.5475845415420155, learning rate: 0.00019848\n",
      "Epoch 77, Loss: 0.5475828564859365, learning rate: 0.00019846000000000002\n",
      "Epoch 78, Loss: 0.5475811675590861, learning rate: 0.00019844\n",
      "Epoch 79, Loss: 0.5475794801642196, learning rate: 0.00019842000000000001\n",
      "Epoch 80, Loss: 0.5475777911066864, learning rate: 0.0001984\n",
      "Epoch 81, Loss: 0.5475761089862692, learning rate: 0.00019838\n",
      "Epoch 82, Loss: 0.5475744161228638, learning rate: 0.00019836000000000002\n",
      "Epoch 83, Loss: 0.5475727295423404, learning rate: 0.00019834\n",
      "Epoch 84, Loss: 0.5475710425653979, learning rate: 0.00019832\n",
      "Epoch 85, Loss: 0.5475693539720622, learning rate: 0.00019830000000000002\n",
      "Epoch 86, Loss: 0.5475676714911853, learning rate: 0.00019828\n",
      "Epoch 87, Loss: 0.5475659803938676, learning rate: 0.00019826000000000002\n",
      "Epoch 88, Loss: 0.5475642965287035, learning rate: 0.00019824\n",
      "Epoch 89, Loss: 0.5475626095368519, learning rate: 0.00019822\n",
      "Epoch 90, Loss: 0.5475609236938819, learning rate: 0.00019820000000000002\n",
      "Epoch 91, Loss: 0.5475592415876419, learning rate: 0.00019818\n",
      "Epoch 92, Loss: 0.5475575510106913, learning rate: 0.00019816000000000001\n",
      "Epoch 93, Loss: 0.5475558690981152, learning rate: 0.00019814\n",
      "Epoch 94, Loss: 0.5475541810072695, learning rate: 0.00019812\n",
      "Epoch 95, Loss: 0.547552497759341, learning rate: 0.00019810000000000002\n",
      "Epoch 96, Loss: 0.5475508122065299, learning rate: 0.00019808\n",
      "Epoch 97, Loss: 0.5475491253765061, learning rate: 0.00019806\n",
      "Epoch 98, Loss: 0.5475474482062259, learning rate: 0.00019804000000000002\n",
      "Epoch 99, Loss: 0.547545756793313, learning rate: 0.00019802\n",
      "Epoch 100, Loss: 0.5475440781871974, learning rate: 0.00019800000000000002\n",
      "Epoch 101, Loss: 0.5475423938790579, learning rate: 0.00019798\n",
      "Epoch 102, Loss: 0.5475407061121632, learning rate: 0.00019796\n",
      "Epoch 103, Loss: 0.5475390266697269, learning rate: 0.00019794000000000002\n",
      "Epoch 104, Loss: 0.547537340416799, learning rate: 0.00019792\n",
      "Epoch 105, Loss: 0.5475356605298782, learning rate: 0.0001979\n",
      "Epoch 106, Loss: 0.5475339800305125, learning rate: 0.00019788\n",
      "Epoch 107, Loss: 0.5475322931774006, learning rate: 0.00019786\n",
      "Epoch 108, Loss: 0.5475306126183106, learning rate: 0.00019784000000000002\n",
      "Epoch 109, Loss: 0.5475289343112805, learning rate: 0.00019782\n",
      "Epoch 110, Loss: 0.5475272467514335, learning rate: 0.0001978\n",
      "Epoch 111, Loss: 0.547525569533356, learning rate: 0.00019778000000000002\n",
      "Epoch 112, Loss: 0.5475238836491533, learning rate: 0.00019776\n",
      "Epoch 113, Loss: 0.5475222046744975, learning rate: 0.00019774000000000001\n",
      "Epoch 114, Loss: 0.5475205267259267, learning rate: 0.00019772\n",
      "Epoch 115, Loss: 0.5475188407172736, learning rate: 0.0001977\n",
      "Epoch 116, Loss: 0.5475171617687156, learning rate: 0.00019768000000000002\n",
      "Epoch 117, Loss: 0.5475154824273758, learning rate: 0.00019766\n",
      "Epoch 118, Loss: 0.5475138030892124, learning rate: 0.00019764\n",
      "Epoch 119, Loss: 0.5475121205335383, learning rate: 0.00019762000000000002\n",
      "Epoch 120, Loss: 0.5475104403037824, learning rate: 0.0001976\n",
      "Epoch 121, Loss: 0.547508764038589, learning rate: 0.00019758000000000002\n",
      "Epoch 122, Loss: 0.5475070815644816, learning rate: 0.00019756\n",
      "Epoch 123, Loss: 0.5475054027915042, learning rate: 0.00019754\n",
      "Epoch 124, Loss: 0.5475037255949392, learning rate: 0.00019752000000000002\n",
      "Epoch 125, Loss: 0.5475020478378186, learning rate: 0.0001975\n",
      "Epoch 126, Loss: 0.5475003668945656, learning rate: 0.00019748\n",
      "Epoch 127, Loss: 0.5474986887612481, learning rate: 0.00019746\n",
      "Epoch 128, Loss: 0.5474970124589918, learning rate: 0.00019744\n",
      "Epoch 129, Loss: 0.5474953321796611, learning rate: 0.00019742000000000002\n",
      "Epoch 130, Loss: 0.5474936561713178, learning rate: 0.0001974\n",
      "Epoch 131, Loss: 0.5474919779369548, learning rate: 0.00019738\n",
      "Epoch 132, Loss: 0.547490300181226, learning rate: 0.00019736000000000002\n",
      "Epoch 133, Loss: 0.5474886227183523, learning rate: 0.00019734\n",
      "Epoch 134, Loss: 0.5474869466297921, learning rate: 0.00019732000000000001\n",
      "Epoch 135, Loss: 0.5474852722989311, learning rate: 0.0001973\n",
      "Epoch 136, Loss: 0.5474835901622581, learning rate: 0.00019728\n",
      "Epoch 137, Loss: 0.5474819137029506, learning rate: 0.00019726000000000002\n",
      "Epoch 138, Loss: 0.5474802409362577, learning rate: 0.00019724\n",
      "Epoch 139, Loss: 0.5474785593670511, learning rate: 0.00019722\n",
      "Epoch 140, Loss: 0.5474768824306118, learning rate: 0.00019720000000000002\n",
      "Epoch 141, Loss: 0.5474752076539112, learning rate: 0.00019718\n",
      "Epoch 142, Loss: 0.5474735332297354, learning rate: 0.00019716000000000002\n",
      "Epoch 143, Loss: 0.5474718551168533, learning rate: 0.00019714\n",
      "Epoch 144, Loss: 0.5474701788447804, learning rate: 0.00019712\n",
      "Epoch 145, Loss: 0.5474685028974258, learning rate: 0.00019710000000000002\n",
      "Epoch 146, Loss: 0.5474668295716826, learning rate: 0.00019708\n",
      "Epoch 147, Loss: 0.5474651529405672, learning rate: 0.00019706000000000001\n",
      "Epoch 148, Loss: 0.5474634759678927, learning rate: 0.00019704\n",
      "Epoch 149, Loss: 0.5474618012085868, learning rate: 0.00019702\n",
      "Epoch 150, Loss: 0.5474601270713229, learning rate: 0.00019700000000000002\n",
      "Epoch 151, Loss: 0.5474584519128627, learning rate: 0.00019698\n",
      "Epoch 152, Loss: 0.5474567780134421, learning rate: 0.00019696\n",
      "Epoch 153, Loss: 0.5474551036143681, learning rate: 0.00019694000000000002\n",
      "Epoch 154, Loss: 0.5474534314817958, learning rate: 0.00019692\n",
      "Epoch 155, Loss: 0.5474517599551411, learning rate: 0.00019690000000000002\n",
      "Epoch 156, Loss: 0.5474500826176278, learning rate: 0.00019688\n",
      "Epoch 157, Loss: 0.5474484108856068, learning rate: 0.00019686\n",
      "Epoch 158, Loss: 0.5474467364875091, learning rate: 0.00019684000000000002\n",
      "Epoch 159, Loss: 0.5474450624855977, learning rate: 0.00019682\n",
      "Epoch 160, Loss: 0.5474433893864736, learning rate: 0.0001968\n",
      "Epoch 161, Loss: 0.5474417151524035, learning rate: 0.00019678\n",
      "Epoch 162, Loss: 0.5474400467416676, learning rate: 0.00019676\n",
      "Epoch 163, Loss: 0.5474383743011741, learning rate: 0.00019674000000000002\n",
      "Epoch 164, Loss: 0.5474367004414263, learning rate: 0.00019672\n",
      "Epoch 165, Loss: 0.547435027201817, learning rate: 0.0001967\n",
      "Epoch 166, Loss: 0.5474333557079123, learning rate: 0.00019668000000000002\n",
      "Epoch 167, Loss: 0.5474316865228589, learning rate: 0.00019666\n",
      "Epoch 168, Loss: 0.5474300129924616, learning rate: 0.00019664000000000001\n",
      "Epoch 169, Loss: 0.5474283422176023, learning rate: 0.00019662\n",
      "Epoch 170, Loss: 0.5474266732276734, learning rate: 0.0001966\n",
      "Epoch 171, Loss: 0.5474250003255684, learning rate: 0.00019658000000000002\n",
      "Epoch 172, Loss: 0.5474233300750001, learning rate: 0.00019656\n",
      "Epoch 173, Loss: 0.5474216588590317, learning rate: 0.00019654\n",
      "Epoch 174, Loss: 0.5474199928907613, learning rate: 0.00019652\n",
      "Epoch 175, Loss: 0.5474183185694452, learning rate: 0.0001965\n",
      "Epoch 176, Loss: 0.5474166509253784, learning rate: 0.00019648000000000002\n",
      "Epoch 177, Loss: 0.5474149859224118, learning rate: 0.00019646\n",
      "Epoch 178, Loss: 0.5474133114468774, learning rate: 0.00019644\n",
      "Epoch 179, Loss: 0.5474116419544169, learning rate: 0.00019642000000000002\n",
      "Epoch 180, Loss: 0.5474099749234588, learning rate: 0.0001964\n",
      "Epoch 181, Loss: 0.5474083071906944, learning rate: 0.00019638000000000001\n",
      "Epoch 182, Loss: 0.5474066387549171, learning rate: 0.00019636\n",
      "Epoch 183, Loss: 0.5474049706574963, learning rate: 0.00019634\n",
      "Epoch 184, Loss: 0.5474033008258006, learning rate: 0.00019632000000000002\n",
      "Epoch 185, Loss: 0.5474016318434832, learning rate: 0.0001963\n",
      "Epoch 186, Loss: 0.5473999661229249, learning rate: 0.00019628\n",
      "Epoch 187, Loss: 0.5473983005113028, learning rate: 0.00019626000000000002\n",
      "Epoch 188, Loss: 0.5473966312259352, learning rate: 0.00019624\n",
      "Epoch 189, Loss: 0.5473949635279188, learning rate: 0.00019622000000000002\n",
      "Epoch 190, Loss: 0.5473932984891133, learning rate: 0.0001962\n",
      "Epoch 191, Loss: 0.5473916295724873, learning rate: 0.00019618\n",
      "Epoch 192, Loss: 0.5473899635062179, learning rate: 0.00019616000000000002\n",
      "Epoch 193, Loss: 0.5473882987980657, learning rate: 0.00019614\n",
      "Epoch 194, Loss: 0.5473866305294822, learning rate: 0.00019612\n",
      "Epoch 195, Loss: 0.5473849666135234, learning rate: 0.0001961\n",
      "Epoch 196, Loss: 0.5473832998909484, learning rate: 0.00019608\n",
      "Epoch 197, Loss: 0.5473816336278037, learning rate: 0.00019606000000000002\n",
      "Epoch 198, Loss: 0.5473799692715082, learning rate: 0.00019604\n",
      "Epoch 199, Loss: 0.5473783057965387, learning rate: 0.00019602\n",
      "Epoch 200, Loss: 0.5473766366030938, learning rate: 0.00019600000000000002\n",
      "Epoch 201, Loss: 0.547374974183308, learning rate: 0.00019598\n",
      "Epoch 202, Loss: 0.5473733111930214, learning rate: 0.00019596000000000001\n",
      "Epoch 203, Loss: 0.5473716443807168, learning rate: 0.00019594\n",
      "Epoch 204, Loss: 0.5473699808582492, learning rate: 0.00019592\n",
      "Epoch 205, Loss: 0.5473683173633407, learning rate: 0.00019590000000000002\n",
      "Epoch 206, Loss: 0.5473666530308555, learning rate: 0.00019588\n",
      "Epoch 207, Loss: 0.5473649899369647, learning rate: 0.00019586\n",
      "Epoch 208, Loss: 0.5473633305439807, learning rate: 0.00019584000000000002\n",
      "Epoch 209, Loss: 0.5473616615205368, learning rate: 0.00019582\n",
      "Epoch 210, Loss: 0.547359999892729, learning rate: 0.00019580000000000002\n",
      "Epoch 211, Loss: 0.5473583371736966, learning rate: 0.00019578\n",
      "Epoch 212, Loss: 0.5473566753606065, learning rate: 0.00019576\n",
      "Epoch 213, Loss: 0.5473550118443535, learning rate: 0.00019574000000000002\n",
      "Epoch 214, Loss: 0.5473533506378119, learning rate: 0.00019572\n",
      "Epoch 215, Loss: 0.5473516848798182, learning rate: 0.0001957\n",
      "Epoch 216, Loss: 0.5473500258784492, learning rate: 0.00019568\n",
      "Epoch 217, Loss: 0.547348365048287, learning rate: 0.00019566\n",
      "Epoch 218, Loss: 0.547346697688406, learning rate: 0.00019564000000000002\n",
      "Epoch 219, Loss: 0.5473450377092155, learning rate: 0.00019562\n",
      "Epoch 220, Loss: 0.5473433745958289, learning rate: 0.0001956\n",
      "Epoch 221, Loss: 0.5473417140628789, learning rate: 0.00019558000000000002\n",
      "Epoch 222, Loss: 0.547340048710904, learning rate: 0.00019556\n",
      "Epoch 223, Loss: 0.5473383912899383, learning rate: 0.00019554000000000002\n",
      "Epoch 224, Loss: 0.547336728352534, learning rate: 0.00019552\n",
      "Epoch 225, Loss: 0.5473350665575857, learning rate: 0.0001955\n",
      "Epoch 226, Loss: 0.5473334099979729, learning rate: 0.00019548000000000002\n",
      "Epoch 227, Loss: 0.5473317463292409, learning rate: 0.00019546\n",
      "Epoch 228, Loss: 0.5473300857491055, learning rate: 0.00019544\n",
      "Epoch 229, Loss: 0.5473284248748492, learning rate: 0.00019542000000000002\n",
      "Epoch 230, Loss: 0.547326767581369, learning rate: 0.0001954\n",
      "Epoch 231, Loss: 0.547325104248912, learning rate: 0.00019538000000000002\n",
      "Epoch 232, Loss: 0.5473234435938334, learning rate: 0.00019536\n",
      "Epoch 233, Loss: 0.5473217883330236, learning rate: 0.00019534\n",
      "Epoch 234, Loss: 0.5473201254846652, learning rate: 0.00019532000000000002\n",
      "Epoch 235, Loss: 0.5473184674386844, learning rate: 0.0001953\n",
      "Epoch 236, Loss: 0.5473168105398006, learning rate: 0.00019528000000000001\n",
      "Epoch 237, Loss: 0.5473151498373271, learning rate: 0.00019526\n",
      "Epoch 238, Loss: 0.5473134917794348, learning rate: 0.00019524\n",
      "Epoch 239, Loss: 0.5473118336046159, learning rate: 0.00019522000000000002\n",
      "Epoch 240, Loss: 0.5473101802422318, learning rate: 0.0001952\n",
      "Epoch 241, Loss: 0.5473085151640427, learning rate: 0.00019518\n",
      "Epoch 242, Loss: 0.5473068598595463, learning rate: 0.00019516000000000002\n",
      "Epoch 243, Loss: 0.5473051993665942, learning rate: 0.00019514\n",
      "Epoch 244, Loss: 0.5473035455077503, learning rate: 0.00019512000000000002\n",
      "Epoch 245, Loss: 0.5473018870031768, learning rate: 0.0001951\n",
      "Epoch 246, Loss: 0.5473002308246837, learning rate: 0.00019508\n",
      "Epoch 247, Loss: 0.5472985782804247, learning rate: 0.00019506000000000002\n",
      "Epoch 248, Loss: 0.5472969146645655, learning rate: 0.00019504\n",
      "Epoch 249, Loss: 0.5472952590666084, learning rate: 0.00019502\n",
      "Epoch 250, Loss: 0.5472936001842932, learning rate: 0.000195\n",
      "Epoch 251, Loss: 0.5472919475495246, learning rate: 0.00019498\n",
      "Epoch 252, Loss: 0.5472902941044971, learning rate: 0.00019496000000000002\n",
      "Epoch 253, Loss: 0.5472886336229693, learning rate: 0.00019494\n",
      "Epoch 254, Loss: 0.5472869803759084, learning rate: 0.00019492\n",
      "Epoch 255, Loss: 0.5472853254676949, learning rate: 0.00019490000000000002\n",
      "Epoch 256, Loss: 0.5472836716741233, learning rate: 0.00019488\n",
      "Epoch 257, Loss: 0.5472820122069593, learning rate: 0.00019486000000000001\n",
      "Epoch 258, Loss: 0.5472803591836949, learning rate: 0.00019484\n",
      "Epoch 259, Loss: 0.54727870812861, learning rate: 0.00019482\n",
      "Epoch 260, Loss: 0.5472770482634772, learning rate: 0.00019480000000000002\n",
      "Epoch 261, Loss: 0.5472753957797665, learning rate: 0.00019478\n",
      "Epoch 262, Loss: 0.5472737411039625, learning rate: 0.00019476\n",
      "Epoch 263, Loss: 0.5472720879775663, learning rate: 0.00019474\n",
      "Epoch 264, Loss: 0.5472704338694615, learning rate: 0.00019472\n",
      "Epoch 265, Loss: 0.5472687827267207, learning rate: 0.00019470000000000002\n",
      "Epoch 266, Loss: 0.547267131285142, learning rate: 0.00019468\n",
      "Epoch 267, Loss: 0.5472654723108541, learning rate: 0.00019466\n",
      "Epoch 268, Loss: 0.5472638224950401, learning rate: 0.00019464000000000002\n",
      "Epoch 269, Loss: 0.5472621704999368, learning rate: 0.00019462\n",
      "Epoch 270, Loss: 0.547260516751228, learning rate: 0.00019460000000000001\n",
      "Epoch 271, Loss: 0.5472588643186694, learning rate: 0.00019458\n",
      "Epoch 272, Loss: 0.5472572144858767, learning rate: 0.00019456\n",
      "Epoch 273, Loss: 0.5472555672031815, learning rate: 0.00019454000000000002\n",
      "Epoch 274, Loss: 0.5472539089645021, learning rate: 0.00019452\n",
      "Epoch 275, Loss: 0.5472522597452583, learning rate: 0.0001945\n",
      "Epoch 276, Loss: 0.5472506110128909, learning rate: 0.00019448000000000002\n",
      "Epoch 277, Loss: 0.5472489605781442, learning rate: 0.00019446\n",
      "Epoch 278, Loss: 0.5472473104843888, learning rate: 0.00019444000000000002\n",
      "Epoch 279, Loss: 0.5472456603019107, learning rate: 0.00019442\n",
      "Epoch 280, Loss: 0.5472440084554971, learning rate: 0.0001944\n",
      "Epoch 281, Loss: 0.5472423623001726, learning rate: 0.00019438000000000002\n",
      "Epoch 282, Loss: 0.547240713362088, learning rate: 0.00019436\n",
      "Epoch 283, Loss: 0.5472390634481048, learning rate: 0.00019434\n",
      "Epoch 284, Loss: 0.5472374174109131, learning rate: 0.00019432\n",
      "Epoch 285, Loss: 0.5472357665445787, learning rate: 0.0001943\n",
      "Epoch 286, Loss: 0.5472341194827536, learning rate: 0.00019428000000000002\n",
      "Epoch 287, Loss: 0.5472324721452382, learning rate: 0.00019426\n",
      "Epoch 288, Loss: 0.5472308215261985, learning rate: 0.00019424\n",
      "Epoch 289, Loss: 0.5472291755208348, learning rate: 0.00019422000000000002\n",
      "Epoch 290, Loss: 0.5472275295645367, learning rate: 0.0001942\n",
      "Epoch 291, Loss: 0.547225879689367, learning rate: 0.00019418000000000001\n",
      "Epoch 292, Loss: 0.5472242347326911, learning rate: 0.00019416\n",
      "Epoch 293, Loss: 0.547222587335577, learning rate: 0.00019414\n",
      "Epoch 294, Loss: 0.547220940791528, learning rate: 0.00019412000000000002\n",
      "Epoch 295, Loss: 0.547219295443303, learning rate: 0.0001941\n",
      "Epoch 296, Loss: 0.5472176479893639, learning rate: 0.00019408\n",
      "Epoch 297, Loss: 0.5472160029747001, learning rate: 0.00019406000000000002\n",
      "Epoch 298, Loss: 0.547214355893415, learning rate: 0.00019404\n",
      "Epoch 299, Loss: 0.5472127108184606, learning rate: 0.00019402000000000002\n",
      "Epoch 300, Loss: 0.5472110681488609, learning rate: 0.000194\n",
      "Epoch 301, Loss: 0.547209418401205, learning rate: 0.00019398\n",
      "Epoch 302, Loss: 0.5472077768512041, learning rate: 0.00019396000000000002\n",
      "Epoch 303, Loss: 0.547206129308862, learning rate: 0.00019394\n",
      "Epoch 304, Loss: 0.5472044854700613, learning rate: 0.00019392000000000001\n",
      "Epoch 305, Loss: 0.5472028414810797, learning rate: 0.0001939\n",
      "Epoch 306, Loss: 0.5472011947466673, learning rate: 0.00019388\n",
      "Epoch 307, Loss: 0.5471995513861794, learning rate: 0.00019386000000000002\n",
      "Epoch 308, Loss: 0.5471979055782903, learning rate: 0.00019384\n",
      "Epoch 309, Loss: 0.5471962644873336, learning rate: 0.00019382\n",
      "Epoch 310, Loss: 0.5471946171485983, learning rate: 0.00019380000000000002\n",
      "Epoch 311, Loss: 0.5471929737203877, learning rate: 0.00019378\n",
      "Epoch 312, Loss: 0.5471913315840139, learning rate: 0.00019376000000000002\n",
      "Epoch 313, Loss: 0.5471896881653466, learning rate: 0.00019374\n",
      "Epoch 314, Loss: 0.547188046408819, learning rate: 0.00019372\n",
      "Epoch 315, Loss: 0.5471864011609525, learning rate: 0.00019370000000000002\n",
      "Epoch 316, Loss: 0.5471847587992865, learning rate: 0.00019368\n",
      "Epoch 317, Loss: 0.5471831173063724, learning rate: 0.00019366\n",
      "Epoch 318, Loss: 0.5471814733433397, learning rate: 0.00019364000000000002\n",
      "Epoch 319, Loss: 0.5471798326500334, learning rate: 0.00019362\n",
      "Epoch 320, Loss: 0.5471781891333667, learning rate: 0.00019360000000000002\n",
      "Epoch 321, Loss: 0.547176547888755, learning rate: 0.00019358\n",
      "Epoch 322, Loss: 0.5471749085786541, learning rate: 0.00019356\n",
      "Epoch 323, Loss: 0.5471732625756113, learning rate: 0.00019354000000000002\n",
      "Epoch 324, Loss: 0.5471716237475497, learning rate: 0.00019352\n",
      "Epoch 325, Loss: 0.5471699847643225, learning rate: 0.00019350000000000001\n",
      "Epoch 326, Loss: 0.5471683391107778, learning rate: 0.00019348\n",
      "Epoch 327, Loss: 0.5471667019627935, learning rate: 0.00019346\n",
      "Epoch 328, Loss: 0.5471650586089806, learning rate: 0.00019344000000000002\n",
      "Epoch 329, Loss: 0.5471634230007123, learning rate: 0.00019342\n",
      "Epoch 330, Loss: 0.5471617840128167, learning rate: 0.0001934\n",
      "Epoch 331, Loss: 0.5471601390415302, learning rate: 0.00019338\n",
      "Epoch 332, Loss: 0.5471585018463715, learning rate: 0.00019336\n",
      "Epoch 333, Loss: 0.5471568602100506, learning rate: 0.00019334000000000002\n",
      "Epoch 334, Loss: 0.5471552239393102, learning rate: 0.00019332\n",
      "Epoch 335, Loss: 0.5471535834539794, learning rate: 0.0001933\n",
      "Epoch 336, Loss: 0.5471519459987517, learning rate: 0.00019328000000000002\n",
      "Epoch 337, Loss: 0.5471503066702534, learning rate: 0.00019326\n",
      "Epoch 338, Loss: 0.5471486690784692, learning rate: 0.00019324\n",
      "Epoch 339, Loss: 0.5471470334615511, learning rate: 0.00019322\n",
      "Epoch 340, Loss: 0.5471453926038109, learning rate: 0.0001932\n",
      "Epoch 341, Loss: 0.5471437572482829, learning rate: 0.00019318000000000002\n",
      "Epoch 342, Loss: 0.5471421169000196, learning rate: 0.00019316\n",
      "Epoch 343, Loss: 0.5471404807188051, learning rate: 0.00019314\n",
      "Epoch 344, Loss: 0.5471388434360573, learning rate: 0.00019312000000000002\n",
      "Epoch 345, Loss: 0.547137205447153, learning rate: 0.0001931\n",
      "Epoch 346, Loss: 0.5471355731870833, learning rate: 0.00019308000000000001\n",
      "Epoch 347, Loss: 0.5471339325957307, learning rate: 0.00019306\n",
      "Epoch 348, Loss: 0.5471323022975773, learning rate: 0.00019304\n",
      "Epoch 349, Loss: 0.5471306608252645, learning rate: 0.00019302000000000002\n",
      "Epoch 350, Loss: 0.5471290262990058, learning rate: 0.000193\n",
      "Epoch 351, Loss: 0.5471273910032478, learning rate: 0.00019298\n",
      "Epoch 352, Loss: 0.5471257537161855, learning rate: 0.00019296\n",
      "Epoch 353, Loss: 0.5471241212137469, learning rate: 0.00019294\n",
      "Epoch 354, Loss: 0.5471224843387072, learning rate: 0.00019292000000000002\n",
      "Epoch 355, Loss: 0.5471208528384627, learning rate: 0.0001929\n",
      "Epoch 356, Loss: 0.5471192158985655, learning rate: 0.00019288\n",
      "Epoch 357, Loss: 0.5471175825104008, learning rate: 0.00019286000000000002\n",
      "Epoch 358, Loss: 0.5471159480669731, learning rate: 0.00019284\n",
      "Epoch 359, Loss: 0.5471143109669628, learning rate: 0.00019282000000000001\n",
      "Epoch 360, Loss: 0.5471126774552157, learning rate: 0.0001928\n",
      "Epoch 361, Loss: 0.5471110408243579, learning rate: 0.00019278\n",
      "Epoch 362, Loss: 0.5471094067746869, learning rate: 0.00019276000000000002\n",
      "Epoch 363, Loss: 0.5471077697930068, learning rate: 0.00019274\n",
      "Epoch 364, Loss: 0.5471061393932708, learning rate: 0.00019272\n",
      "Epoch 365, Loss: 0.5471045024266236, learning rate: 0.00019270000000000002\n",
      "Epoch 366, Loss: 0.5471028702790156, learning rate: 0.00019268\n",
      "Epoch 367, Loss: 0.5471012364661999, learning rate: 0.00019266000000000002\n",
      "Epoch 368, Loss: 0.5470996011328181, learning rate: 0.00019264\n",
      "Epoch 369, Loss: 0.5470979665107117, learning rate: 0.00019262\n",
      "Epoch 370, Loss: 0.5470963352891044, learning rate: 0.00019260000000000002\n",
      "Epoch 371, Loss: 0.547094701881243, learning rate: 0.00019258\n",
      "Epoch 372, Loss: 0.5470930709665881, learning rate: 0.00019256\n",
      "Epoch 373, Loss: 0.5470914386712623, learning rate: 0.00019254\n",
      "Epoch 374, Loss: 0.5470898035813149, learning rate: 0.00019252\n",
      "Epoch 375, Loss: 0.5470881752099452, learning rate: 0.00019250000000000002\n",
      "Epoch 376, Loss: 0.5470865402797069, learning rate: 0.00019248\n",
      "Epoch 377, Loss: 0.5470849124594582, learning rate: 0.00019246\n",
      "Epoch 378, Loss: 0.5470832777686646, learning rate: 0.00019244000000000002\n",
      "Epoch 379, Loss: 0.5470816505838689, learning rate: 0.00019242\n",
      "Epoch 380, Loss: 0.5470800152046555, learning rate: 0.00019240000000000001\n",
      "Epoch 381, Loss: 0.5470783862243942, learning rate: 0.00019238\n",
      "Epoch 382, Loss: 0.5470767570471963, learning rate: 0.00019236\n",
      "Epoch 383, Loss: 0.5470751245701887, learning rate: 0.00019234000000000002\n",
      "Epoch 384, Loss: 0.5470734969257957, learning rate: 0.00019232\n",
      "Epoch 385, Loss: 0.5470718641627119, learning rate: 0.0001923\n",
      "Epoch 386, Loss: 0.5470702377331785, learning rate: 0.00019228000000000002\n",
      "Epoch 387, Loss: 0.5470686066797223, learning rate: 0.00019226\n",
      "Epoch 388, Loss: 0.5470669754597485, learning rate: 0.00019224000000000002\n",
      "Epoch 389, Loss: 0.5470653484961457, learning rate: 0.00019222\n",
      "Epoch 390, Loss: 0.5470637198697137, learning rate: 0.0001922\n",
      "Epoch 391, Loss: 0.547062089294083, learning rate: 0.00019218000000000002\n",
      "Epoch 392, Loss: 0.5470604622418849, learning rate: 0.00019216\n",
      "Epoch 393, Loss: 0.5470588320014736, learning rate: 0.00019214000000000001\n",
      "Epoch 394, Loss: 0.5470572080582129, learning rate: 0.00019212\n",
      "Epoch 395, Loss: 0.5470555742246258, learning rate: 0.0001921\n",
      "Epoch 396, Loss: 0.5470539505250358, learning rate: 0.00019208000000000002\n",
      "Epoch 397, Loss: 0.5470523216134368, learning rate: 0.00019206\n",
      "Epoch 398, Loss: 0.5470506925448099, learning rate: 0.00019204\n",
      "Epoch 399, Loss: 0.5470490685150153, learning rate: 0.00019202\n",
      "Epoch 400, Loss: 0.5470474382190724, learning rate: 0.000192\n",
      "Epoch 401, Loss: 0.5470458158533748, learning rate: 0.00019198000000000002\n",
      "Epoch 402, Loss: 0.5470441837312797, learning rate: 0.00019196\n",
      "Epoch 403, Loss: 0.5470425599369125, learning rate: 0.00019194\n",
      "Epoch 404, Loss: 0.5470409311222179, learning rate: 0.00019192000000000002\n",
      "Epoch 405, Loss: 0.54703930581263, learning rate: 0.0001919\n",
      "Epoch 406, Loss: 0.5470376809498363, learning rate: 0.00019188\n",
      "Epoch 407, Loss: 0.5470360534634419, learning rate: 0.00019186000000000002\n",
      "Epoch 408, Loss: 0.5470344314159669, learning rate: 0.00019184\n",
      "Epoch 409, Loss: 0.5470328011292529, learning rate: 0.00019182000000000002\n",
      "Epoch 410, Loss: 0.547031179429204, learning rate: 0.0001918\n",
      "Epoch 411, Loss: 0.5470295516120458, learning rate: 0.00019178\n",
      "Epoch 412, Loss: 0.5470279294048093, learning rate: 0.00019176000000000002\n",
      "Epoch 413, Loss: 0.5470263014085147, learning rate: 0.00019174\n",
      "Epoch 414, Loss: 0.5470246796849825, learning rate: 0.00019172000000000001\n",
      "Epoch 415, Loss: 0.5470230558498936, learning rate: 0.0001917\n",
      "Epoch 416, Loss: 0.5470214277634146, learning rate: 0.00019168\n",
      "Epoch 417, Loss: 0.5470198069434618, learning rate: 0.00019166000000000002\n",
      "Epoch 418, Loss: 0.5470181813760389, learning rate: 0.00019164\n",
      "Epoch 419, Loss: 0.5470165596635239, learning rate: 0.00019162\n",
      "Epoch 420, Loss: 0.547014933864935, learning rate: 0.0001916\n",
      "Epoch 421, Loss: 0.5470133141552826, learning rate: 0.00019158\n",
      "Epoch 422, Loss: 0.5470116870267703, learning rate: 0.00019156000000000002\n",
      "Epoch 423, Loss: 0.5470100637144868, learning rate: 0.00019154\n",
      "Epoch 424, Loss: 0.5470084427054894, learning rate: 0.00019152\n",
      "Epoch 425, Loss: 0.5470068167181225, learning rate: 0.00019150000000000002\n",
      "Epoch 426, Loss: 0.5470051980891463, learning rate: 0.00019148\n",
      "Epoch 427, Loss: 0.5470035736984415, learning rate: 0.00019146\n",
      "Epoch 428, Loss: 0.5470019494973303, learning rate: 0.00019144\n",
      "Epoch 429, Loss: 0.5470003297165398, learning rate: 0.00019142\n",
      "Epoch 430, Loss: 0.5469987030851505, learning rate: 0.00019140000000000002\n",
      "Epoch 431, Loss: 0.5469970868906895, learning rate: 0.00019138\n",
      "Epoch 432, Loss: 0.5469954608642053, learning rate: 0.00019136\n",
      "Epoch 433, Loss: 0.5469938420459945, learning rate: 0.00019134000000000002\n",
      "Epoch 434, Loss: 0.5469922198739643, learning rate: 0.00019132\n",
      "Epoch 435, Loss: 0.5469905972920326, learning rate: 0.00019130000000000001\n",
      "Epoch 436, Loss: 0.5469889800652804, learning rate: 0.00019128\n",
      "Epoch 437, Loss: 0.5469873540945475, learning rate: 0.00019126\n",
      "Epoch 438, Loss: 0.546985736163654, learning rate: 0.00019124000000000002\n",
      "Epoch 439, Loss: 0.5469841153996239, learning rate: 0.00019122\n",
      "Epoch 440, Loss: 0.5469824928105265, learning rate: 0.0001912\n",
      "Epoch 441, Loss: 0.546980874721974, learning rate: 0.00019118\n",
      "Epoch 442, Loss: 0.5469792547692526, learning rate: 0.00019116\n",
      "Epoch 443, Loss: 0.5469776348775476, learning rate: 0.00019114000000000002\n",
      "Epoch 444, Loss: 0.5469760164677306, learning rate: 0.00019112\n",
      "Epoch 445, Loss: 0.5469743943004292, learning rate: 0.0001911\n",
      "Epoch 446, Loss: 0.54697277765693, learning rate: 0.00019108000000000002\n",
      "Epoch 447, Loss: 0.5469711592976906, learning rate: 0.00019106\n",
      "Epoch 448, Loss: 0.5469695382035848, learning rate: 0.00019104000000000001\n",
      "Epoch 449, Loss: 0.5469679224785664, learning rate: 0.00019102\n",
      "Epoch 450, Loss: 0.5469663040148725, learning rate: 0.000191\n",
      "Epoch 451, Loss: 0.5469646825055483, learning rate: 0.00019098000000000002\n",
      "Epoch 452, Loss: 0.5469630686744053, learning rate: 0.00019096\n",
      "Epoch 453, Loss: 0.5469614491818434, learning rate: 0.00019094\n",
      "Epoch 454, Loss: 0.546959831412774, learning rate: 0.00019092000000000002\n",
      "Epoch 455, Loss: 0.5469582167555744, learning rate: 0.0001909\n",
      "Epoch 456, Loss: 0.5469565955340198, learning rate: 0.00019088000000000002\n",
      "Epoch 457, Loss: 0.5469549799315218, learning rate: 0.00019086\n",
      "Epoch 458, Loss: 0.546953363826475, learning rate: 0.00019084\n",
      "Epoch 459, Loss: 0.5469517458538167, learning rate: 0.00019082000000000002\n",
      "Epoch 460, Loss: 0.5469501308595054, learning rate: 0.0001908\n",
      "Epoch 461, Loss: 0.5469485138683944, learning rate: 0.00019078\n",
      "Epoch 462, Loss: 0.5469468973100264, learning rate: 0.00019076\n",
      "Epoch 463, Loss: 0.5469452825942676, learning rate: 0.00019074\n",
      "Epoch 464, Loss: 0.5469436664990581, learning rate: 0.00019072000000000002\n",
      "Epoch 465, Loss: 0.5469420501280157, learning rate: 0.0001907\n",
      "Epoch 466, Loss: 0.5469404344521752, learning rate: 0.00019068\n",
      "Epoch 467, Loss: 0.5469388212840289, learning rate: 0.00019066000000000002\n",
      "Epoch 468, Loss: 0.5469372035997109, learning rate: 0.00019064\n",
      "Epoch 469, Loss: 0.5469355899632227, learning rate: 0.00019062000000000001\n",
      "Epoch 470, Loss: 0.5469339762268984, learning rate: 0.0001906\n",
      "Epoch 471, Loss: 0.5469323612226198, learning rate: 0.00019058\n",
      "Epoch 472, Loss: 0.5469307500719253, learning rate: 0.00019056000000000002\n",
      "Epoch 473, Loss: 0.5469291313924359, learning rate: 0.00019054\n",
      "Epoch 474, Loss: 0.5469275167202212, learning rate: 0.00019052\n",
      "Epoch 475, Loss: 0.5469259053267355, learning rate: 0.00019050000000000002\n",
      "Epoch 476, Loss: 0.5469242934216753, learning rate: 0.00019048\n",
      "Epoch 477, Loss: 0.5469226750221249, learning rate: 0.00019046000000000002\n",
      "Epoch 478, Loss: 0.5469210675611516, learning rate: 0.00019044\n",
      "Epoch 479, Loss: 0.5469194542185029, learning rate: 0.00019042\n",
      "Epoch 480, Loss: 0.5469178364305526, learning rate: 0.00019040000000000002\n",
      "Epoch 481, Loss: 0.54691622890962, learning rate: 0.00019038\n",
      "Epoch 482, Loss: 0.5469146126281799, learning rate: 0.00019036000000000001\n",
      "Epoch 483, Loss: 0.5469130040039575, learning rate: 0.00019034\n",
      "Epoch 484, Loss: 0.5469113876254741, learning rate: 0.00019032\n",
      "Epoch 485, Loss: 0.5469097758823293, learning rate: 0.00019030000000000002\n",
      "Epoch 486, Loss: 0.5469081680114891, learning rate: 0.00019028\n",
      "Epoch 487, Loss: 0.546906552161928, learning rate: 0.00019026\n",
      "Epoch 488, Loss: 0.5469049410246932, learning rate: 0.00019024\n",
      "Epoch 489, Loss: 0.5469033315535631, learning rate: 0.00019022\n",
      "Epoch 490, Loss: 0.5469017187741146, learning rate: 0.00019020000000000002\n",
      "Epoch 491, Loss: 0.5469001070450599, learning rate: 0.00019018\n",
      "Epoch 492, Loss: 0.5468984986360933, learning rate: 0.00019016\n",
      "Epoch 493, Loss: 0.5468968857068119, learning rate: 0.00019014000000000002\n",
      "Epoch 494, Loss: 0.5468952768571972, learning rate: 0.00019012\n",
      "Epoch 495, Loss: 0.5468936662226895, learning rate: 0.0001901\n",
      "Epoch 496, Loss: 0.5468920532648742, learning rate: 0.00019008\n",
      "Epoch 497, Loss: 0.5468904486586146, learning rate: 0.00019006\n",
      "Epoch 498, Loss: 0.5468888327065795, learning rate: 0.00019004000000000002\n",
      "Epoch 499, Loss: 0.5468872257148923, learning rate: 0.00019002\n",
      "Epoch 500, Loss: 0.5468856167482689, learning rate: 0.00019\n",
      "Epoch 501, Loss: 0.5468840050574479, learning rate: 0.00018998000000000002\n",
      "Epoch 502, Loss: 0.5468824006086404, learning rate: 0.00018996\n",
      "Epoch 503, Loss: 0.5468807870614257, learning rate: 0.00018994000000000001\n",
      "Epoch 504, Loss: 0.5468791782871288, learning rate: 0.00018992\n",
      "Epoch 505, Loss: 0.546877570700359, learning rate: 0.0001899\n",
      "Epoch 506, Loss: 0.5468759604349631, learning rate: 0.00018988000000000002\n",
      "Epoch 507, Loss: 0.5468743553373999, learning rate: 0.00018986\n",
      "Epoch 508, Loss: 0.546872745389831, learning rate: 0.00018984\n",
      "Epoch 509, Loss: 0.5468711362886093, learning rate: 0.00018982\n",
      "Epoch 510, Loss: 0.5468695332583045, learning rate: 0.0001898\n",
      "Epoch 511, Loss: 0.5468679204845568, learning rate: 0.00018978000000000002\n",
      "Epoch 512, Loss: 0.5468663134072423, learning rate: 0.00018976\n",
      "Epoch 513, Loss: 0.5468647074171762, learning rate: 0.00018974\n",
      "Epoch 514, Loss: 0.5468630992566319, learning rate: 0.00018972000000000002\n",
      "Epoch 515, Loss: 0.546861495175533, learning rate: 0.0001897\n",
      "Epoch 516, Loss: 0.5468598866867969, learning rate: 0.00018968\n",
      "Epoch 517, Loss: 0.5468582807319748, learning rate: 0.00018966\n",
      "Epoch 518, Loss: 0.546856675598949, learning rate: 0.00018964\n",
      "Epoch 519, Loss: 0.5468550681014259, learning rate: 0.00018962000000000002\n",
      "Epoch 520, Loss: 0.546853466786473, learning rate: 0.0001896\n",
      "Epoch 521, Loss: 0.5468518555177626, learning rate: 0.00018958\n",
      "Epoch 522, Loss: 0.5468502515827544, learning rate: 0.00018956000000000002\n",
      "Epoch 523, Loss: 0.5468486489429618, learning rate: 0.00018954\n",
      "Epoch 524, Loss: 0.5468470425578953, learning rate: 0.00018952000000000002\n",
      "Epoch 525, Loss: 0.5468454378509521, learning rate: 0.0001895\n",
      "Epoch 526, Loss: 0.546843834613546, learning rate: 0.00018948\n",
      "Epoch 527, Loss: 0.5468422274421585, learning rate: 0.00018946000000000002\n",
      "Epoch 528, Loss: 0.5468406245979428, learning rate: 0.00018944\n",
      "Epoch 529, Loss: 0.5468390193723902, learning rate: 0.00018942\n",
      "Epoch 530, Loss: 0.546837417714853, learning rate: 0.0001894\n",
      "Epoch 531, Loss: 0.5468358137220501, learning rate: 0.00018938\n",
      "Epoch 532, Loss: 0.5468342085016381, learning rate: 0.00018936000000000002\n",
      "Epoch 533, Loss: 0.546832607699514, learning rate: 0.00018934\n",
      "Epoch 534, Loss: 0.546831000908761, learning rate: 0.00018932\n",
      "Epoch 535, Loss: 0.5468294031399662, learning rate: 0.00018930000000000002\n",
      "Epoch 536, Loss: 0.5468277996266958, learning rate: 0.00018928\n",
      "Epoch 537, Loss: 0.5468261928525615, learning rate: 0.00018926000000000001\n",
      "Epoch 538, Loss: 0.5468245923237096, learning rate: 0.00018924\n",
      "Epoch 539, Loss: 0.5468229917150043, learning rate: 0.00018922\n",
      "Epoch 540, Loss: 0.5468213870670663, learning rate: 0.00018920000000000002\n",
      "Epoch 541, Loss: 0.5468197885362034, learning rate: 0.00018918\n",
      "Epoch 542, Loss: 0.5468181835636398, learning rate: 0.00018916\n",
      "Epoch 543, Loss: 0.5468165827427892, learning rate: 0.00018914000000000002\n",
      "Epoch 544, Loss: 0.5468149837500257, learning rate: 0.00018912\n",
      "Epoch 545, Loss: 0.546813379422042, learning rate: 0.00018910000000000002\n",
      "Epoch 546, Loss: 0.5468117831910084, learning rate: 0.00018908\n",
      "Epoch 547, Loss: 0.546810178093809, learning rate: 0.00018906\n",
      "Epoch 548, Loss: 0.5468085776810482, learning rate: 0.00018904000000000002\n",
      "Epoch 549, Loss: 0.5468069800868931, learning rate: 0.00018902\n",
      "Epoch 550, Loss: 0.5468053758002852, learning rate: 0.000189\n",
      "Epoch 551, Loss: 0.5468037796766011, learning rate: 0.00018898\n",
      "Epoch 552, Loss: 0.5468021792161979, learning rate: 0.00018896\n",
      "Epoch 553, Loss: 0.5468005768734334, learning rate: 0.00018894000000000002\n",
      "Epoch 554, Loss: 0.5467989805904409, learning rate: 0.00018892\n",
      "Epoch 555, Loss: 0.5467973813077979, learning rate: 0.0001889\n",
      "Epoch 556, Loss: 0.5467957811429812, learning rate: 0.00018888000000000002\n",
      "Epoch 557, Loss: 0.5467941849228224, learning rate: 0.00018886\n",
      "Epoch 558, Loss: 0.5467925831583753, learning rate: 0.00018884000000000001\n",
      "Epoch 559, Loss: 0.5467909874243971, learning rate: 0.00018882\n",
      "Epoch 560, Loss: 0.5467893879374522, learning rate: 0.0001888\n",
      "Epoch 561, Loss: 0.5467877906325774, learning rate: 0.00018878000000000002\n",
      "Epoch 562, Loss: 0.5467861915520993, learning rate: 0.00018876\n",
      "Epoch 563, Loss: 0.5467845968461228, learning rate: 0.00018874\n",
      "Epoch 564, Loss: 0.5467829973832057, learning rate: 0.00018872000000000002\n",
      "Epoch 565, Loss: 0.5467813990258059, learning rate: 0.0001887\n",
      "Epoch 566, Loss: 0.5467798041151952, learning rate: 0.00018868000000000002\n",
      "Epoch 567, Loss: 0.5467782061189829, learning rate: 0.00018866\n",
      "Epoch 568, Loss: 0.5467766103323899, learning rate: 0.00018864\n",
      "Epoch 569, Loss: 0.5467750143800181, learning rate: 0.00018862000000000002\n",
      "Epoch 570, Loss: 0.5467734173979969, learning rate: 0.0001886\n",
      "Epoch 571, Loss: 0.5467718225733325, learning rate: 0.00018858000000000001\n",
      "Epoch 572, Loss: 0.5467702278569077, learning rate: 0.00018856\n",
      "Epoch 573, Loss: 0.5467686309503906, learning rate: 0.00018854\n",
      "Epoch 574, Loss: 0.5467670353647225, learning rate: 0.00018852000000000002\n",
      "Epoch 575, Loss: 0.5467654425385018, learning rate: 0.0001885\n",
      "Epoch 576, Loss: 0.546763844610794, learning rate: 0.00018848\n",
      "Epoch 577, Loss: 0.546762249724159, learning rate: 0.00018846000000000002\n",
      "Epoch 578, Loss: 0.5467606599398959, learning rate: 0.00018844\n",
      "Epoch 579, Loss: 0.5467590622794547, learning rate: 0.00018842000000000002\n",
      "Epoch 580, Loss: 0.5467574672635693, learning rate: 0.0001884\n",
      "Epoch 581, Loss: 0.5467558764134121, learning rate: 0.00018838\n",
      "Epoch 582, Loss: 0.5467542804149055, learning rate: 0.00018836000000000002\n",
      "Epoch 583, Loss: 0.5467526883524368, learning rate: 0.00018834\n",
      "Epoch 584, Loss: 0.546751098750706, learning rate: 0.00018832\n",
      "Epoch 585, Loss: 0.5467495006133392, learning rate: 0.0001883\n",
      "Epoch 586, Loss: 0.5467479103444602, learning rate: 0.00018828\n",
      "Epoch 587, Loss: 0.5467463186669383, learning rate: 0.00018826000000000002\n",
      "Epoch 588, Loss: 0.5467447232739939, learning rate: 0.00018824\n",
      "Epoch 589, Loss: 0.5467431326342982, learning rate: 0.00018822\n",
      "Epoch 590, Loss: 0.5467415418863019, learning rate: 0.00018820000000000002\n",
      "Epoch 591, Loss: 0.5467399474937439, learning rate: 0.00018818\n",
      "Epoch 592, Loss: 0.5467383576339769, learning rate: 0.00018816000000000001\n",
      "Epoch 593, Loss: 0.5467367657524089, learning rate: 0.00018814\n",
      "Epoch 594, Loss: 0.5467351744226251, learning rate: 0.00018812\n",
      "Epoch 595, Loss: 0.5467335837762943, learning rate: 0.00018810000000000002\n",
      "Epoch 596, Loss: 0.5467319930102886, learning rate: 0.00018808\n",
      "Epoch 597, Loss: 0.5467304005157597, learning rate: 0.00018806\n",
      "Epoch 598, Loss: 0.5467288113955182, learning rate: 0.00018804\n",
      "Epoch 599, Loss: 0.54672722120414, learning rate: 0.00018802\n",
      "Epoch 600, Loss: 0.5467256334811833, learning rate: 0.00018800000000000002\n",
      "Epoch 601, Loss: 0.5467240402087699, learning rate: 0.00018798\n",
      "Epoch 602, Loss: 0.546722449838144, learning rate: 0.00018796\n",
      "Epoch 603, Loss: 0.546720864385552, learning rate: 0.00018794000000000002\n",
      "Epoch 604, Loss: 0.5467192716922975, learning rate: 0.00018792\n",
      "Epoch 605, Loss: 0.5467176820321228, learning rate: 0.00018790000000000001\n",
      "Epoch 606, Loss: 0.5467160966051237, learning rate: 0.00018788\n",
      "Epoch 607, Loss: 0.5467145034487447, learning rate: 0.00018786\n",
      "Epoch 608, Loss: 0.5467129168199626, learning rate: 0.00018784000000000002\n",
      "Epoch 609, Loss: 0.5467113309302525, learning rate: 0.00018782\n",
      "Epoch 610, Loss: 0.5467097379659087, learning rate: 0.0001878\n",
      "Epoch 611, Loss: 0.5467081527913284, learning rate: 0.00018778000000000002\n",
      "Epoch 612, Loss: 0.5467065660323205, learning rate: 0.00018776\n",
      "Epoch 613, Loss: 0.5467049754729695, learning rate: 0.00018774000000000002\n",
      "Epoch 614, Loss: 0.5467033878098025, learning rate: 0.00018772\n",
      "Epoch 615, Loss: 0.5467018036587228, learning rate: 0.0001877\n",
      "Epoch 616, Loss: 0.5467002136490647, learning rate: 0.00018768000000000002\n",
      "Epoch 617, Loss: 0.5466986247859972, learning rate: 0.00018766\n",
      "Epoch 618, Loss: 0.5466970430910897, learning rate: 0.00018764\n",
      "Epoch 619, Loss: 0.5466954522595863, learning rate: 0.00018762\n",
      "Epoch 620, Loss: 0.5466938657287161, learning rate: 0.0001876\n",
      "Epoch 621, Loss: 0.5466922840480455, learning rate: 0.00018758000000000002\n",
      "Epoch 622, Loss: 0.5466906929263214, learning rate: 0.00018756\n",
      "Epoch 623, Loss: 0.5466891090800591, learning rate: 0.00018754\n",
      "Epoch 624, Loss: 0.546687525828966, learning rate: 0.00018752000000000002\n",
      "Epoch 625, Loss: 0.5466859369613133, learning rate: 0.0001875\n",
      "Epoch 626, Loss: 0.5466843513420183, learning rate: 0.00018748000000000001\n",
      "Epoch 627, Loss: 0.5466827690765855, learning rate: 0.00018746\n",
      "Epoch 628, Loss: 0.5466811816991505, learning rate: 0.00018744\n",
      "Epoch 629, Loss: 0.5466795952415192, learning rate: 0.00018742000000000002\n",
      "Epoch 630, Loss: 0.5466780120800508, learning rate: 0.0001874\n",
      "Epoch 631, Loss: 0.546676428183733, learning rate: 0.00018738\n",
      "Epoch 632, Loss: 0.5466748402290423, learning rate: 0.00018736000000000002\n",
      "Epoch 633, Loss: 0.5466732580461918, learning rate: 0.00018734\n",
      "Epoch 634, Loss: 0.5466716746183924, learning rate: 0.00018732000000000002\n",
      "Epoch 635, Loss: 0.5466700889784414, learning rate: 0.0001873\n",
      "Epoch 636, Loss: 0.5466685041309356, learning rate: 0.00018728\n",
      "Epoch 637, Loss: 0.5466669168358105, learning rate: 0.00018726000000000002\n",
      "Epoch 638, Loss: 0.5466653335689862, learning rate: 0.00018724\n",
      "Epoch 639, Loss: 0.546663745448203, learning rate: 0.00018722\n",
      "Epoch 640, Loss: 0.5466621617362535, learning rate: 0.0001872\n",
      "Epoch 641, Loss: 0.5466605765501672, learning rate: 0.00018718\n",
      "Epoch 642, Loss: 0.5466589927192834, learning rate: 0.00018716000000000002\n",
      "Epoch 643, Loss: 0.5466574064824498, learning rate: 0.00018714\n",
      "Epoch 644, Loss: 0.5466558221313529, learning rate: 0.00018712\n",
      "Epoch 645, Loss: 0.5466542393839474, learning rate: 0.00018710000000000002\n",
      "Epoch 646, Loss: 0.5466526558287239, learning rate: 0.00018708\n",
      "Epoch 647, Loss: 0.5466510722510387, learning rate: 0.00018706000000000001\n",
      "Epoch 648, Loss: 0.546649486876075, learning rate: 0.00018704\n",
      "Epoch 649, Loss: 0.5466479051693112, learning rate: 0.00018702\n",
      "Epoch 650, Loss: 0.546646321438894, learning rate: 0.00018700000000000002\n",
      "Epoch 651, Loss: 0.5466447403816163, learning rate: 0.00018698\n",
      "Epoch 652, Loss: 0.5466431560065922, learning rate: 0.00018696\n",
      "Epoch 653, Loss: 0.5466415752470192, learning rate: 0.00018694000000000002\n",
      "Epoch 654, Loss: 0.546639994844972, learning rate: 0.00018692\n",
      "Epoch 655, Loss: 0.5466384104724613, learning rate: 0.00018690000000000002\n",
      "Epoch 656, Loss: 0.5466368323754514, learning rate: 0.00018688\n",
      "Epoch 657, Loss: 0.5466352486396215, learning rate: 0.00018686\n",
      "Epoch 658, Loss: 0.5466336683525292, learning rate: 0.00018684000000000002\n",
      "Epoch 659, Loss: 0.5466320890894075, learning rate: 0.00018682\n",
      "Epoch 660, Loss: 0.5466305048711405, learning rate: 0.00018680000000000001\n",
      "Epoch 661, Loss: 0.5466289271206908, learning rate: 0.00018678\n",
      "Epoch 662, Loss: 0.5466273463516232, learning rate: 0.00018676\n",
      "Epoch 663, Loss: 0.5466257642292731, learning rate: 0.00018674000000000002\n",
      "Epoch 664, Loss: 0.5466241852803073, learning rate: 0.00018672\n",
      "Epoch 665, Loss: 0.5466226042523753, learning rate: 0.0001867\n",
      "Epoch 666, Loss: 0.5466210252669628, learning rate: 0.00018668\n",
      "Epoch 667, Loss: 0.5466194474135546, learning rate: 0.00018666\n",
      "Epoch 668, Loss: 0.5466178650599067, learning rate: 0.00018664000000000002\n",
      "Epoch 669, Loss: 0.5466162900482368, learning rate: 0.00018662\n",
      "Epoch 670, Loss: 0.5466147081286629, learning rate: 0.0001866\n",
      "Epoch 671, Loss: 0.5466131300340706, learning rate: 0.00018658000000000002\n",
      "Epoch 672, Loss: 0.546611551797175, learning rate: 0.00018656\n",
      "Epoch 673, Loss: 0.546609972618785, learning rate: 0.00018654\n",
      "Epoch 674, Loss: 0.5466083941985408, learning rate: 0.00018652000000000002\n",
      "Epoch 675, Loss: 0.5466068177655132, learning rate: 0.0001865\n",
      "Epoch 676, Loss: 0.5466052372327395, learning rate: 0.00018648000000000002\n",
      "Epoch 677, Loss: 0.5466036621426281, learning rate: 0.00018646\n",
      "Epoch 678, Loss: 0.5466020842204408, learning rate: 0.00018644\n",
      "Epoch 679, Loss: 0.5466005047901157, learning rate: 0.00018642000000000002\n",
      "Epoch 680, Loss: 0.5465989289592773, learning rate: 0.0001864\n",
      "Epoch 681, Loss: 0.5465973535971322, learning rate: 0.00018638000000000001\n",
      "Epoch 682, Loss: 0.546595773728471, learning rate: 0.00018636\n",
      "Epoch 683, Loss: 0.546594200876523, learning rate: 0.00018634\n",
      "Epoch 684, Loss: 0.5465926222379445, learning rate: 0.00018632000000000002\n",
      "Epoch 685, Loss: 0.5465910449148422, learning rate: 0.0001863\n",
      "Epoch 686, Loss: 0.546589471635034, learning rate: 0.00018628\n",
      "Epoch 687, Loss: 0.5465878953078359, learning rate: 0.00018626000000000002\n",
      "Epoch 688, Loss: 0.5465863175787146, learning rate: 0.00018624\n",
      "Epoch 689, Loss: 0.5465847449081596, learning rate: 0.00018622000000000002\n",
      "Epoch 690, Loss: 0.5465831676584362, learning rate: 0.0001862\n",
      "Epoch 691, Loss: 0.5465815914231031, learning rate: 0.00018618\n",
      "Epoch 692, Loss: 0.546580018216291, learning rate: 0.00018616000000000002\n",
      "Epoch 693, Loss: 0.5465784445587377, learning rate: 0.00018614\n",
      "Epoch 694, Loss: 0.5465768666042312, learning rate: 0.00018612000000000001\n",
      "Epoch 695, Loss: 0.5465752922537398, learning rate: 0.0001861\n",
      "Epoch 696, Loss: 0.5465737216889578, learning rate: 0.00018608\n",
      "Epoch 697, Loss: 0.5465721441970517, learning rate: 0.00018606000000000002\n",
      "Epoch 698, Loss: 0.5465705703358898, learning rate: 0.00018604\n",
      "Epoch 699, Loss: 0.5465689979040894, learning rate: 0.00018602\n",
      "Epoch 700, Loss: 0.5465674254304781, learning rate: 0.00018600000000000002\n",
      "Epoch 701, Loss: 0.5465658498097203, learning rate: 0.00018598\n",
      "Epoch 702, Loss: 0.5465642765133207, learning rate: 0.00018596000000000002\n",
      "Epoch 703, Loss: 0.5465627035205103, learning rate: 0.00018594\n",
      "Epoch 704, Loss: 0.5465611311332177, learning rate: 0.00018592\n",
      "Epoch 705, Loss: 0.5465595592020885, learning rate: 0.00018590000000000002\n",
      "Epoch 706, Loss: 0.5465579871838726, learning rate: 0.00018588\n",
      "Epoch 707, Loss: 0.5465564124520037, learning rate: 0.00018586\n",
      "Epoch 708, Loss: 0.5465548414100512, learning rate: 0.00018584\n",
      "Epoch 709, Loss: 0.5465532697072825, learning rate: 0.00018582\n",
      "Epoch 710, Loss: 0.5465516965986651, learning rate: 0.00018580000000000002\n",
      "Epoch 711, Loss: 0.5465501268305284, learning rate: 0.00018578\n",
      "Epoch 712, Loss: 0.5465485572908025, learning rate: 0.00018576\n",
      "Epoch 713, Loss: 0.546546982932185, learning rate: 0.00018574000000000002\n",
      "Epoch 714, Loss: 0.5465454103380598, learning rate: 0.00018572\n",
      "Epoch 715, Loss: 0.5465438439366513, learning rate: 0.00018570000000000001\n",
      "Epoch 716, Loss: 0.5465422694006359, learning rate: 0.00018568\n",
      "Epoch 717, Loss: 0.5465406983379845, learning rate: 0.00018566\n",
      "Epoch 718, Loss: 0.5465391294667168, learning rate: 0.00018564000000000002\n",
      "Epoch 719, Loss: 0.5465375605331617, learning rate: 0.00018562\n",
      "Epoch 720, Loss: 0.5465359863780123, learning rate: 0.0001856\n",
      "Epoch 721, Loss: 0.5465344187663552, learning rate: 0.00018558000000000002\n",
      "Epoch 722, Loss: 0.5465328505281062, learning rate: 0.00018556\n",
      "Epoch 723, Loss: 0.5465312783067457, learning rate: 0.00018554000000000002\n",
      "Epoch 724, Loss: 0.5465297078750343, learning rate: 0.00018552\n",
      "Epoch 725, Loss: 0.5465281400129123, learning rate: 0.0001855\n",
      "Epoch 726, Loss: 0.5465265714969634, learning rate: 0.00018548000000000002\n",
      "Epoch 727, Loss: 0.546525000664521, learning rate: 0.00018546\n",
      "Epoch 728, Loss: 0.5465234349525955, learning rate: 0.00018544\n",
      "Epoch 729, Loss: 0.5465218663032417, learning rate: 0.00018542\n",
      "Epoch 730, Loss: 0.5465202972979174, learning rate: 0.0001854\n",
      "Epoch 731, Loss: 0.5465187276344554, learning rate: 0.00018538000000000002\n",
      "Epoch 732, Loss: 0.5465171601442249, learning rate: 0.00018536\n",
      "Epoch 733, Loss: 0.5465155934103586, learning rate: 0.00018534\n",
      "Epoch 734, Loss: 0.5465140253625232, learning rate: 0.00018532\n",
      "Epoch 735, Loss: 0.5465124568908635, learning rate: 0.0001853\n",
      "Epoch 736, Loss: 0.5465108916871995, learning rate: 0.00018528000000000001\n",
      "Epoch 737, Loss: 0.5465093238985654, learning rate: 0.00018526\n",
      "Epoch 738, Loss: 0.5465077554609915, learning rate: 0.00018524\n",
      "Epoch 739, Loss: 0.5465061920250495, learning rate: 0.00018522000000000002\n",
      "Epoch 740, Loss: 0.5465046235953337, learning rate: 0.0001852\n",
      "Epoch 741, Loss: 0.5465030560539387, learning rate: 0.00018518\n",
      "Epoch 742, Loss: 0.5465014928786917, learning rate: 0.00018516000000000002\n",
      "Epoch 743, Loss: 0.5464999286245479, learning rate: 0.00018514\n",
      "Epoch 744, Loss: 0.5464983597435781, learning rate: 0.00018512000000000002\n",
      "Epoch 745, Loss: 0.5464967930184469, learning rate: 0.0001851\n",
      "Epoch 746, Loss: 0.5464952317414031, learning rate: 0.00018508\n",
      "Epoch 747, Loss: 0.5464936641735819, learning rate: 0.00018506000000000002\n",
      "Epoch 748, Loss: 0.5464920988256562, learning rate: 0.00018504\n",
      "Epoch 749, Loss: 0.5464905344159031, learning rate: 0.00018502000000000001\n",
      "Epoch 750, Loss: 0.5464889706679166, learning rate: 0.000185\n",
      "Epoch 751, Loss: 0.5464874048972761, learning rate: 0.00018498\n",
      "Epoch 752, Loss: 0.5464858395729071, learning rate: 0.00018496000000000002\n",
      "Epoch 753, Loss: 0.546484276654791, learning rate: 0.00018494\n",
      "Epoch 754, Loss: 0.5464827153681302, learning rate: 0.00018492\n",
      "Epoch 755, Loss: 0.5464811489617866, learning rate: 0.00018490000000000002\n",
      "Epoch 756, Loss: 0.54647958446368, learning rate: 0.00018488\n",
      "Epoch 757, Loss: 0.5464780241602526, learning rate: 0.00018486000000000002\n",
      "Epoch 758, Loss: 0.5464764584192079, learning rate: 0.00018484\n",
      "Epoch 759, Loss: 0.5464748955823778, learning rate: 0.00018482\n",
      "Epoch 760, Loss: 0.5464733332271521, learning rate: 0.00018480000000000002\n",
      "Epoch 761, Loss: 0.5464717705517783, learning rate: 0.00018478\n",
      "Epoch 762, Loss: 0.5464702093264506, learning rate: 0.00018476\n",
      "Epoch 763, Loss: 0.5464686466531665, learning rate: 0.00018474\n",
      "Epoch 764, Loss: 0.5464670837039304, learning rate: 0.00018472\n",
      "Epoch 765, Loss: 0.5464655236869502, learning rate: 0.00018470000000000002\n",
      "Epoch 766, Loss: 0.5464639610911406, learning rate: 0.00018468\n",
      "Epoch 767, Loss: 0.5464623998690239, learning rate: 0.00018466\n",
      "Epoch 768, Loss: 0.5464608403345447, learning rate: 0.00018464000000000002\n",
      "Epoch 769, Loss: 0.5464592775423338, learning rate: 0.00018462\n",
      "Epoch 770, Loss: 0.5464577155090956, learning rate: 0.00018460000000000001\n",
      "Epoch 771, Loss: 0.5464561567044458, learning rate: 0.00018458\n",
      "Epoch 772, Loss: 0.546454597017253, learning rate: 0.00018456\n",
      "Epoch 773, Loss: 0.5464530368200603, learning rate: 0.00018454000000000002\n",
      "Epoch 774, Loss: 0.5464514743475041, learning rate: 0.00018452\n",
      "Epoch 775, Loss: 0.5464499166712025, learning rate: 0.0001845\n",
      "Epoch 776, Loss: 0.5464483576313309, learning rate: 0.00018448\n",
      "Epoch 777, Loss: 0.5464467947999004, learning rate: 0.00018446\n",
      "Epoch 778, Loss: 0.5464452363840743, learning rate: 0.00018444000000000002\n",
      "Epoch 779, Loss: 0.5464436784397143, learning rate: 0.00018442\n",
      "Epoch 780, Loss: 0.5464421178271416, learning rate: 0.0001844\n",
      "Epoch 781, Loss: 0.5464405572168453, learning rate: 0.00018438000000000002\n",
      "Epoch 782, Loss: 0.546439000245657, learning rate: 0.00018436\n",
      "Epoch 783, Loss: 0.5464374439828601, learning rate: 0.00018434000000000001\n",
      "Epoch 784, Loss: 0.5464358825382166, learning rate: 0.00018432000000000002\n",
      "Epoch 785, Loss: 0.5464343235633533, learning rate: 0.0001843\n",
      "Epoch 786, Loss: 0.5464327661812357, learning rate: 0.00018428000000000002\n",
      "Epoch 787, Loss: 0.5464312096375178, learning rate: 0.00018426\n",
      "Epoch 788, Loss: 0.5464296498704468, learning rate: 0.00018424\n",
      "Epoch 789, Loss: 0.546428092489391, learning rate: 0.00018422000000000002\n",
      "Epoch 790, Loss: 0.5464265365598199, learning rate: 0.0001842\n",
      "Epoch 791, Loss: 0.5464249787237384, learning rate: 0.00018418000000000002\n",
      "Epoch 792, Loss: 0.5464234202822107, learning rate: 0.00018416\n",
      "Epoch 793, Loss: 0.546421863080575, learning rate: 0.00018414\n",
      "Epoch 794, Loss: 0.5464203085289749, learning rate: 0.00018412000000000002\n",
      "Epoch 795, Loss: 0.5464187529162718, learning rate: 0.0001841\n",
      "Epoch 796, Loss: 0.5464171948210688, learning rate: 0.00018408\n",
      "Epoch 797, Loss: 0.5464156379383269, learning rate: 0.00018406\n",
      "Epoch 798, Loss: 0.546414084372714, learning rate: 0.00018404\n",
      "Epoch 799, Loss: 0.546412528207061, learning rate: 0.00018402000000000002\n",
      "Epoch 800, Loss: 0.546410970817304, learning rate: 0.000184\n",
      "Epoch 801, Loss: 0.5464094168415788, learning rate: 0.00018398\n",
      "Epoch 802, Loss: 0.5464078627417495, learning rate: 0.00018396\n",
      "Epoch 803, Loss: 0.5464063062284205, learning rate: 0.00018394\n",
      "Epoch 804, Loss: 0.5464047510548234, learning rate: 0.00018392000000000001\n",
      "Epoch 805, Loss: 0.5464031957778875, learning rate: 0.0001839\n",
      "Epoch 806, Loss: 0.5464016422053228, learning rate: 0.00018388\n",
      "Epoch 807, Loss: 0.5464000884203001, learning rate: 0.00018386000000000002\n",
      "Epoch 808, Loss: 0.546398534899263, learning rate: 0.00018384\n",
      "Epoch 809, Loss: 0.5463969799414057, learning rate: 0.00018382\n",
      "Epoch 810, Loss: 0.5463954267494552, learning rate: 0.00018380000000000002\n",
      "Epoch 811, Loss: 0.5463938754664114, learning rate: 0.00018378\n",
      "Epoch 812, Loss: 0.5463923207990928, learning rate: 0.00018376000000000002\n",
      "Epoch 813, Loss: 0.5463907662121796, learning rate: 0.00018374\n",
      "Epoch 814, Loss: 0.5463892133189738, learning rate: 0.00018372\n",
      "Epoch 815, Loss: 0.5463876646926223, learning rate: 0.00018370000000000002\n",
      "Epoch 816, Loss: 0.5463861090654529, learning rate: 0.00018368\n",
      "Epoch 817, Loss: 0.5463845561278324, learning rate: 0.00018366000000000001\n",
      "Epoch 818, Loss: 0.5463830038771286, learning rate: 0.00018364\n",
      "Epoch 819, Loss: 0.5463814542155983, learning rate: 0.00018362\n",
      "Epoch 820, Loss: 0.5463799022797744, learning rate: 0.00018360000000000002\n",
      "Epoch 821, Loss: 0.5463783478999354, learning rate: 0.00018358\n",
      "Epoch 822, Loss: 0.5463767983349423, learning rate: 0.00018356\n",
      "Epoch 823, Loss: 0.5463752461646961, learning rate: 0.00018354000000000002\n",
      "Epoch 824, Loss: 0.5463736966134988, learning rate: 0.00018352\n",
      "Epoch 825, Loss: 0.5463721440163198, learning rate: 0.00018350000000000002\n",
      "Epoch 826, Loss: 0.5463705948404367, learning rate: 0.00018348\n",
      "Epoch 827, Loss: 0.5463690427491507, learning rate: 0.00018346\n",
      "Epoch 828, Loss: 0.5463674933759096, learning rate: 0.00018344000000000002\n",
      "Epoch 829, Loss: 0.5463659441264028, learning rate: 0.00018342\n",
      "Epoch 830, Loss: 0.5463643925025603, learning rate: 0.0001834\n",
      "Epoch 831, Loss: 0.5463628431504239, learning rate: 0.00018338000000000002\n",
      "Epoch 832, Loss: 0.5463612925677197, learning rate: 0.00018336\n",
      "Epoch 833, Loss: 0.5463597448559117, learning rate: 0.00018334000000000002\n",
      "Epoch 834, Loss: 0.546358194500808, learning rate: 0.00018332\n",
      "Epoch 835, Loss: 0.5463566458355418, learning rate: 0.0001833\n",
      "Epoch 836, Loss: 0.5463550950577498, learning rate: 0.00018328000000000002\n",
      "Epoch 837, Loss: 0.5463535516115862, learning rate: 0.00018326\n",
      "Epoch 838, Loss: 0.5463519997159344, learning rate: 0.00018324000000000001\n",
      "Epoch 839, Loss: 0.5463504501086371, learning rate: 0.00018322\n",
      "Epoch 840, Loss: 0.5463489047505967, learning rate: 0.0001832\n",
      "Epoch 841, Loss: 0.5463473610617758, learning rate: 0.00018318000000000002\n",
      "Epoch 842, Loss: 0.5463458089703744, learning rate: 0.00018316\n",
      "Epoch 843, Loss: 0.5463442600023591, learning rate: 0.00018314\n",
      "Epoch 844, Loss: 0.5463427109301258, learning rate: 0.00018312\n",
      "Epoch 845, Loss: 0.5463411678378152, learning rate: 0.0001831\n",
      "Epoch 846, Loss: 0.5463396170922662, learning rate: 0.00018308000000000002\n",
      "Epoch 847, Loss: 0.5463380713855059, learning rate: 0.00018306\n",
      "Epoch 848, Loss: 0.5463365298093044, learning rate: 0.00018304\n",
      "Epoch 849, Loss: 0.5463349773797854, learning rate: 0.00018302000000000002\n",
      "Epoch 850, Loss: 0.546333431608154, learning rate: 0.000183\n",
      "Epoch 851, Loss: 0.5463318848217805, learning rate: 0.00018298\n",
      "Epoch 852, Loss: 0.5463303427107674, learning rate: 0.00018296000000000002\n",
      "Epoch 853, Loss: 0.5463287932559353, learning rate: 0.00018294\n",
      "Epoch 854, Loss: 0.5463272493062912, learning rate: 0.00018292000000000002\n",
      "Epoch 855, Loss: 0.5463257030660031, learning rate: 0.0001829\n",
      "Epoch 856, Loss: 0.546324158443382, learning rate: 0.00018288\n",
      "Epoch 857, Loss: 0.5463226143963036, learning rate: 0.00018286000000000002\n",
      "Epoch 858, Loss: 0.5463210680349042, learning rate: 0.00018284\n",
      "Epoch 859, Loss: 0.5463195250843824, learning rate: 0.00018282000000000001\n",
      "Epoch 860, Loss: 0.5463179809838323, learning rate: 0.0001828\n",
      "Epoch 861, Loss: 0.5463164411515173, learning rate: 0.00018278\n",
      "Epoch 862, Loss: 0.5463148920278951, learning rate: 0.00018276000000000002\n",
      "Epoch 863, Loss: 0.5463133486421671, learning rate: 0.00018274\n",
      "Epoch 864, Loss: 0.5463118066791544, learning rate: 0.00018272\n",
      "Epoch 865, Loss: 0.5463102646170575, learning rate: 0.0001827\n",
      "Epoch 866, Loss: 0.5463087193909281, learning rate: 0.00018268\n",
      "Epoch 867, Loss: 0.5463071795274701, learning rate: 0.00018266000000000002\n",
      "Epoch 868, Loss: 0.5463056382295484, learning rate: 0.00018264\n",
      "Epoch 869, Loss: 0.5463041001863246, learning rate: 0.00018262\n",
      "Epoch 870, Loss: 0.5463025514977601, learning rate: 0.0001826\n",
      "Epoch 871, Loss: 0.5463010105421098, learning rate: 0.00018258\n",
      "Epoch 872, Loss: 0.5462994712140611, learning rate: 0.00018256000000000001\n",
      "Epoch 873, Loss: 0.5462979264420159, learning rate: 0.00018254000000000002\n",
      "Epoch 874, Loss: 0.5462963896902809, learning rate: 0.00018252\n",
      "Epoch 875, Loss: 0.5462948453090914, learning rate: 0.00018250000000000002\n",
      "Epoch 876, Loss: 0.546293307078139, learning rate: 0.00018248\n",
      "Epoch 877, Loss: 0.5462917633067522, learning rate: 0.00018246\n",
      "Epoch 878, Loss: 0.5462902233348188, learning rate: 0.00018244000000000002\n",
      "Epoch 879, Loss: 0.5462886860756198, learning rate: 0.00018242\n",
      "Epoch 880, Loss: 0.5462871449650681, learning rate: 0.00018240000000000002\n",
      "Epoch 881, Loss: 0.5462856090770875, learning rate: 0.00018238\n",
      "Epoch 882, Loss: 0.5462840631810002, learning rate: 0.00018236\n",
      "Epoch 883, Loss: 0.5462825244579416, learning rate: 0.00018234000000000002\n",
      "Epoch 884, Loss: 0.5462809883247071, learning rate: 0.00018232\n",
      "Epoch 885, Loss: 0.5462794463370396, learning rate: 0.0001823\n",
      "Epoch 886, Loss: 0.5462779090110852, learning rate: 0.00018228\n",
      "Epoch 887, Loss: 0.5462763690070546, learning rate: 0.00018226\n",
      "Epoch 888, Loss: 0.546274831003713, learning rate: 0.00018224000000000002\n",
      "Epoch 889, Loss: 0.5462732920827224, learning rate: 0.00018222\n",
      "Epoch 890, Loss: 0.5462717549812373, learning rate: 0.0001822\n",
      "Epoch 891, Loss: 0.5462702166405714, learning rate: 0.00018218000000000002\n",
      "Epoch 892, Loss: 0.5462686844219637, learning rate: 0.00018216\n",
      "Epoch 893, Loss: 0.5462671392860449, learning rate: 0.00018214000000000001\n",
      "Epoch 894, Loss: 0.5462656044469383, learning rate: 0.00018212\n",
      "Epoch 895, Loss: 0.5462640669164012, learning rate: 0.0001821\n",
      "Epoch 896, Loss: 0.5462625286777891, learning rate: 0.00018208000000000002\n",
      "Epoch 897, Loss: 0.5462609945794396, learning rate: 0.00018206\n",
      "Epoch 898, Loss: 0.5462594546599878, learning rate: 0.00018204\n",
      "Epoch 899, Loss: 0.5462579213033799, learning rate: 0.00018202000000000002\n",
      "Epoch 900, Loss: 0.5462563823688575, learning rate: 0.000182\n",
      "Epoch 901, Loss: 0.5462548494557022, learning rate: 0.00018198000000000002\n",
      "Epoch 902, Loss: 0.5462533150303964, learning rate: 0.00018196\n",
      "Epoch 903, Loss: 0.546251783182883, learning rate: 0.00018194\n",
      "Epoch 904, Loss: 0.5462502421607778, learning rate: 0.00018192000000000002\n",
      "Epoch 905, Loss: 0.5462487113056922, learning rate: 0.0001819\n",
      "Epoch 906, Loss: 0.546247174194249, learning rate: 0.00018188000000000001\n",
      "Epoch 907, Loss: 0.5462456412258024, learning rate: 0.00018186\n",
      "Epoch 908, Loss: 0.546244105504121, learning rate: 0.00018184\n",
      "Epoch 909, Loss: 0.5462425778992843, learning rate: 0.00018182000000000002\n",
      "Epoch 910, Loss: 0.5462410373982829, learning rate: 0.0001818\n",
      "Epoch 911, Loss: 0.5462395060596814, learning rate: 0.00018178\n",
      "Epoch 912, Loss: 0.5462379710459995, learning rate: 0.00018176\n",
      "Epoch 913, Loss: 0.5462364361715616, learning rate: 0.00018174\n",
      "Epoch 914, Loss: 0.5462349049138688, learning rate: 0.00018172000000000002\n",
      "Epoch 915, Loss: 0.5462333696773634, learning rate: 0.0001817\n",
      "Epoch 916, Loss: 0.5462318392176813, learning rate: 0.00018168\n",
      "Epoch 917, Loss: 0.5462303103682246, learning rate: 0.00018166000000000002\n",
      "Epoch 918, Loss: 0.546228774310394, learning rate: 0.00018164\n",
      "Epoch 919, Loss: 0.5462272433938105, learning rate: 0.00018162\n",
      "Epoch 920, Loss: 0.5462257101231883, learning rate: 0.00018160000000000002\n",
      "Epoch 921, Loss: 0.5462241789678814, learning rate: 0.00018158\n",
      "Epoch 922, Loss: 0.5462226435973724, learning rate: 0.00018156000000000002\n",
      "Epoch 923, Loss: 0.5462211159911681, learning rate: 0.00018154\n",
      "Epoch 924, Loss: 0.5462195888103357, learning rate: 0.00018152\n",
      "Epoch 925, Loss: 0.5462180539209822, learning rate: 0.00018150000000000002\n",
      "Epoch 926, Loss: 0.5462165220826102, learning rate: 0.00018148\n",
      "Epoch 927, Loss: 0.546214989500332, learning rate: 0.00018146000000000001\n",
      "Epoch 928, Loss: 0.5462134629162708, learning rate: 0.00018144\n",
      "Epoch 929, Loss: 0.546211928354867, learning rate: 0.00018142\n",
      "Epoch 930, Loss: 0.5462104011799759, learning rate: 0.00018140000000000002\n",
      "Epoch 931, Loss: 0.546208871419045, learning rate: 0.00018138\n",
      "Epoch 932, Loss: 0.5462073435397186, learning rate: 0.00018136\n",
      "Epoch 933, Loss: 0.5462058096630392, learning rate: 0.00018134\n",
      "Epoch 934, Loss: 0.5462042817488255, learning rate: 0.00018132\n",
      "Epoch 935, Loss: 0.5462027506131407, learning rate: 0.00018130000000000002\n",
      "Epoch 936, Loss: 0.5462012244375107, learning rate: 0.00018128\n",
      "Epoch 937, Loss: 0.5461996921363277, learning rate: 0.00018126\n",
      "Epoch 938, Loss: 0.546198165516535, learning rate: 0.00018124\n",
      "Epoch 939, Loss: 0.5461966370819413, learning rate: 0.00018122\n",
      "Epoch 940, Loss: 0.5461951128983802, learning rate: 0.0001812\n",
      "Epoch 941, Loss: 0.5461935785232856, learning rate: 0.00018118000000000002\n",
      "Epoch 942, Loss: 0.5461920535269917, learning rate: 0.00018116\n",
      "Epoch 943, Loss: 0.5461905218901569, learning rate: 0.00018114000000000002\n",
      "Epoch 944, Loss: 0.5461889976012799, learning rate: 0.00018112\n",
      "Epoch 945, Loss: 0.5461874686070011, learning rate: 0.0001811\n",
      "Epoch 946, Loss: 0.5461859411427298, learning rate: 0.00018108000000000002\n",
      "Epoch 947, Loss: 0.5461844161619737, learning rate: 0.00018106\n",
      "Epoch 948, Loss: 0.5461828847342146, learning rate: 0.00018104000000000001\n",
      "Epoch 949, Loss: 0.5461813634259745, learning rate: 0.00018102\n",
      "Epoch 950, Loss: 0.5461798359645323, learning rate: 0.000181\n",
      "Epoch 951, Loss: 0.5461783119284502, learning rate: 0.00018098000000000002\n",
      "Epoch 952, Loss: 0.5461767809233216, learning rate: 0.00018096\n",
      "Epoch 953, Loss: 0.5461752564160706, learning rate: 0.00018094\n",
      "Epoch 954, Loss: 0.5461737296172622, learning rate: 0.00018092\n",
      "Epoch 955, Loss: 0.5461722042144961, learning rate: 0.0001809\n",
      "Epoch 956, Loss: 0.546170678081916, learning rate: 0.00018088000000000002\n",
      "Epoch 957, Loss: 0.5461691532398751, learning rate: 0.00018086\n",
      "Epoch 958, Loss: 0.5461676304433658, learning rate: 0.00018084\n",
      "Epoch 959, Loss: 0.5461661067364048, learning rate: 0.00018082000000000002\n",
      "Epoch 960, Loss: 0.5461645788357761, learning rate: 0.0001808\n",
      "Epoch 961, Loss: 0.5461630558182274, learning rate: 0.00018078000000000001\n",
      "Epoch 962, Loss: 0.5461615286464696, learning rate: 0.00018076\n",
      "Epoch 963, Loss: 0.5461600078551108, learning rate: 0.00018074\n",
      "Epoch 964, Loss: 0.5461584826464722, learning rate: 0.00018072000000000002\n",
      "Epoch 965, Loss: 0.5461569591824171, learning rate: 0.0001807\n",
      "Epoch 966, Loss: 0.5461554349191202, learning rate: 0.00018068\n",
      "Epoch 967, Loss: 0.546153915954809, learning rate: 0.00018066000000000002\n",
      "Epoch 968, Loss: 0.5461523868066123, learning rate: 0.00018064\n",
      "Epoch 969, Loss: 0.5461508649607173, learning rate: 0.00018062000000000002\n",
      "Epoch 970, Loss: 0.5461493410271985, learning rate: 0.0001806\n",
      "Epoch 971, Loss: 0.5461478199440407, learning rate: 0.00018058\n",
      "Epoch 972, Loss: 0.5461462957119726, learning rate: 0.00018056000000000002\n",
      "Epoch 973, Loss: 0.5461447740173709, learning rate: 0.00018054\n",
      "Epoch 974, Loss: 0.5461432542725211, learning rate: 0.00018052\n",
      "Epoch 975, Loss: 0.5461417288428734, learning rate: 0.0001805\n",
      "Epoch 976, Loss: 0.5461402089800369, learning rate: 0.00018048\n",
      "Epoch 977, Loss: 0.5461386945271747, learning rate: 0.00018046000000000002\n",
      "Epoch 978, Loss: 0.5461371641775131, learning rate: 0.00018044\n",
      "Epoch 979, Loss: 0.5461356453882809, learning rate: 0.00018042\n",
      "Epoch 980, Loss: 0.546134123520712, learning rate: 0.0001804\n",
      "Epoch 981, Loss: 0.5461326042778316, learning rate: 0.00018038\n",
      "Epoch 982, Loss: 0.5461310806479782, learning rate: 0.00018036000000000001\n",
      "Epoch 983, Loss: 0.5461295611561328, learning rate: 0.00018034000000000003\n",
      "Epoch 984, Loss: 0.5461280404600806, learning rate: 0.00018032\n",
      "Epoch 985, Loss: 0.5461265217398575, learning rate: 0.00018030000000000002\n",
      "Epoch 986, Loss: 0.5461250045831489, learning rate: 0.00018028\n",
      "Epoch 987, Loss: 0.5461234843640929, learning rate: 0.00018026\n",
      "Epoch 988, Loss: 0.5461219639269199, learning rate: 0.00018024000000000002\n",
      "Epoch 989, Loss: 0.5461204418558667, learning rate: 0.00018022\n",
      "Epoch 990, Loss: 0.546118926821924, learning rate: 0.00018020000000000002\n",
      "Epoch 991, Loss: 0.5461174046407308, learning rate: 0.00018018\n",
      "Epoch 992, Loss: 0.5461158876840111, learning rate: 0.00018016\n",
      "Epoch 993, Loss: 0.5461143671151996, learning rate: 0.00018014000000000002\n",
      "Epoch 994, Loss: 0.5461128491538934, learning rate: 0.00018012\n",
      "Epoch 995, Loss: 0.5461113300766912, learning rate: 0.00018010000000000001\n",
      "Epoch 996, Loss: 0.5461098121187142, learning rate: 0.00018008\n",
      "Epoch 997, Loss: 0.546108295650071, learning rate: 0.00018006\n",
      "Epoch 998, Loss: 0.5461067827517028, learning rate: 0.00018004000000000002\n",
      "Epoch 999, Loss: 0.5461052613319942, learning rate: 0.00018002\n",
      "Epoch 1000, Loss: 0.5461037430371584, learning rate: 0.00018\n",
      "Epoch 1001, Loss: 0.5461022252354151, learning rate: 0.00017998\n",
      "Epoch 1002, Loss: 0.5461007088516431, learning rate: 0.00017996\n",
      "Epoch 1003, Loss: 0.5460991926987164, learning rate: 0.00017994000000000002\n",
      "Epoch 1004, Loss: 0.546097675664829, learning rate: 0.00017992\n",
      "Epoch 1005, Loss: 0.5460961600587159, learning rate: 0.0001799\n",
      "Epoch 1006, Loss: 0.5460946423324634, learning rate: 0.00017988000000000002\n",
      "Epoch 1007, Loss: 0.5460931273506873, learning rate: 0.00017986\n",
      "Epoch 1008, Loss: 0.5460916148382223, learning rate: 0.00017984\n",
      "Epoch 1009, Loss: 0.5460901004879772, learning rate: 0.00017982000000000002\n",
      "Epoch 1010, Loss: 0.5460885817251862, learning rate: 0.0001798\n",
      "Epoch 1011, Loss: 0.5460870645138399, learning rate: 0.00017978000000000002\n",
      "Epoch 1012, Loss: 0.546085553152741, learning rate: 0.00017976\n",
      "Epoch 1013, Loss: 0.5460840372775414, learning rate: 0.00017974\n",
      "Epoch 1014, Loss: 0.5460825196710652, learning rate: 0.00017972000000000002\n",
      "Epoch 1015, Loss: 0.5460810078062085, learning rate: 0.0001797\n",
      "Epoch 1016, Loss: 0.5460794947954186, learning rate: 0.00017968000000000001\n",
      "Epoch 1017, Loss: 0.5460779815056204, learning rate: 0.00017966\n",
      "Epoch 1018, Loss: 0.5460764677657726, learning rate: 0.00017964\n",
      "Epoch 1019, Loss: 0.5460749501998783, learning rate: 0.00017962000000000002\n",
      "Epoch 1020, Loss: 0.5460734386218615, learning rate: 0.0001796\n",
      "Epoch 1021, Loss: 0.5460719237467692, learning rate: 0.00017958\n",
      "Epoch 1022, Loss: 0.5460704107681574, learning rate: 0.00017956\n",
      "Epoch 1023, Loss: 0.546068902043229, learning rate: 0.00017954\n",
      "Epoch 1024, Loss: 0.546067384405843, learning rate: 0.00017952000000000002\n",
      "Epoch 1025, Loss: 0.5460658741834403, learning rate: 0.0001795\n",
      "Epoch 1026, Loss: 0.5460643639654986, learning rate: 0.00017948\n",
      "Epoch 1027, Loss: 0.5460628505613678, learning rate: 0.00017946000000000002\n",
      "Epoch 1028, Loss: 0.5460613389402987, learning rate: 0.00017944\n",
      "Epoch 1029, Loss: 0.5460598240749605, learning rate: 0.00017942\n",
      "Epoch 1030, Loss: 0.5460583164369108, learning rate: 0.0001794\n",
      "Epoch 1031, Loss: 0.546056802355073, learning rate: 0.00017938\n",
      "Epoch 1032, Loss: 0.5460552920533395, learning rate: 0.00017936000000000002\n",
      "Epoch 1033, Loss: 0.5460537814399629, learning rate: 0.00017934\n",
      "Epoch 1034, Loss: 0.5460522682263113, learning rate: 0.00017932\n",
      "Epoch 1035, Loss: 0.5460507621271155, learning rate: 0.00017930000000000002\n",
      "Epoch 1036, Loss: 0.5460492507168987, learning rate: 0.00017928\n",
      "Epoch 1037, Loss: 0.5460477424827329, learning rate: 0.00017926000000000002\n",
      "Epoch 1038, Loss: 0.5460462297255748, learning rate: 0.00017924\n",
      "Epoch 1039, Loss: 0.5460447215484799, learning rate: 0.00017922\n",
      "Epoch 1040, Loss: 0.546043209268194, learning rate: 0.00017920000000000002\n",
      "Epoch 1041, Loss: 0.5460416992384433, learning rate: 0.00017918\n",
      "Epoch 1042, Loss: 0.5460401927417099, learning rate: 0.00017916\n",
      "Epoch 1043, Loss: 0.5460386808581206, learning rate: 0.00017914\n",
      "Epoch 1044, Loss: 0.5460371724632846, learning rate: 0.00017912\n",
      "Epoch 1045, Loss: 0.546035665197466, learning rate: 0.00017910000000000002\n",
      "Epoch 1046, Loss: 0.5460341546858415, learning rate: 0.00017908\n",
      "Epoch 1047, Loss: 0.5460326461316196, learning rate: 0.00017906\n",
      "Epoch 1048, Loss: 0.5460311373537097, learning rate: 0.00017904000000000002\n",
      "Epoch 1049, Loss: 0.5460296294094426, learning rate: 0.00017902\n",
      "Epoch 1050, Loss: 0.546028122099614, learning rate: 0.00017900000000000001\n",
      "Epoch 1051, Loss: 0.5460266148230746, learning rate: 0.00017898000000000002\n",
      "Epoch 1052, Loss: 0.5460251109967692, learning rate: 0.00017896\n",
      "Epoch 1053, Loss: 0.5460236014285963, learning rate: 0.00017894000000000002\n",
      "Epoch 1054, Loss: 0.5460220937387664, learning rate: 0.00017892\n",
      "Epoch 1055, Loss: 0.5460205843880876, learning rate: 0.0001789\n",
      "Epoch 1056, Loss: 0.5460190765718869, learning rate: 0.00017888000000000002\n",
      "Epoch 1057, Loss: 0.5460175747860122, learning rate: 0.00017886\n",
      "Epoch 1058, Loss: 0.5460160638144483, learning rate: 0.00017884000000000002\n",
      "Epoch 1059, Loss: 0.5460145566581186, learning rate: 0.00017882\n",
      "Epoch 1060, Loss: 0.5460130532119466, learning rate: 0.0001788\n",
      "Epoch 1061, Loss: 0.5460115478303846, learning rate: 0.00017878000000000002\n",
      "Epoch 1062, Loss: 0.5460100408053276, learning rate: 0.00017876\n",
      "Epoch 1063, Loss: 0.5460085344647364, learning rate: 0.00017874\n",
      "Epoch 1064, Loss: 0.5460070294248439, learning rate: 0.00017872\n",
      "Epoch 1065, Loss: 0.546005523299705, learning rate: 0.0001787\n",
      "Epoch 1066, Loss: 0.5460040177553883, learning rate: 0.00017868000000000002\n",
      "Epoch 1067, Loss: 0.5460025161114279, learning rate: 0.00017866\n",
      "Epoch 1068, Loss: 0.5460010097010993, learning rate: 0.00017864\n",
      "Epoch 1069, Loss: 0.5459995082544997, learning rate: 0.00017862\n",
      "Epoch 1070, Loss: 0.5459980008365524, learning rate: 0.0001786\n",
      "Epoch 1071, Loss: 0.5459964948533285, learning rate: 0.00017858000000000001\n",
      "Epoch 1072, Loss: 0.5459949934728574, learning rate: 0.00017856\n",
      "Epoch 1073, Loss: 0.5459934926982472, learning rate: 0.00017854\n",
      "Epoch 1074, Loss: 0.545991984707325, learning rate: 0.00017852000000000002\n",
      "Epoch 1075, Loss: 0.5459904807921433, learning rate: 0.0001785\n",
      "Epoch 1076, Loss: 0.5459889804808714, learning rate: 0.00017848\n",
      "Epoch 1077, Loss: 0.5459874762146115, learning rate: 0.00017846000000000002\n",
      "Epoch 1078, Loss: 0.5459859715851716, learning rate: 0.00017844\n",
      "Epoch 1079, Loss: 0.5459844711367452, learning rate: 0.00017842000000000002\n",
      "Epoch 1080, Loss: 0.5459829650861476, learning rate: 0.0001784\n",
      "Epoch 1081, Loss: 0.5459814667633066, learning rate: 0.00017838\n",
      "Epoch 1082, Loss: 0.545979965761211, learning rate: 0.00017836000000000002\n",
      "Epoch 1083, Loss: 0.5459784609473681, learning rate: 0.00017834\n",
      "Epoch 1084, Loss: 0.5459769589827421, learning rate: 0.00017832000000000001\n",
      "Epoch 1085, Loss: 0.5459754555750043, learning rate: 0.0001783\n",
      "Epoch 1086, Loss: 0.5459739562624797, learning rate: 0.00017828\n",
      "Epoch 1087, Loss: 0.54597245595591, learning rate: 0.00017826000000000002\n",
      "Epoch 1088, Loss: 0.5459709519359193, learning rate: 0.00017824\n",
      "Epoch 1089, Loss: 0.5459694501468343, learning rate: 0.00017822\n",
      "Epoch 1090, Loss: 0.5459679508841339, learning rate: 0.0001782\n",
      "Epoch 1091, Loss: 0.5459664482028652, learning rate: 0.00017818\n",
      "Epoch 1092, Loss: 0.5459649470560326, learning rate: 0.00017816000000000002\n",
      "Epoch 1093, Loss: 0.5459634510770193, learning rate: 0.00017814\n",
      "Epoch 1094, Loss: 0.5459619493773334, learning rate: 0.00017812\n",
      "Epoch 1095, Loss: 0.5459604494117809, learning rate: 0.00017810000000000002\n",
      "Epoch 1096, Loss: 0.5459589468329655, learning rate: 0.00017808\n",
      "Epoch 1097, Loss: 0.5459574481154483, learning rate: 0.00017806\n",
      "Epoch 1098, Loss: 0.5459559497853789, learning rate: 0.00017804\n",
      "Epoch 1099, Loss: 0.5459544495114599, learning rate: 0.00017802\n",
      "Epoch 1100, Loss: 0.5459529526154966, learning rate: 0.00017800000000000002\n",
      "Epoch 1101, Loss: 0.5459514553868765, learning rate: 0.00017798\n",
      "Epoch 1102, Loss: 0.5459499533585193, learning rate: 0.00017796\n",
      "Epoch 1103, Loss: 0.5459484539568791, learning rate: 0.00017794000000000002\n",
      "Epoch 1104, Loss: 0.5459469581242175, learning rate: 0.00017792\n",
      "Epoch 1105, Loss: 0.5459454572281157, learning rate: 0.00017790000000000001\n",
      "Epoch 1106, Loss: 0.5459439591401356, learning rate: 0.00017788\n",
      "Epoch 1107, Loss: 0.5459424631454675, learning rate: 0.00017786\n",
      "Epoch 1108, Loss: 0.5459409644065909, learning rate: 0.00017784000000000002\n",
      "Epoch 1109, Loss: 0.545939469247262, learning rate: 0.00017782\n",
      "Epoch 1110, Loss: 0.5459379738729232, learning rate: 0.0001778\n",
      "Epoch 1111, Loss: 0.5459364740567362, learning rate: 0.00017778\n",
      "Epoch 1112, Loss: 0.5459349804605843, learning rate: 0.00017776\n",
      "Epoch 1113, Loss: 0.5459334807552021, learning rate: 0.00017774000000000002\n",
      "Epoch 1114, Loss: 0.5459319823371149, learning rate: 0.00017772\n",
      "Epoch 1115, Loss: 0.5459304872786576, learning rate: 0.0001777\n",
      "Epoch 1116, Loss: 0.5459289944222273, learning rate: 0.00017768000000000002\n",
      "Epoch 1117, Loss: 0.5459274944897655, learning rate: 0.00017766\n",
      "Epoch 1118, Loss: 0.5459259995964197, learning rate: 0.00017764000000000001\n",
      "Epoch 1119, Loss: 0.5459245045824275, learning rate: 0.00017762000000000002\n",
      "Epoch 1120, Loss: 0.5459230083055566, learning rate: 0.0001776\n",
      "Epoch 1121, Loss: 0.5459215126191397, learning rate: 0.00017758000000000002\n",
      "Epoch 1122, Loss: 0.5459200200894586, learning rate: 0.00017756\n",
      "Epoch 1123, Loss: 0.545918523346637, learning rate: 0.00017754\n",
      "Epoch 1124, Loss: 0.5459170271379713, learning rate: 0.00017752000000000002\n",
      "Epoch 1125, Loss: 0.5459155329633686, learning rate: 0.0001775\n",
      "Epoch 1126, Loss: 0.5459140403202829, learning rate: 0.00017748000000000002\n",
      "Epoch 1127, Loss: 0.5459125486560735, learning rate: 0.00017746\n",
      "Epoch 1128, Loss: 0.545911051330506, learning rate: 0.00017744\n",
      "Epoch 1129, Loss: 0.5459095563330723, learning rate: 0.00017742000000000002\n",
      "Epoch 1130, Loss: 0.5459080648161257, learning rate: 0.0001774\n",
      "Epoch 1131, Loss: 0.5459065778294879, learning rate: 0.00017738\n",
      "Epoch 1132, Loss: 0.545905084603288, learning rate: 0.00017736\n",
      "Epoch 1133, Loss: 0.5459035873465833, learning rate: 0.00017734\n",
      "Epoch 1134, Loss: 0.545902091470003, learning rate: 0.00017732000000000002\n",
      "Epoch 1135, Loss: 0.5459005998356309, learning rate: 0.0001773\n",
      "Epoch 1136, Loss: 0.5458991115335156, learning rate: 0.00017728\n",
      "Epoch 1137, Loss: 0.545897615512581, learning rate: 0.00017726\n",
      "Epoch 1138, Loss: 0.5458961228628103, learning rate: 0.00017724\n",
      "Epoch 1139, Loss: 0.5458946322140914, learning rate: 0.00017722000000000001\n",
      "Epoch 1140, Loss: 0.5458931384144695, learning rate: 0.0001772\n",
      "Epoch 1141, Loss: 0.5458916471582604, learning rate: 0.00017718\n",
      "Epoch 1142, Loss: 0.5458901601304568, learning rate: 0.00017716000000000002\n",
      "Epoch 1143, Loss: 0.5458886733906021, learning rate: 0.00017714\n",
      "Epoch 1144, Loss: 0.5458871820477705, learning rate: 0.00017712\n",
      "Epoch 1145, Loss: 0.5458856897806085, learning rate: 0.00017710000000000002\n",
      "Epoch 1146, Loss: 0.5458841958732387, learning rate: 0.00017708\n",
      "Epoch 1147, Loss: 0.5458827032516944, learning rate: 0.00017706000000000002\n",
      "Epoch 1148, Loss: 0.5458812138450526, learning rate: 0.00017704\n",
      "Epoch 1149, Loss: 0.5458797228686041, learning rate: 0.00017702\n",
      "Epoch 1150, Loss: 0.5458782344787448, learning rate: 0.00017700000000000002\n",
      "Epoch 1151, Loss: 0.5458767481906542, learning rate: 0.00017698\n",
      "Epoch 1152, Loss: 0.5458752573960115, learning rate: 0.00017696\n",
      "Epoch 1153, Loss: 0.5458737693934755, learning rate: 0.00017694\n",
      "Epoch 1154, Loss: 0.5458722784498702, learning rate: 0.00017692\n",
      "Epoch 1155, Loss: 0.5458707893273489, learning rate: 0.00017690000000000002\n",
      "Epoch 1156, Loss: 0.5458693009618896, learning rate: 0.00017688\n",
      "Epoch 1157, Loss: 0.5458678165830751, learning rate: 0.00017686\n",
      "Epoch 1158, Loss: 0.5458663249503254, learning rate: 0.00017684000000000002\n",
      "Epoch 1159, Loss: 0.545864835993341, learning rate: 0.00017682\n",
      "Epoch 1160, Loss: 0.5458633478947285, learning rate: 0.00017680000000000001\n",
      "Epoch 1161, Loss: 0.5458618619839055, learning rate: 0.00017678\n",
      "Epoch 1162, Loss: 0.5458603803080976, learning rate: 0.00017676\n",
      "Epoch 1163, Loss: 0.5458588895953211, learning rate: 0.00017674000000000002\n",
      "Epoch 1164, Loss: 0.5458574038347445, learning rate: 0.00017672\n",
      "Epoch 1165, Loss: 0.5458559114766729, learning rate: 0.0001767\n",
      "Epoch 1166, Loss: 0.54585442554159, learning rate: 0.00017668000000000002\n",
      "Epoch 1167, Loss: 0.5458529399738562, learning rate: 0.00017666\n",
      "Epoch 1168, Loss: 0.5458514573558452, learning rate: 0.00017664000000000002\n",
      "Epoch 1169, Loss: 0.5458499671734995, learning rate: 0.00017662\n",
      "Epoch 1170, Loss: 0.545848479342793, learning rate: 0.0001766\n",
      "Epoch 1171, Loss: 0.5458469953057469, learning rate: 0.00017658000000000002\n",
      "Epoch 1172, Loss: 0.5458455111269125, learning rate: 0.00017656\n",
      "Epoch 1173, Loss: 0.545844021738319, learning rate: 0.00017654000000000001\n",
      "Epoch 1174, Loss: 0.5458425380935475, learning rate: 0.00017652\n",
      "Epoch 1175, Loss: 0.5458410540227263, learning rate: 0.0001765\n",
      "Epoch 1176, Loss: 0.5458395760802306, learning rate: 0.00017648000000000002\n",
      "Epoch 1177, Loss: 0.5458380869793781, learning rate: 0.00017646\n",
      "Epoch 1178, Loss: 0.5458366050178377, learning rate: 0.00017644\n",
      "Epoch 1179, Loss: 0.5458351148229358, learning rate: 0.00017642\n",
      "Epoch 1180, Loss: 0.5458336285525175, learning rate: 0.0001764\n",
      "Epoch 1181, Loss: 0.5458321466376582, learning rate: 0.00017638000000000002\n",
      "Epoch 1182, Loss: 0.5458306617700338, learning rate: 0.00017636\n",
      "Epoch 1183, Loss: 0.5458291801284073, learning rate: 0.00017634\n",
      "Epoch 1184, Loss: 0.5458277001027314, learning rate: 0.00017632000000000002\n",
      "Epoch 1185, Loss: 0.5458262114660688, learning rate: 0.0001763\n",
      "Epoch 1186, Loss: 0.545824729698555, learning rate: 0.00017628\n",
      "Epoch 1187, Loss: 0.5458232517460616, learning rate: 0.00017626000000000002\n",
      "Epoch 1188, Loss: 0.5458217642919613, learning rate: 0.00017624\n",
      "Epoch 1189, Loss: 0.5458202794498749, learning rate: 0.00017622000000000002\n",
      "Epoch 1190, Loss: 0.5458187989318318, learning rate: 0.0001762\n",
      "Epoch 1191, Loss: 0.5458173218416246, learning rate: 0.00017618\n",
      "Epoch 1192, Loss: 0.5458158329902052, learning rate: 0.00017616000000000002\n",
      "Epoch 1193, Loss: 0.5458143517974339, learning rate: 0.00017614\n",
      "Epoch 1194, Loss: 0.5458128725759445, learning rate: 0.00017612000000000001\n",
      "Epoch 1195, Loss: 0.5458113945406177, learning rate: 0.0001761\n",
      "Epoch 1196, Loss: 0.5458099083073956, learning rate: 0.00017608\n",
      "Epoch 1197, Loss: 0.545808427592503, learning rate: 0.00017606000000000002\n",
      "Epoch 1198, Loss: 0.5458069509157492, learning rate: 0.00017604\n",
      "Epoch 1199, Loss: 0.5458054649893967, learning rate: 0.00017602\n",
      "Epoch 1200, Loss: 0.5458039834745784, learning rate: 0.000176\n",
      "Epoch 1201, Loss: 0.5458025063048033, learning rate: 0.00017598\n",
      "Epoch 1202, Loss: 0.5458010300672708, learning rate: 0.00017596000000000002\n",
      "Epoch 1203, Loss: 0.5457995432352586, learning rate: 0.00017594\n",
      "Epoch 1204, Loss: 0.5457980625000406, learning rate: 0.00017592\n",
      "Epoch 1205, Loss: 0.5457965881475183, learning rate: 0.0001759\n",
      "Epoch 1206, Loss: 0.5457951025367274, learning rate: 0.00017588\n",
      "Epoch 1207, Loss: 0.5457936243797769, learning rate: 0.00017586000000000001\n",
      "Epoch 1208, Loss: 0.5457921503179017, learning rate: 0.00017584\n",
      "Epoch 1209, Loss: 0.5457906651256231, learning rate: 0.00017582\n",
      "Epoch 1210, Loss: 0.5457891878815159, learning rate: 0.00017580000000000002\n",
      "Epoch 1211, Loss: 0.5457877151655697, learning rate: 0.00017578\n",
      "Epoch 1212, Loss: 0.5457862315664533, learning rate: 0.00017576\n",
      "Epoch 1213, Loss: 0.5457847519862332, learning rate: 0.00017574000000000002\n",
      "Epoch 1214, Loss: 0.5457832754898831, learning rate: 0.00017572\n",
      "Epoch 1215, Loss: 0.5457818024653083, learning rate: 0.00017570000000000002\n",
      "Epoch 1216, Loss: 0.5457803190463201, learning rate: 0.00017568\n",
      "Epoch 1217, Loss: 0.5457788430280222, learning rate: 0.00017566\n",
      "Epoch 1218, Loss: 0.5457773649528448, learning rate: 0.00017564000000000002\n",
      "Epoch 1219, Loss: 0.5457758959842816, learning rate: 0.00017562\n",
      "Epoch 1220, Loss: 0.5457744135800034, learning rate: 0.0001756\n",
      "Epoch 1221, Loss: 0.5457729379940545, learning rate: 0.00017558\n",
      "Epoch 1222, Loss: 0.5457714653383092, learning rate: 0.00017556\n",
      "Epoch 1223, Loss: 0.5457699861418088, learning rate: 0.00017554000000000002\n",
      "Epoch 1224, Loss: 0.5457685094933545, learning rate: 0.00017552\n",
      "Epoch 1225, Loss: 0.5457670341231609, learning rate: 0.0001755\n",
      "Epoch 1226, Loss: 0.545765559294321, learning rate: 0.00017548000000000002\n",
      "Epoch 1227, Loss: 0.5457640891660833, learning rate: 0.00017546\n",
      "Epoch 1228, Loss: 0.545762610435758, learning rate: 0.00017544000000000001\n",
      "Epoch 1229, Loss: 0.5457611368257832, learning rate: 0.00017542\n",
      "Epoch 1230, Loss: 0.5457596645649261, learning rate: 0.0001754\n",
      "Epoch 1231, Loss: 0.5457581865376414, learning rate: 0.00017538000000000002\n",
      "Epoch 1232, Loss: 0.5457567114819932, learning rate: 0.00017536\n",
      "Epoch 1233, Loss: 0.5457552360578339, learning rate: 0.00017534\n",
      "Epoch 1234, Loss: 0.5457537642894864, learning rate: 0.00017532000000000002\n",
      "Epoch 1235, Loss: 0.5457522977131505, learning rate: 0.0001753\n",
      "Epoch 1236, Loss: 0.545750818467413, learning rate: 0.00017528000000000002\n",
      "Epoch 1237, Loss: 0.5457493449347938, learning rate: 0.00017526\n",
      "Epoch 1238, Loss: 0.5457478706853699, learning rate: 0.00017524\n",
      "Epoch 1239, Loss: 0.5457464030409737, learning rate: 0.00017522000000000002\n",
      "Epoch 1240, Loss: 0.545744926390948, learning rate: 0.0001752\n",
      "Epoch 1241, Loss: 0.5457434554090865, learning rate: 0.00017518\n",
      "Epoch 1242, Loss: 0.5457419851999866, learning rate: 0.00017516\n",
      "Epoch 1243, Loss: 0.5457405161884674, learning rate: 0.00017514\n",
      "Epoch 1244, Loss: 0.5457390440709233, learning rate: 0.00017512000000000002\n",
      "Epoch 1245, Loss: 0.5457375761014476, learning rate: 0.0001751\n",
      "Epoch 1246, Loss: 0.5457360996523348, learning rate: 0.00017508\n",
      "Epoch 1247, Loss: 0.5457346282707223, learning rate: 0.00017506\n",
      "Epoch 1248, Loss: 0.5457331585493927, learning rate: 0.00017504\n",
      "Epoch 1249, Loss: 0.5457316875834114, learning rate: 0.00017502000000000001\n",
      "Epoch 1250, Loss: 0.545730217022099, learning rate: 0.000175\n",
      "Epoch 1251, Loss: 0.5457287503291927, learning rate: 0.00017498\n",
      "Epoch 1252, Loss: 0.5457272822671919, learning rate: 0.00017496000000000002\n",
      "Epoch 1253, Loss: 0.5457258113635803, learning rate: 0.00017494\n",
      "Epoch 1254, Loss: 0.5457243442309021, learning rate: 0.00017492\n",
      "Epoch 1255, Loss: 0.5457228703307586, learning rate: 0.00017490000000000002\n",
      "Epoch 1256, Loss: 0.5457214010380645, learning rate: 0.00017488\n",
      "Epoch 1257, Loss: 0.5457199325502491, learning rate: 0.00017486000000000002\n",
      "Epoch 1258, Loss: 0.5457184637833526, learning rate: 0.00017484\n",
      "Epoch 1259, Loss: 0.5457170003351354, learning rate: 0.00017482\n",
      "Epoch 1260, Loss: 0.5457155302901184, learning rate: 0.00017480000000000002\n",
      "Epoch 1261, Loss: 0.5457140597996618, learning rate: 0.00017478\n",
      "Epoch 1262, Loss: 0.5457125918021248, learning rate: 0.00017476000000000001\n",
      "Epoch 1263, Loss: 0.5457111285321107, learning rate: 0.00017474\n",
      "Epoch 1264, Loss: 0.5457096579838631, learning rate: 0.00017472\n",
      "Epoch 1265, Loss: 0.5457081893070092, learning rate: 0.00017470000000000002\n",
      "Epoch 1266, Loss: 0.5457067222511904, learning rate: 0.00017468\n",
      "Epoch 1267, Loss: 0.5457052567564955, learning rate: 0.00017466\n",
      "Epoch 1268, Loss: 0.5457037890381846, learning rate: 0.00017464000000000002\n",
      "Epoch 1269, Loss: 0.5457023300676795, learning rate: 0.00017462\n",
      "Epoch 1270, Loss: 0.5457008572639316, learning rate: 0.00017460000000000002\n",
      "Epoch 1271, Loss: 0.5456993911569923, learning rate: 0.00017458\n",
      "Epoch 1272, Loss: 0.5456979252861808, learning rate: 0.00017456\n",
      "Epoch 1273, Loss: 0.5456964580510529, learning rate: 0.00017454000000000002\n",
      "Epoch 1274, Loss: 0.5456949930641594, learning rate: 0.00017452\n",
      "Epoch 1275, Loss: 0.5456935292023993, learning rate: 0.0001745\n",
      "Epoch 1276, Loss: 0.5456920620310629, learning rate: 0.00017448\n",
      "Epoch 1277, Loss: 0.5456905986092981, learning rate: 0.00017446\n",
      "Epoch 1278, Loss: 0.5456891317512637, learning rate: 0.00017444000000000002\n",
      "Epoch 1279, Loss: 0.5456876678097868, learning rate: 0.00017442\n",
      "Epoch 1280, Loss: 0.5456862053718206, learning rate: 0.0001744\n",
      "Epoch 1281, Loss: 0.5456847384756901, learning rate: 0.00017438\n",
      "Epoch 1282, Loss: 0.5456832766346938, learning rate: 0.00017436\n",
      "Epoch 1283, Loss: 0.5456818145111793, learning rate: 0.00017434000000000001\n",
      "Epoch 1284, Loss: 0.5456803558306679, learning rate: 0.00017432\n",
      "Epoch 1285, Loss: 0.5456788853410867, learning rate: 0.0001743\n",
      "Epoch 1286, Loss: 0.5456774230106037, learning rate: 0.00017428000000000002\n",
      "Epoch 1287, Loss: 0.5456759600037459, learning rate: 0.00017426\n",
      "Epoch 1288, Loss: 0.5456744950262907, learning rate: 0.00017424\n",
      "Epoch 1289, Loss: 0.5456730339580649, learning rate: 0.00017422\n",
      "Epoch 1290, Loss: 0.5456715705782732, learning rate: 0.0001742\n",
      "Epoch 1291, Loss: 0.5456701118360083, learning rate: 0.00017418000000000002\n",
      "Epoch 1292, Loss: 0.545668647662744, learning rate: 0.00017416\n",
      "Epoch 1293, Loss: 0.5456671824143058, learning rate: 0.00017414\n",
      "Epoch 1294, Loss: 0.545665721390136, learning rate: 0.00017412\n",
      "Epoch 1295, Loss: 0.5456642586454459, learning rate: 0.0001741\n",
      "Epoch 1296, Loss: 0.5456628028445969, learning rate: 0.00017408000000000001\n",
      "Epoch 1297, Loss: 0.5456613366808303, learning rate: 0.00017406\n",
      "Epoch 1298, Loss: 0.5456598733173625, learning rate: 0.00017404\n",
      "Epoch 1299, Loss: 0.5456584150659288, learning rate: 0.00017402000000000002\n",
      "Epoch 1300, Loss: 0.5456569545762656, learning rate: 0.000174\n",
      "Epoch 1301, Loss: 0.5456554917166855, learning rate: 0.00017398\n",
      "Epoch 1302, Loss: 0.5456540309137821, learning rate: 0.00017396000000000002\n",
      "Epoch 1303, Loss: 0.5456525697527477, learning rate: 0.00017394\n",
      "Epoch 1304, Loss: 0.545651111245534, learning rate: 0.00017392000000000002\n",
      "Epoch 1305, Loss: 0.5456496495538186, learning rate: 0.0001739\n",
      "Epoch 1306, Loss: 0.54564818823091, learning rate: 0.00017388\n",
      "Epoch 1307, Loss: 0.545646731067491, learning rate: 0.00017386000000000002\n",
      "Epoch 1308, Loss: 0.5456452702808556, learning rate: 0.00017384\n",
      "Epoch 1309, Loss: 0.5456438102949223, learning rate: 0.00017382\n",
      "Epoch 1310, Loss: 0.5456423520526932, learning rate: 0.0001738\n",
      "Epoch 1311, Loss: 0.545640891152846, learning rate: 0.00017378\n",
      "Epoch 1312, Loss: 0.5456394403685298, learning rate: 0.00017376000000000002\n",
      "Epoch 1313, Loss: 0.5456379768919063, learning rate: 0.00017374\n",
      "Epoch 1314, Loss: 0.5456365157490933, learning rate: 0.00017372\n",
      "Epoch 1315, Loss: 0.5456350567432483, learning rate: 0.0001737\n",
      "Epoch 1316, Loss: 0.5456335994095453, learning rate: 0.00017368\n",
      "Epoch 1317, Loss: 0.5456321419998946, learning rate: 0.00017366000000000001\n",
      "Epoch 1318, Loss: 0.5456306836333262, learning rate: 0.00017364\n",
      "Epoch 1319, Loss: 0.545629231170616, learning rate: 0.00017362\n",
      "Epoch 1320, Loss: 0.5456277700080481, learning rate: 0.00017360000000000002\n",
      "Epoch 1321, Loss: 0.5456263118119495, learning rate: 0.00017358\n",
      "Epoch 1322, Loss: 0.5456248575493079, learning rate: 0.00017356\n",
      "Epoch 1323, Loss: 0.5456233979752529, learning rate: 0.00017354000000000002\n",
      "Epoch 1324, Loss: 0.5456219401320939, learning rate: 0.00017352\n",
      "Epoch 1325, Loss: 0.5456204842151865, learning rate: 0.00017350000000000002\n",
      "Epoch 1326, Loss: 0.5456190308305123, learning rate: 0.00017348\n",
      "Epoch 1327, Loss: 0.5456175731959617, learning rate: 0.00017346\n",
      "Epoch 1328, Loss: 0.5456161152677416, learning rate: 0.00017344000000000002\n",
      "Epoch 1329, Loss: 0.5456146604812573, learning rate: 0.00017342\n",
      "Epoch 1330, Loss: 0.5456132035178555, learning rate: 0.0001734\n",
      "Epoch 1331, Loss: 0.5456117479774486, learning rate: 0.00017338\n",
      "Epoch 1332, Loss: 0.5456102945003355, learning rate: 0.00017336\n",
      "Epoch 1333, Loss: 0.5456088372495593, learning rate: 0.00017334000000000002\n",
      "Epoch 1334, Loss: 0.545607383120879, learning rate: 0.00017332\n",
      "Epoch 1335, Loss: 0.5456059300151757, learning rate: 0.0001733\n",
      "Epoch 1336, Loss: 0.545604473821645, learning rate: 0.00017328000000000002\n",
      "Epoch 1337, Loss: 0.545603020731984, learning rate: 0.00017326\n",
      "Epoch 1338, Loss: 0.5456015723973011, learning rate: 0.00017324000000000002\n",
      "Epoch 1339, Loss: 0.5456001138274089, learning rate: 0.00017322\n",
      "Epoch 1340, Loss: 0.5455986572749018, learning rate: 0.0001732\n",
      "Epoch 1341, Loss: 0.5455972048189719, learning rate: 0.00017318000000000002\n",
      "Epoch 1342, Loss: 0.5455957576182026, learning rate: 0.00017316\n",
      "Epoch 1343, Loss: 0.545594299888675, learning rate: 0.00017314\n",
      "Epoch 1344, Loss: 0.5455928458243888, learning rate: 0.00017312\n",
      "Epoch 1345, Loss: 0.545591398691476, learning rate: 0.0001731\n",
      "Epoch 1346, Loss: 0.5455899410728496, learning rate: 0.00017308000000000002\n",
      "Epoch 1347, Loss: 0.5455884892358246, learning rate: 0.00017306\n",
      "Epoch 1348, Loss: 0.5455870333883601, learning rate: 0.00017304\n",
      "Epoch 1349, Loss: 0.5455855825118061, learning rate: 0.00017302000000000002\n",
      "Epoch 1350, Loss: 0.5455841311293285, learning rate: 0.000173\n",
      "Epoch 1351, Loss: 0.545582678168056, learning rate: 0.00017298000000000001\n",
      "Epoch 1352, Loss: 0.5455812266938612, learning rate: 0.00017296\n",
      "Epoch 1353, Loss: 0.5455797769630296, learning rate: 0.00017294\n",
      "Epoch 1354, Loss: 0.5455783309819773, learning rate: 0.00017292000000000002\n",
      "Epoch 1355, Loss: 0.5455768758424899, learning rate: 0.0001729\n",
      "Epoch 1356, Loss: 0.5455754221694091, learning rate: 0.00017288\n",
      "Epoch 1357, Loss: 0.5455739713202437, learning rate: 0.00017286\n",
      "Epoch 1358, Loss: 0.5455725229121559, learning rate: 0.00017284\n",
      "Epoch 1359, Loss: 0.5455710714998295, learning rate: 0.00017282000000000002\n",
      "Epoch 1360, Loss: 0.5455696200929913, learning rate: 0.0001728\n",
      "Epoch 1361, Loss: 0.5455681690366276, learning rate: 0.00017278\n",
      "Epoch 1362, Loss: 0.5455667241320032, learning rate: 0.00017276\n",
      "Epoch 1363, Loss: 0.5455652761766485, learning rate: 0.00017274\n",
      "Epoch 1364, Loss: 0.5455638234391553, learning rate: 0.00017272\n",
      "Epoch 1365, Loss: 0.5455623716768051, learning rate: 0.0001727\n",
      "Epoch 1366, Loss: 0.5455609245966367, learning rate: 0.00017268\n",
      "Epoch 1367, Loss: 0.5455594775728834, learning rate: 0.00017266000000000002\n",
      "Epoch 1368, Loss: 0.5455580265534032, learning rate: 0.00017264\n",
      "Epoch 1369, Loss: 0.5455565765588304, learning rate: 0.00017262\n",
      "Epoch 1370, Loss: 0.5455551295304869, learning rate: 0.00017260000000000002\n",
      "Epoch 1371, Loss: 0.5455536797582226, learning rate: 0.00017258\n",
      "Epoch 1372, Loss: 0.5455522371055288, learning rate: 0.00017256000000000001\n",
      "Epoch 1373, Loss: 0.545550785959437, learning rate: 0.00017254\n",
      "Epoch 1374, Loss: 0.54554933783767, learning rate: 0.00017252\n",
      "Epoch 1375, Loss: 0.5455478901927518, learning rate: 0.00017250000000000002\n",
      "Epoch 1376, Loss: 0.545546443776315, learning rate: 0.00017248\n",
      "Epoch 1377, Loss: 0.5455449977730142, learning rate: 0.00017246\n",
      "Epoch 1378, Loss: 0.5455435493887868, learning rate: 0.00017244\n",
      "Epoch 1379, Loss: 0.5455421022803885, learning rate: 0.00017242\n",
      "Epoch 1380, Loss: 0.5455406544573234, learning rate: 0.00017240000000000002\n",
      "Epoch 1381, Loss: 0.545539206863261, learning rate: 0.00017238\n",
      "Epoch 1382, Loss: 0.5455377631113049, learning rate: 0.00017236\n",
      "Epoch 1383, Loss: 0.5455363163408059, learning rate: 0.00017234\n",
      "Epoch 1384, Loss: 0.5455348706361052, learning rate: 0.00017232\n",
      "Epoch 1385, Loss: 0.5455334330725738, learning rate: 0.00017230000000000001\n",
      "Epoch 1386, Loss: 0.5455319808373401, learning rate: 0.00017228000000000002\n",
      "Epoch 1387, Loss: 0.5455305342664892, learning rate: 0.00017226\n",
      "Epoch 1388, Loss: 0.5455290890066716, learning rate: 0.00017224000000000002\n",
      "Epoch 1389, Loss: 0.5455276428898469, learning rate: 0.00017222\n",
      "Epoch 1390, Loss: 0.5455262005857415, learning rate: 0.0001722\n",
      "Epoch 1391, Loss: 0.5455247568313084, learning rate: 0.00017218000000000002\n",
      "Epoch 1392, Loss: 0.5455233147629188, learning rate: 0.00017216\n",
      "Epoch 1393, Loss: 0.545521868333153, learning rate: 0.00017214000000000002\n",
      "Epoch 1394, Loss: 0.5455204224488654, learning rate: 0.00017212\n",
      "Epoch 1395, Loss: 0.5455189788442438, learning rate: 0.0001721\n",
      "Epoch 1396, Loss: 0.5455175411354404, learning rate: 0.00017208000000000002\n",
      "Epoch 1397, Loss: 0.5455160930607277, learning rate: 0.00017206\n",
      "Epoch 1398, Loss: 0.5455146481040734, learning rate: 0.00017204\n",
      "Epoch 1399, Loss: 0.5455132045204558, learning rate: 0.00017202\n",
      "Epoch 1400, Loss: 0.545511764453869, learning rate: 0.000172\n",
      "Epoch 1401, Loss: 0.5455103203373727, learning rate: 0.00017198000000000002\n",
      "Epoch 1402, Loss: 0.5455088811538935, learning rate: 0.00017196\n",
      "Epoch 1403, Loss: 0.5455074362009427, learning rate: 0.00017194\n",
      "Epoch 1404, Loss: 0.5455059925449012, learning rate: 0.00017192000000000002\n",
      "Epoch 1405, Loss: 0.5455045494268171, learning rate: 0.0001719\n",
      "Epoch 1406, Loss: 0.5455031084374277, learning rate: 0.00017188000000000001\n",
      "Epoch 1407, Loss: 0.5455016666469984, learning rate: 0.00017186\n",
      "Epoch 1408, Loss: 0.545500224280423, learning rate: 0.00017184\n",
      "Epoch 1409, Loss: 0.5454987906536604, learning rate: 0.00017182000000000002\n",
      "Epoch 1410, Loss: 0.5454973454921089, learning rate: 0.0001718\n",
      "Epoch 1411, Loss: 0.5454959019183959, learning rate: 0.00017178\n",
      "Epoch 1412, Loss: 0.54549446033227, learning rate: 0.00017176\n",
      "Epoch 1413, Loss: 0.5454930190583124, learning rate: 0.00017174\n",
      "Epoch 1414, Loss: 0.5454915840328229, learning rate: 0.00017172000000000002\n",
      "Epoch 1415, Loss: 0.5454901392692799, learning rate: 0.0001717\n",
      "Epoch 1416, Loss: 0.5454886983291359, learning rate: 0.00017168\n",
      "Epoch 1417, Loss: 0.5454872612432455, learning rate: 0.00017166000000000002\n",
      "Epoch 1418, Loss: 0.5454858192675482, learning rate: 0.00017164\n",
      "Epoch 1419, Loss: 0.5454843780889457, learning rate: 0.00017162000000000001\n",
      "Epoch 1420, Loss: 0.5454829405199615, learning rate: 0.0001716\n",
      "Epoch 1421, Loss: 0.5454815057521408, learning rate: 0.00017158\n",
      "Epoch 1422, Loss: 0.545480064021748, learning rate: 0.00017156000000000002\n",
      "Epoch 1423, Loss: 0.5454786230352825, learning rate: 0.00017154\n",
      "Epoch 1424, Loss: 0.5454771891262012, learning rate: 0.00017152\n",
      "Epoch 1425, Loss: 0.545475748482986, learning rate: 0.0001715\n",
      "Epoch 1426, Loss: 0.5454743071257709, learning rate: 0.00017148\n",
      "Epoch 1427, Loss: 0.5454728701374345, learning rate: 0.00017146000000000002\n",
      "Epoch 1428, Loss: 0.54547143090411, learning rate: 0.00017144\n",
      "Epoch 1429, Loss: 0.5454699940725058, learning rate: 0.00017142\n",
      "Epoch 1430, Loss: 0.5454685561539917, learning rate: 0.0001714\n",
      "Epoch 1431, Loss: 0.5454671170178133, learning rate: 0.00017138\n",
      "Epoch 1432, Loss: 0.545465681733708, learning rate: 0.00017136\n",
      "Epoch 1433, Loss: 0.545464243164809, learning rate: 0.00017134\n",
      "Epoch 1434, Loss: 0.5454628057678802, learning rate: 0.00017132\n",
      "Epoch 1435, Loss: 0.5454613712042047, learning rate: 0.00017130000000000002\n",
      "Epoch 1436, Loss: 0.5454599334511764, learning rate: 0.00017128\n",
      "Epoch 1437, Loss: 0.5454585025342004, learning rate: 0.00017126\n",
      "Epoch 1438, Loss: 0.5454570625706694, learning rate: 0.00017124000000000002\n",
      "Epoch 1439, Loss: 0.545455623944694, learning rate: 0.00017122\n",
      "Epoch 1440, Loss: 0.5454541931538681, learning rate: 0.00017120000000000001\n",
      "Epoch 1441, Loss: 0.5454527526690985, learning rate: 0.00017118\n",
      "Epoch 1442, Loss: 0.545451317592337, learning rate: 0.00017116\n",
      "Epoch 1443, Loss: 0.5454498819663078, learning rate: 0.00017114000000000002\n",
      "Epoch 1444, Loss: 0.5454484475427493, learning rate: 0.00017112\n",
      "Epoch 1445, Loss: 0.5454470133190616, learning rate: 0.0001711\n",
      "Epoch 1446, Loss: 0.54544557710955, learning rate: 0.00017108\n",
      "Epoch 1447, Loss: 0.54544414269574, learning rate: 0.00017106\n",
      "Epoch 1448, Loss: 0.5454427081658791, learning rate: 0.00017104000000000002\n",
      "Epoch 1449, Loss: 0.5454412765707916, learning rate: 0.00017102\n",
      "Epoch 1450, Loss: 0.5454398422157132, learning rate: 0.000171\n",
      "Epoch 1451, Loss: 0.5454384111003676, learning rate: 0.00017098000000000002\n",
      "Epoch 1452, Loss: 0.5454369744531573, learning rate: 0.00017096\n",
      "Epoch 1453, Loss: 0.5454355392108916, learning rate: 0.00017094\n",
      "Epoch 1454, Loss: 0.5454341052060436, learning rate: 0.00017092000000000002\n",
      "Epoch 1455, Loss: 0.5454326740341184, learning rate: 0.0001709\n",
      "Epoch 1456, Loss: 0.545431239094887, learning rate: 0.00017088000000000002\n",
      "Epoch 1457, Loss: 0.5454298066286849, learning rate: 0.00017086\n",
      "Epoch 1458, Loss: 0.5454283748928626, learning rate: 0.00017084\n",
      "Epoch 1459, Loss: 0.5454269449172401, learning rate: 0.00017082000000000002\n",
      "Epoch 1460, Loss: 0.5454255103474187, learning rate: 0.0001708\n",
      "Epoch 1461, Loss: 0.5454240759857396, learning rate: 0.00017078000000000001\n",
      "Epoch 1462, Loss: 0.5454226455143502, learning rate: 0.00017076\n",
      "Epoch 1463, Loss: 0.5454212128491283, learning rate: 0.00017074\n",
      "Epoch 1464, Loss: 0.5454197834327504, learning rate: 0.00017072000000000002\n",
      "Epoch 1465, Loss: 0.5454183547452505, learning rate: 0.0001707\n",
      "Epoch 1466, Loss: 0.5454169200320592, learning rate: 0.00017068\n",
      "Epoch 1467, Loss: 0.5454154882435597, learning rate: 0.00017066\n",
      "Epoch 1468, Loss: 0.5454140580180898, learning rate: 0.00017064\n",
      "Epoch 1469, Loss: 0.5454126255463116, learning rate: 0.00017062000000000002\n",
      "Epoch 1470, Loss: 0.5454111954729692, learning rate: 0.0001706\n",
      "Epoch 1471, Loss: 0.5454097664628003, learning rate: 0.00017058\n",
      "Epoch 1472, Loss: 0.5454083360315956, learning rate: 0.00017056000000000002\n",
      "Epoch 1473, Loss: 0.5454069045747204, learning rate: 0.00017054\n",
      "Epoch 1474, Loss: 0.5454054771398055, learning rate: 0.00017052000000000001\n",
      "Epoch 1475, Loss: 0.5454040513326845, learning rate: 0.0001705\n",
      "Epoch 1476, Loss: 0.5454026197070958, learning rate: 0.00017048\n",
      "Epoch 1477, Loss: 0.5454011861926417, learning rate: 0.00017046000000000002\n",
      "Epoch 1478, Loss: 0.5453997580752483, learning rate: 0.00017044\n",
      "Epoch 1479, Loss: 0.5453983335271042, learning rate: 0.00017042\n",
      "Epoch 1480, Loss: 0.5453969007837569, learning rate: 0.0001704\n",
      "Epoch 1481, Loss: 0.5453954766030826, learning rate: 0.00017038\n",
      "Epoch 1482, Loss: 0.5453940468472225, learning rate: 0.00017036000000000002\n",
      "Epoch 1483, Loss: 0.5453926148728783, learning rate: 0.00017034\n",
      "Epoch 1484, Loss: 0.5453911867029589, learning rate: 0.00017032\n",
      "Epoch 1485, Loss: 0.5453897606317468, learning rate: 0.00017030000000000002\n",
      "Epoch 1486, Loss: 0.5453883327359011, learning rate: 0.00017028\n",
      "Epoch 1487, Loss: 0.545386910572291, learning rate: 0.00017026\n",
      "Epoch 1488, Loss: 0.5453854810079856, learning rate: 0.00017024\n",
      "Epoch 1489, Loss: 0.5453840516537548, learning rate: 0.00017022\n",
      "Epoch 1490, Loss: 0.5453826277335811, learning rate: 0.00017020000000000002\n",
      "Epoch 1491, Loss: 0.5453811988422392, learning rate: 0.00017018\n",
      "Epoch 1492, Loss: 0.5453797710380865, learning rate: 0.00017016\n",
      "Epoch 1493, Loss: 0.5453783449505663, learning rate: 0.00017014\n",
      "Epoch 1494, Loss: 0.545376918754426, learning rate: 0.00017012\n",
      "Epoch 1495, Loss: 0.5453754925225622, learning rate: 0.00017010000000000001\n",
      "Epoch 1496, Loss: 0.5453740692457953, learning rate: 0.00017008\n",
      "Epoch 1497, Loss: 0.5453726455656176, learning rate: 0.00017006\n",
      "Epoch 1498, Loss: 0.5453712179829733, learning rate: 0.00017004\n",
      "Epoch 1499, Loss: 0.545369790592028, learning rate: 0.00017002\n",
      "Epoch 1500, Loss: 0.5453683690313315, learning rate: 0.00017\n",
      "Epoch 1501, Loss: 0.5453669427299072, learning rate: 0.00016998000000000002\n",
      "Epoch 1502, Loss: 0.5453655186988177, learning rate: 0.00016996\n",
      "Epoch 1503, Loss: 0.5453640922069402, learning rate: 0.00016994000000000002\n",
      "Epoch 1504, Loss: 0.5453626679515401, learning rate: 0.00016992\n",
      "Epoch 1505, Loss: 0.545361244736916, learning rate: 0.0001699\n",
      "Epoch 1506, Loss: 0.5453598225705956, learning rate: 0.00016988000000000002\n",
      "Epoch 1507, Loss: 0.545358396754729, learning rate: 0.00016986\n",
      "Epoch 1508, Loss: 0.5453569735346743, learning rate: 0.00016984000000000001\n",
      "Epoch 1509, Loss: 0.5453555512273975, learning rate: 0.00016982\n",
      "Epoch 1510, Loss: 0.5453541284698317, learning rate: 0.0001698\n",
      "Epoch 1511, Loss: 0.5453527122351008, learning rate: 0.00016978000000000002\n",
      "Epoch 1512, Loss: 0.5453512857052076, learning rate: 0.00016976\n",
      "Epoch 1513, Loss: 0.5453498588190088, learning rate: 0.00016974\n",
      "Epoch 1514, Loss: 0.5453484359923916, learning rate: 0.00016972\n",
      "Epoch 1515, Loss: 0.5453470155487762, learning rate: 0.0001697\n",
      "Epoch 1516, Loss: 0.5453455946920281, learning rate: 0.00016968000000000002\n",
      "Epoch 1517, Loss: 0.5453441696505368, learning rate: 0.00016966\n",
      "Epoch 1518, Loss: 0.5453427479375929, learning rate: 0.00016964\n",
      "Epoch 1519, Loss: 0.5453413268836524, learning rate: 0.00016962000000000002\n",
      "Epoch 1520, Loss: 0.5453399048481485, learning rate: 0.0001696\n",
      "Epoch 1521, Loss: 0.5453384879689871, learning rate: 0.00016958\n",
      "Epoch 1522, Loss: 0.5453370663628372, learning rate: 0.00016956000000000002\n",
      "Epoch 1523, Loss: 0.5453356429671841, learning rate: 0.00016954\n",
      "Epoch 1524, Loss: 0.54533422399493, learning rate: 0.00016952000000000002\n",
      "Epoch 1525, Loss: 0.5453328021254608, learning rate: 0.0001695\n",
      "Epoch 1526, Loss: 0.5453313803290142, learning rate: 0.00016948\n",
      "Epoch 1527, Loss: 0.5453299618896921, learning rate: 0.00016946000000000002\n",
      "Epoch 1528, Loss: 0.5453285404412005, learning rate: 0.00016944\n",
      "Epoch 1529, Loss: 0.5453271204692491, learning rate: 0.00016942000000000001\n",
      "Epoch 1530, Loss: 0.5453257091590179, learning rate: 0.0001694\n",
      "Epoch 1531, Loss: 0.5453242837939584, learning rate: 0.00016938\n",
      "Epoch 1532, Loss: 0.5453228622494828, learning rate: 0.00016936000000000002\n",
      "Epoch 1533, Loss: 0.5453214439644188, learning rate: 0.00016934\n",
      "Epoch 1534, Loss: 0.545320026175467, learning rate: 0.00016932\n",
      "Epoch 1535, Loss: 0.5453186058810906, learning rate: 0.0001693\n",
      "Epoch 1536, Loss: 0.5453171871084558, learning rate: 0.00016928\n",
      "Epoch 1537, Loss: 0.5453157704635994, learning rate: 0.00016926000000000002\n",
      "Epoch 1538, Loss: 0.5453143519687799, learning rate: 0.00016924\n",
      "Epoch 1539, Loss: 0.545312935305043, learning rate: 0.00016922\n",
      "Epoch 1540, Loss: 0.5453115150319919, learning rate: 0.00016920000000000002\n",
      "Epoch 1541, Loss: 0.5453100951403763, learning rate: 0.00016918\n",
      "Epoch 1542, Loss: 0.5453086779873119, learning rate: 0.00016916\n",
      "Epoch 1543, Loss: 0.5453072602146404, learning rate: 0.00016914\n",
      "Epoch 1544, Loss: 0.5453058444736587, learning rate: 0.00016912\n",
      "Epoch 1545, Loss: 0.5453044246919171, learning rate: 0.00016910000000000002\n",
      "Epoch 1546, Loss: 0.545303005855107, learning rate: 0.00016908\n",
      "Epoch 1547, Loss: 0.545301589858311, learning rate: 0.00016906\n",
      "Epoch 1548, Loss: 0.5453001734539107, learning rate: 0.00016904000000000002\n",
      "Epoch 1549, Loss: 0.5452987606452494, learning rate: 0.00016902\n",
      "Epoch 1550, Loss: 0.5452973392743631, learning rate: 0.00016900000000000002\n",
      "Epoch 1551, Loss: 0.5452959209356479, learning rate: 0.00016898\n",
      "Epoch 1552, Loss: 0.5452945051403895, learning rate: 0.00016896\n",
      "Epoch 1553, Loss: 0.5452930908271169, learning rate: 0.00016894000000000002\n",
      "Epoch 1554, Loss: 0.5452916745297056, learning rate: 0.00016892\n",
      "Epoch 1555, Loss: 0.5452902563945649, learning rate: 0.0001689\n",
      "Epoch 1556, Loss: 0.5452888422647281, learning rate: 0.00016888\n",
      "Epoch 1557, Loss: 0.5452874250846063, learning rate: 0.00016886\n",
      "Epoch 1558, Loss: 0.5452860113852692, learning rate: 0.00016884000000000002\n",
      "Epoch 1559, Loss: 0.5452846000857716, learning rate: 0.00016882\n",
      "Epoch 1560, Loss: 0.5452831815839814, learning rate: 0.0001688\n",
      "Epoch 1561, Loss: 0.5452817649998335, learning rate: 0.00016878\n",
      "Epoch 1562, Loss: 0.5452803577610651, learning rate: 0.00016876\n",
      "Epoch 1563, Loss: 0.5452789393285722, learning rate: 0.00016874000000000001\n",
      "Epoch 1564, Loss: 0.5452775287080291, learning rate: 0.00016872\n",
      "Epoch 1565, Loss: 0.545276112138719, learning rate: 0.0001687\n",
      "Epoch 1566, Loss: 0.5452746969902305, learning rate: 0.00016868\n",
      "Epoch 1567, Loss: 0.5452732854935363, learning rate: 0.00016866\n",
      "Epoch 1568, Loss: 0.5452718743473272, learning rate: 0.00016864\n",
      "Epoch 1569, Loss: 0.5452704595929337, learning rate: 0.00016862000000000002\n",
      "Epoch 1570, Loss: 0.54526905351014, learning rate: 0.0001686\n",
      "Epoch 1571, Loss: 0.5452676357492663, learning rate: 0.00016858000000000002\n",
      "Epoch 1572, Loss: 0.5452662229617259, learning rate: 0.00016856\n",
      "Epoch 1573, Loss: 0.5452648113582338, learning rate: 0.00016854\n",
      "Epoch 1574, Loss: 0.5452633992858161, learning rate: 0.00016852000000000002\n",
      "Epoch 1575, Loss: 0.5452619881652486, learning rate: 0.0001685\n",
      "Epoch 1576, Loss: 0.5452605746475028, learning rate: 0.00016848\n",
      "Epoch 1577, Loss: 0.5452591662631795, learning rate: 0.00016846\n",
      "Epoch 1578, Loss: 0.5452577550485506, learning rate: 0.00016844\n",
      "Epoch 1579, Loss: 0.5452563483230306, learning rate: 0.00016842000000000002\n",
      "Epoch 1580, Loss: 0.5452549326152505, learning rate: 0.0001684\n",
      "Epoch 1581, Loss: 0.545253522684692, learning rate: 0.00016838\n",
      "Epoch 1582, Loss: 0.5452521129502026, learning rate: 0.00016836\n",
      "Epoch 1583, Loss: 0.545250701132666, learning rate: 0.00016834\n",
      "Epoch 1584, Loss: 0.5452492933140872, learning rate: 0.00016832000000000001\n",
      "Epoch 1585, Loss: 0.545247883568722, learning rate: 0.00016830000000000003\n",
      "Epoch 1586, Loss: 0.5452464717654846, learning rate: 0.00016828\n",
      "Epoch 1587, Loss: 0.5452450645337114, learning rate: 0.00016826\n",
      "Epoch 1588, Loss: 0.5452436543612701, learning rate: 0.00016824\n",
      "Epoch 1589, Loss: 0.545242245699897, learning rate: 0.00016822\n",
      "Epoch 1590, Loss: 0.5452408364397799, learning rate: 0.00016820000000000002\n",
      "Epoch 1591, Loss: 0.5452394287172985, learning rate: 0.00016818\n",
      "Epoch 1592, Loss: 0.5452380194111005, learning rate: 0.00016816000000000002\n",
      "Epoch 1593, Loss: 0.545236610607736, learning rate: 0.00016814\n",
      "Epoch 1594, Loss: 0.5452352031255595, learning rate: 0.00016812\n",
      "Epoch 1595, Loss: 0.5452337945713731, learning rate: 0.00016810000000000002\n",
      "Epoch 1596, Loss: 0.5452323879122822, learning rate: 0.00016808\n",
      "Epoch 1597, Loss: 0.5452309803448611, learning rate: 0.00016806000000000001\n",
      "Epoch 1598, Loss: 0.5452295723717573, learning rate: 0.00016804\n",
      "Epoch 1599, Loss: 0.5452281649217856, learning rate: 0.00016802\n",
      "Epoch 1600, Loss: 0.5452267598605284, learning rate: 0.00016800000000000002\n",
      "Epoch 1601, Loss: 0.5452253524737881, learning rate: 0.00016798\n",
      "Epoch 1602, Loss: 0.5452239428595017, learning rate: 0.00016796\n",
      "Epoch 1603, Loss: 0.5452225374966101, learning rate: 0.00016794\n",
      "Epoch 1604, Loss: 0.5452211334275916, learning rate: 0.00016792\n",
      "Epoch 1605, Loss: 0.5452197274081049, learning rate: 0.00016790000000000002\n",
      "Epoch 1606, Loss: 0.545218318991388, learning rate: 0.00016788\n",
      "Epoch 1607, Loss: 0.5452169151592221, learning rate: 0.00016786\n",
      "Epoch 1608, Loss: 0.545215508482563, learning rate: 0.00016784000000000002\n",
      "Epoch 1609, Loss: 0.5452141086197588, learning rate: 0.00016782\n",
      "Epoch 1610, Loss: 0.5452126990095388, learning rate: 0.0001678\n",
      "Epoch 1611, Loss: 0.5452112933051034, learning rate: 0.00016778\n",
      "Epoch 1612, Loss: 0.5452098881230915, learning rate: 0.00016776\n",
      "Epoch 1613, Loss: 0.5452084827626934, learning rate: 0.00016774000000000002\n",
      "Epoch 1614, Loss: 0.5452070781796711, learning rate: 0.00016772\n",
      "Epoch 1615, Loss: 0.5452056756087681, learning rate: 0.0001677\n",
      "Epoch 1616, Loss: 0.5452042711569155, learning rate: 0.00016768000000000002\n",
      "Epoch 1617, Loss: 0.5452028667484249, learning rate: 0.00016766\n",
      "Epoch 1618, Loss: 0.5452014651188486, learning rate: 0.00016764000000000001\n",
      "Epoch 1619, Loss: 0.5452000581057658, learning rate: 0.00016762\n",
      "Epoch 1620, Loss: 0.5451986574100672, learning rate: 0.0001676\n",
      "Epoch 1621, Loss: 0.5451972548811683, learning rate: 0.00016758000000000002\n",
      "Epoch 1622, Loss: 0.545195848850842, learning rate: 0.00016756\n",
      "Epoch 1623, Loss: 0.545194447074252, learning rate: 0.00016754\n",
      "Epoch 1624, Loss: 0.5451930462185884, learning rate: 0.00016752\n",
      "Epoch 1625, Loss: 0.5451916416472182, learning rate: 0.0001675\n",
      "Epoch 1626, Loss: 0.5451902414780776, learning rate: 0.00016748000000000002\n",
      "Epoch 1627, Loss: 0.5451888383526754, learning rate: 0.00016746\n",
      "Epoch 1628, Loss: 0.5451874359316397, learning rate: 0.00016744\n",
      "Epoch 1629, Loss: 0.5451860327414813, learning rate: 0.00016742\n",
      "Epoch 1630, Loss: 0.5451846311222566, learning rate: 0.0001674\n",
      "Epoch 1631, Loss: 0.5451832310409348, learning rate: 0.00016738000000000001\n",
      "Epoch 1632, Loss: 0.5451818315797453, learning rate: 0.00016736000000000002\n",
      "Epoch 1633, Loss: 0.5451804277735209, learning rate: 0.00016734\n",
      "Epoch 1634, Loss: 0.5451790271024659, learning rate: 0.00016732\n",
      "Epoch 1635, Loss: 0.5451776270131928, learning rate: 0.0001673\n",
      "Epoch 1636, Loss: 0.5451762259767714, learning rate: 0.00016728\n",
      "Epoch 1637, Loss: 0.54517482543734, learning rate: 0.00016726000000000002\n",
      "Epoch 1638, Loss: 0.5451734253620405, learning rate: 0.00016724\n",
      "Epoch 1639, Loss: 0.5451720258031107, learning rate: 0.00016722000000000002\n",
      "Epoch 1640, Loss: 0.5451706286666664, learning rate: 0.0001672\n",
      "Epoch 1641, Loss: 0.545169233685544, learning rate: 0.00016718\n",
      "Epoch 1642, Loss: 0.5451678274206858, learning rate: 0.00016716000000000002\n",
      "Epoch 1643, Loss: 0.5451664276995988, learning rate: 0.00016714\n",
      "Epoch 1644, Loss: 0.545165031681129, learning rate: 0.00016712\n",
      "Epoch 1645, Loss: 0.5451636288418048, learning rate: 0.0001671\n",
      "Epoch 1646, Loss: 0.5451622303197547, learning rate: 0.00016708\n",
      "Epoch 1647, Loss: 0.5451608318681641, learning rate: 0.00016706000000000002\n",
      "Epoch 1648, Loss: 0.545159435196322, learning rate: 0.00016704\n",
      "Epoch 1649, Loss: 0.5451580351075153, learning rate: 0.00016702\n",
      "Epoch 1650, Loss: 0.5451566377412997, learning rate: 0.000167\n",
      "Epoch 1651, Loss: 0.545155242126665, learning rate: 0.00016698\n",
      "Epoch 1652, Loss: 0.5451538412941535, learning rate: 0.00016696000000000001\n",
      "Epoch 1653, Loss: 0.5451524447376508, learning rate: 0.00016694000000000002\n",
      "Epoch 1654, Loss: 0.5451510512668385, learning rate: 0.00016692\n",
      "Epoch 1655, Loss: 0.5451496533665475, learning rate: 0.0001669\n",
      "Epoch 1656, Loss: 0.5451482536536801, learning rate: 0.00016688\n",
      "Epoch 1657, Loss: 0.5451468573705004, learning rate: 0.00016686\n",
      "Epoch 1658, Loss: 0.5451454600882406, learning rate: 0.00016684000000000002\n",
      "Epoch 1659, Loss: 0.5451440636264637, learning rate: 0.00016682\n",
      "Epoch 1660, Loss: 0.5451426701073198, learning rate: 0.00016680000000000002\n",
      "Epoch 1661, Loss: 0.5451412727885427, learning rate: 0.00016678\n",
      "Epoch 1662, Loss: 0.5451398746254339, learning rate: 0.00016676\n",
      "Epoch 1663, Loss: 0.545138482052835, learning rate: 0.00016674000000000002\n",
      "Epoch 1664, Loss: 0.5451370860127877, learning rate: 0.00016672\n",
      "Epoch 1665, Loss: 0.5451356904735561, learning rate: 0.0001667\n",
      "Epoch 1666, Loss: 0.5451342943389936, learning rate: 0.00016668\n",
      "Epoch 1667, Loss: 0.5451328992670232, learning rate: 0.00016666\n",
      "Epoch 1668, Loss: 0.545131504699098, learning rate: 0.00016664000000000002\n",
      "Epoch 1669, Loss: 0.5451301097771202, learning rate: 0.00016662\n",
      "Epoch 1670, Loss: 0.5451287146623548, learning rate: 0.0001666\n",
      "Epoch 1671, Loss: 0.5451273224461091, learning rate: 0.00016658\n",
      "Epoch 1672, Loss: 0.5451259280533062, learning rate: 0.00016656\n",
      "Epoch 1673, Loss: 0.5451245326144306, learning rate: 0.00016654000000000001\n",
      "Epoch 1674, Loss: 0.5451231394348854, learning rate: 0.00016652\n",
      "Epoch 1675, Loss: 0.5451217462630079, learning rate: 0.0001665\n",
      "Epoch 1676, Loss: 0.5451203566867125, learning rate: 0.00016648000000000002\n",
      "Epoch 1677, Loss: 0.5451189625550087, learning rate: 0.00016646\n",
      "Epoch 1678, Loss: 0.5451175681742536, learning rate: 0.00016644\n",
      "Epoch 1679, Loss: 0.5451161764209762, learning rate: 0.00016642\n",
      "Epoch 1680, Loss: 0.5451147810801823, learning rate: 0.0001664\n",
      "Epoch 1681, Loss: 0.5451133902832214, learning rate: 0.00016638000000000002\n",
      "Epoch 1682, Loss: 0.5451119990294813, learning rate: 0.00016636\n",
      "Epoch 1683, Loss: 0.5451106049759912, learning rate: 0.00016634\n",
      "Epoch 1684, Loss: 0.5451092123593494, learning rate: 0.00016632000000000002\n",
      "Epoch 1685, Loss: 0.5451078218303155, learning rate: 0.0001663\n",
      "Epoch 1686, Loss: 0.5451064283925741, learning rate: 0.00016628000000000001\n",
      "Epoch 1687, Loss: 0.5451050379827249, learning rate: 0.00016626\n",
      "Epoch 1688, Loss: 0.5451036482823644, learning rate: 0.00016624\n",
      "Epoch 1689, Loss: 0.5451022552978133, learning rate: 0.00016622000000000002\n",
      "Epoch 1690, Loss: 0.5451008634253318, learning rate: 0.0001662\n",
      "Epoch 1691, Loss: 0.5450994755589783, learning rate: 0.00016618\n",
      "Epoch 1692, Loss: 0.545098082560347, learning rate: 0.00016616\n",
      "Epoch 1693, Loss: 0.5450966948657434, learning rate: 0.00016614\n",
      "Epoch 1694, Loss: 0.5450953030990093, learning rate: 0.00016612000000000002\n",
      "Epoch 1695, Loss: 0.545093913787197, learning rate: 0.0001661\n",
      "Epoch 1696, Loss: 0.5450925229452876, learning rate: 0.00016608\n",
      "Epoch 1697, Loss: 0.5450911324498317, learning rate: 0.00016606\n",
      "Epoch 1698, Loss: 0.5450897427975022, learning rate: 0.00016604\n",
      "Epoch 1699, Loss: 0.5450883539645976, learning rate: 0.00016602\n",
      "Epoch 1700, Loss: 0.5450869643018988, learning rate: 0.00016600000000000002\n",
      "Epoch 1701, Loss: 0.5450855748650381, learning rate: 0.00016598\n",
      "Epoch 1702, Loss: 0.5450841869300841, learning rate: 0.00016596\n",
      "Epoch 1703, Loss: 0.5450827979827045, learning rate: 0.00016594\n",
      "Epoch 1704, Loss: 0.5450814093939127, learning rate: 0.00016592\n",
      "Epoch 1705, Loss: 0.5450800230489747, learning rate: 0.00016590000000000002\n",
      "Epoch 1706, Loss: 0.5450786316413331, learning rate: 0.00016588\n",
      "Epoch 1707, Loss: 0.5450772464578615, learning rate: 0.00016586000000000001\n",
      "Epoch 1708, Loss: 0.5450758566173134, learning rate: 0.00016584\n",
      "Epoch 1709, Loss: 0.5450744675893542, learning rate: 0.00016582\n",
      "Epoch 1710, Loss: 0.5450730838108876, learning rate: 0.00016580000000000002\n",
      "Epoch 1711, Loss: 0.5450716946523679, learning rate: 0.00016578\n",
      "Epoch 1712, Loss: 0.5450703071994736, learning rate: 0.00016576\n",
      "Epoch 1713, Loss: 0.5450689223439539, learning rate: 0.00016574\n",
      "Epoch 1714, Loss: 0.5450675374083462, learning rate: 0.00016572\n",
      "Epoch 1715, Loss: 0.5450661469290039, learning rate: 0.00016570000000000002\n",
      "Epoch 1716, Loss: 0.5450647596029705, learning rate: 0.00016568\n",
      "Epoch 1717, Loss: 0.5450633748752793, learning rate: 0.00016566\n",
      "Epoch 1718, Loss: 0.545061986696954, learning rate: 0.00016564\n",
      "Epoch 1719, Loss: 0.5450606019742591, learning rate: 0.00016562\n",
      "Epoch 1720, Loss: 0.5450592136233372, learning rate: 0.00016560000000000001\n",
      "Epoch 1721, Loss: 0.5450578284826453, learning rate: 0.00016558000000000002\n",
      "Epoch 1722, Loss: 0.5450564426248673, learning rate: 0.00016556\n",
      "Epoch 1723, Loss: 0.5450550575337738, learning rate: 0.00016554000000000002\n",
      "Epoch 1724, Loss: 0.5450536739235274, learning rate: 0.00016552\n",
      "Epoch 1725, Loss: 0.5450522851618301, learning rate: 0.0001655\n",
      "Epoch 1726, Loss: 0.5450509022244763, learning rate: 0.00016548000000000002\n",
      "Epoch 1727, Loss: 0.5450495212736824, learning rate: 0.00016546\n",
      "Epoch 1728, Loss: 0.5450481319110408, learning rate: 0.00016544000000000002\n",
      "Epoch 1729, Loss: 0.5450467468799138, learning rate: 0.00016542\n",
      "Epoch 1730, Loss: 0.545045363145682, learning rate: 0.0001654\n",
      "Epoch 1731, Loss: 0.5450439781640115, learning rate: 0.00016538000000000002\n",
      "Epoch 1732, Loss: 0.5450425955005218, learning rate: 0.00016536\n",
      "Epoch 1733, Loss: 0.5450412102584545, learning rate: 0.00016534\n",
      "Epoch 1734, Loss: 0.5450398265153913, learning rate: 0.00016532\n",
      "Epoch 1735, Loss: 0.5450384459444696, learning rate: 0.0001653\n",
      "Epoch 1736, Loss: 0.5450370610771643, learning rate: 0.00016528000000000002\n",
      "Epoch 1737, Loss: 0.5450356771551621, learning rate: 0.00016526\n",
      "Epoch 1738, Loss: 0.5450342948214052, learning rate: 0.00016524\n",
      "Epoch 1739, Loss: 0.5450329114080298, learning rate: 0.00016522\n",
      "Epoch 1740, Loss: 0.5450315288715337, learning rate: 0.0001652\n",
      "Epoch 1741, Loss: 0.5450301477513535, learning rate: 0.00016518000000000001\n",
      "Epoch 1742, Loss: 0.5450287640303588, learning rate: 0.00016516\n",
      "Epoch 1743, Loss: 0.5450273835218932, learning rate: 0.00016514\n",
      "Epoch 1744, Loss: 0.5450260000192941, learning rate: 0.00016512000000000002\n",
      "Epoch 1745, Loss: 0.5450246196710218, learning rate: 0.0001651\n",
      "Epoch 1746, Loss: 0.5450232441645327, learning rate: 0.00016508\n",
      "Epoch 1747, Loss: 0.5450218584758345, learning rate: 0.00016506\n",
      "Epoch 1748, Loss: 0.5450204758591153, learning rate: 0.00016504\n",
      "Epoch 1749, Loss: 0.5450190965952814, learning rate: 0.00016502000000000002\n",
      "Epoch 1750, Loss: 0.545017713394875, learning rate: 0.000165\n",
      "Epoch 1751, Loss: 0.5450163335019186, learning rate: 0.00016498\n",
      "Epoch 1752, Loss: 0.545014952629445, learning rate: 0.00016496000000000002\n",
      "Epoch 1753, Loss: 0.5450135717484438, learning rate: 0.00016494\n",
      "Epoch 1754, Loss: 0.54501219359019, learning rate: 0.00016492\n",
      "Epoch 1755, Loss: 0.5450108111726603, learning rate: 0.0001649\n",
      "Epoch 1756, Loss: 0.5450094315792648, learning rate: 0.00016488\n",
      "Epoch 1757, Loss: 0.54500805210548, learning rate: 0.00016486000000000002\n",
      "Epoch 1758, Loss: 0.5450066752219612, learning rate: 0.00016484\n",
      "Epoch 1759, Loss: 0.5450052941800724, learning rate: 0.00016482\n",
      "Epoch 1760, Loss: 0.5450039140837971, learning rate: 0.0001648\n",
      "Epoch 1761, Loss: 0.5450025347990994, learning rate: 0.00016478\n",
      "Epoch 1762, Loss: 0.5450011584158729, learning rate: 0.00016476000000000001\n",
      "Epoch 1763, Loss: 0.5449997789865254, learning rate: 0.00016474\n",
      "Epoch 1764, Loss: 0.5449984000819338, learning rate: 0.00016472\n",
      "Epoch 1765, Loss: 0.5449970212733185, learning rate: 0.0001647\n",
      "Epoch 1766, Loss: 0.5449956428114575, learning rate: 0.00016468\n",
      "Epoch 1767, Loss: 0.5449942661106049, learning rate: 0.00016466\n",
      "Epoch 1768, Loss: 0.544992887779499, learning rate: 0.00016464000000000002\n",
      "Epoch 1769, Loss: 0.5449915140376003, learning rate: 0.00016462\n",
      "Epoch 1770, Loss: 0.5449901346783265, learning rate: 0.0001646\n",
      "Epoch 1771, Loss: 0.5449887562702265, learning rate: 0.00016458\n",
      "Epoch 1772, Loss: 0.5449873801555543, learning rate: 0.00016456\n",
      "Epoch 1773, Loss: 0.5449860019812174, learning rate: 0.00016454000000000002\n",
      "Epoch 1774, Loss: 0.5449846267185912, learning rate: 0.00016452\n",
      "Epoch 1775, Loss: 0.5449832497375452, learning rate: 0.00016450000000000001\n",
      "Epoch 1776, Loss: 0.5449818729251551, learning rate: 0.00016448\n",
      "Epoch 1777, Loss: 0.5449805014734931, learning rate: 0.00016446\n",
      "Epoch 1778, Loss: 0.5449791235637655, learning rate: 0.00016444000000000002\n",
      "Epoch 1779, Loss: 0.5449777452726265, learning rate: 0.00016442\n",
      "Epoch 1780, Loss: 0.5449763722258302, learning rate: 0.0001644\n",
      "Epoch 1781, Loss: 0.5449749972706475, learning rate: 0.00016438\n",
      "Epoch 1782, Loss: 0.5449736205266251, learning rate: 0.00016436\n",
      "Epoch 1783, Loss: 0.5449722455056958, learning rate: 0.00016434000000000002\n",
      "Epoch 1784, Loss: 0.5449708715532889, learning rate: 0.00016432\n",
      "Epoch 1785, Loss: 0.544969497559566, learning rate: 0.0001643\n",
      "Epoch 1786, Loss: 0.5449681203920904, learning rate: 0.00016428\n",
      "Epoch 1787, Loss: 0.5449667497168024, learning rate: 0.00016426\n",
      "Epoch 1788, Loss: 0.5449653717093189, learning rate: 0.00016424\n",
      "Epoch 1789, Loss: 0.5449639981770737, learning rate: 0.00016422000000000002\n",
      "Epoch 1790, Loss: 0.5449626257664437, learning rate: 0.0001642\n",
      "Epoch 1791, Loss: 0.5449612517461654, learning rate: 0.00016418000000000002\n",
      "Epoch 1792, Loss: 0.5449598844831632, learning rate: 0.00016416\n",
      "Epoch 1793, Loss: 0.5449585069473135, learning rate: 0.00016414\n",
      "Epoch 1794, Loss: 0.5449571321593412, learning rate: 0.00016412000000000002\n",
      "Epoch 1795, Loss: 0.5449557621070531, learning rate: 0.0001641\n",
      "Epoch 1796, Loss: 0.544954386930341, learning rate: 0.00016408000000000001\n",
      "Epoch 1797, Loss: 0.5449530143506806, learning rate: 0.00016406\n",
      "Epoch 1798, Loss: 0.5449516437855406, learning rate: 0.00016404\n",
      "Epoch 1799, Loss: 0.5449502712888118, learning rate: 0.00016402000000000002\n",
      "Epoch 1800, Loss: 0.5449488995137604, learning rate: 0.000164\n",
      "Epoch 1801, Loss: 0.5449475258907441, learning rate: 0.00016398\n",
      "Epoch 1802, Loss: 0.544946155051817, learning rate: 0.00016396\n",
      "Epoch 1803, Loss: 0.5449447836075754, learning rate: 0.00016394\n",
      "Epoch 1804, Loss: 0.5449434133947867, learning rate: 0.00016392000000000002\n",
      "Epoch 1805, Loss: 0.5449420453482837, learning rate: 0.0001639\n",
      "Epoch 1806, Loss: 0.544940674306056, learning rate: 0.00016388\n",
      "Epoch 1807, Loss: 0.5449393029624249, learning rate: 0.00016386\n",
      "Epoch 1808, Loss: 0.5449379346836432, learning rate: 0.00016384\n",
      "Epoch 1809, Loss: 0.5449365620526967, learning rate: 0.00016382000000000001\n",
      "Epoch 1810, Loss: 0.5449351897800341, learning rate: 0.0001638\n",
      "Epoch 1811, Loss: 0.5449338215939747, learning rate: 0.00016378\n",
      "Epoch 1812, Loss: 0.5449324498349927, learning rate: 0.00016376000000000002\n",
      "Epoch 1813, Loss: 0.5449310814854758, learning rate: 0.00016374\n",
      "Epoch 1814, Loss: 0.5449297112616114, learning rate: 0.00016372\n",
      "Epoch 1815, Loss: 0.5449283547587421, learning rate: 0.00016370000000000002\n",
      "Epoch 1816, Loss: 0.5449269768000501, learning rate: 0.00016368\n",
      "Epoch 1817, Loss: 0.5449256081031107, learning rate: 0.00016366000000000002\n",
      "Epoch 1818, Loss: 0.5449242453145765, learning rate: 0.00016364\n",
      "Epoch 1819, Loss: 0.544922874096164, learning rate: 0.00016362\n",
      "Epoch 1820, Loss: 0.5449215093206019, learning rate: 0.00016360000000000002\n",
      "Epoch 1821, Loss: 0.544920136400403, learning rate: 0.00016358\n",
      "Epoch 1822, Loss: 0.5449187725606133, learning rate: 0.00016356\n",
      "Epoch 1823, Loss: 0.5449174025601523, learning rate: 0.00016354\n",
      "Epoch 1824, Loss: 0.5449160345632605, learning rate: 0.00016352\n",
      "Epoch 1825, Loss: 0.5449146706708299, learning rate: 0.00016350000000000002\n",
      "Epoch 1826, Loss: 0.5449133064210511, learning rate: 0.00016348\n",
      "Epoch 1827, Loss: 0.544911937178052, learning rate: 0.00016346\n",
      "Epoch 1828, Loss: 0.5449105705297745, learning rate: 0.00016344\n",
      "Epoch 1829, Loss: 0.544909206602789, learning rate: 0.00016342\n",
      "Epoch 1830, Loss: 0.5449078439096786, learning rate: 0.00016340000000000001\n",
      "Epoch 1831, Loss: 0.5449064785221159, learning rate: 0.00016338\n",
      "Epoch 1832, Loss: 0.5449051061919836, learning rate: 0.00016336\n",
      "Epoch 1833, Loss: 0.544903747779908, learning rate: 0.00016334\n",
      "Epoch 1834, Loss: 0.5449023773746411, learning rate: 0.00016332\n",
      "Epoch 1835, Loss: 0.5449010196037278, learning rate: 0.0001633\n",
      "Epoch 1836, Loss: 0.5448996524585872, learning rate: 0.00016328000000000002\n",
      "Epoch 1837, Loss: 0.5448982872065464, learning rate: 0.00016326\n",
      "Epoch 1838, Loss: 0.5448969210347024, learning rate: 0.00016324000000000002\n",
      "Epoch 1839, Loss: 0.5448955610269867, learning rate: 0.00016322\n",
      "Epoch 1840, Loss: 0.5448942015695019, learning rate: 0.0001632\n",
      "Epoch 1841, Loss: 0.5448928351622091, learning rate: 0.00016318000000000002\n",
      "Epoch 1842, Loss: 0.5448914684501492, learning rate: 0.00016316\n",
      "Epoch 1843, Loss: 0.5448901061699082, learning rate: 0.00016314\n",
      "Epoch 1844, Loss: 0.5448887446362777, learning rate: 0.00016312\n",
      "Epoch 1845, Loss: 0.5448873788187739, learning rate: 0.0001631\n",
      "Epoch 1846, Loss: 0.5448860161480179, learning rate: 0.00016308000000000002\n",
      "Epoch 1847, Loss: 0.5448846568217041, learning rate: 0.00016306\n",
      "Epoch 1848, Loss: 0.5448832900085469, learning rate: 0.00016304\n",
      "Epoch 1849, Loss: 0.5448819328333222, learning rate: 0.00016302\n",
      "Epoch 1850, Loss: 0.5448805706795896, learning rate: 0.000163\n",
      "Epoch 1851, Loss: 0.5448792102462855, learning rate: 0.00016298000000000002\n",
      "Epoch 1852, Loss: 0.5448778423850058, learning rate: 0.00016296000000000003\n",
      "Epoch 1853, Loss: 0.5448764840345698, learning rate: 0.00016294\n",
      "Epoch 1854, Loss: 0.5448751266531568, learning rate: 0.00016292\n",
      "Epoch 1855, Loss: 0.5448737680436023, learning rate: 0.0001629\n",
      "Epoch 1856, Loss: 0.5448724050609575, learning rate: 0.00016288\n",
      "Epoch 1857, Loss: 0.5448710402211259, learning rate: 0.00016286000000000002\n",
      "Epoch 1858, Loss: 0.5448696808180865, learning rate: 0.00016284\n",
      "Epoch 1859, Loss: 0.5448683195032192, learning rate: 0.00016282000000000002\n",
      "Epoch 1860, Loss: 0.5448669649376517, learning rate: 0.0001628\n",
      "Epoch 1861, Loss: 0.5448656001350037, learning rate: 0.00016278\n",
      "Epoch 1862, Loss: 0.5448642455708127, learning rate: 0.00016276000000000002\n",
      "Epoch 1863, Loss: 0.5448628831072371, learning rate: 0.00016274\n",
      "Epoch 1864, Loss: 0.5448615216287038, learning rate: 0.00016272000000000001\n",
      "Epoch 1865, Loss: 0.5448601637070725, learning rate: 0.0001627\n",
      "Epoch 1866, Loss: 0.5448588031435664, learning rate: 0.00016268\n",
      "Epoch 1867, Loss: 0.5448574462053772, learning rate: 0.00016266000000000002\n",
      "Epoch 1868, Loss: 0.5448560911363832, learning rate: 0.00016264\n",
      "Epoch 1869, Loss: 0.5448547296817794, learning rate: 0.00016262\n",
      "Epoch 1870, Loss: 0.5448533722533799, learning rate: 0.0001626\n",
      "Epoch 1871, Loss: 0.5448520091977574, learning rate: 0.00016258\n",
      "Epoch 1872, Loss: 0.5448506536242999, learning rate: 0.00016256000000000002\n",
      "Epoch 1873, Loss: 0.5448492918829325, learning rate: 0.00016254\n",
      "Epoch 1874, Loss: 0.5448479432924419, learning rate: 0.00016252\n",
      "Epoch 1875, Loss: 0.5448465841533048, learning rate: 0.00016250000000000002\n",
      "Epoch 1876, Loss: 0.5448452270470372, learning rate: 0.00016248\n",
      "Epoch 1877, Loss: 0.5448438651466999, learning rate: 0.00016246\n",
      "Epoch 1878, Loss: 0.5448425112839513, learning rate: 0.00016244\n",
      "Epoch 1879, Loss: 0.544841151083743, learning rate: 0.00016242\n",
      "Epoch 1880, Loss: 0.5448397946879988, learning rate: 0.00016240000000000002\n",
      "Epoch 1881, Loss: 0.5448384443375435, learning rate: 0.00016238\n",
      "Epoch 1882, Loss: 0.5448370839643775, learning rate: 0.00016236\n",
      "Epoch 1883, Loss: 0.5448357287456143, learning rate: 0.00016234000000000002\n",
      "Epoch 1884, Loss: 0.5448343739269411, learning rate: 0.00016232\n",
      "Epoch 1885, Loss: 0.5448330265286375, learning rate: 0.00016230000000000001\n",
      "Epoch 1886, Loss: 0.5448316611606455, learning rate: 0.00016228\n",
      "Epoch 1887, Loss: 0.5448303095580509, learning rate: 0.00016226\n",
      "Epoch 1888, Loss: 0.5448289505487336, learning rate: 0.00016224000000000002\n",
      "Epoch 1889, Loss: 0.5448275978797454, learning rate: 0.00016222\n",
      "Epoch 1890, Loss: 0.5448262415968245, learning rate: 0.0001622\n",
      "Epoch 1891, Loss: 0.544824893329112, learning rate: 0.00016218\n",
      "Epoch 1892, Loss: 0.544823536355228, learning rate: 0.00016216\n",
      "Epoch 1893, Loss: 0.5448221901373024, learning rate: 0.00016214000000000002\n",
      "Epoch 1894, Loss: 0.5448208293559981, learning rate: 0.00016212\n",
      "Epoch 1895, Loss: 0.5448194731401773, learning rate: 0.0001621\n",
      "Epoch 1896, Loss: 0.5448181196299455, learning rate: 0.00016208\n",
      "Epoch 1897, Loss: 0.5448167684515104, learning rate: 0.00016206\n",
      "Epoch 1898, Loss: 0.5448154161539711, learning rate: 0.00016204000000000001\n",
      "Epoch 1899, Loss: 0.544814065427108, learning rate: 0.00016202\n",
      "Epoch 1900, Loss: 0.5448127099876708, learning rate: 0.000162\n",
      "Epoch 1901, Loss: 0.5448113621776312, learning rate: 0.00016198\n",
      "Epoch 1902, Loss: 0.5448100029526453, learning rate: 0.00016196\n",
      "Epoch 1903, Loss: 0.5448086518771311, learning rate: 0.00016194\n",
      "Epoch 1904, Loss: 0.5448072972604842, learning rate: 0.00016192000000000002\n",
      "Epoch 1905, Loss: 0.5448059463880276, learning rate: 0.0001619\n",
      "Epoch 1906, Loss: 0.5448046028965821, learning rate: 0.00016188000000000002\n",
      "Epoch 1907, Loss: 0.5448032427740627, learning rate: 0.00016186\n",
      "Epoch 1908, Loss: 0.5448018918466533, learning rate: 0.00016184\n",
      "Epoch 1909, Loss: 0.5448005428394436, learning rate: 0.00016182000000000002\n",
      "Epoch 1910, Loss: 0.5447991992164523, learning rate: 0.0001618\n",
      "Epoch 1911, Loss: 0.5447978433302867, learning rate: 0.00016178\n",
      "Epoch 1912, Loss: 0.544796489823856, learning rate: 0.00016176\n",
      "Epoch 1913, Loss: 0.5447951416969613, learning rate: 0.00016174\n",
      "Epoch 1914, Loss: 0.5447937946220234, learning rate: 0.00016172000000000002\n",
      "Epoch 1915, Loss: 0.5447924371778387, learning rate: 0.0001617\n",
      "Epoch 1916, Loss: 0.544791088429756, learning rate: 0.00016168\n",
      "Epoch 1917, Loss: 0.5447897449181173, learning rate: 0.00016166\n",
      "Epoch 1918, Loss: 0.5447883899201845, learning rate: 0.00016164\n",
      "Epoch 1919, Loss: 0.5447870366096645, learning rate: 0.00016162000000000001\n",
      "Epoch 1920, Loss: 0.5447856967690689, learning rate: 0.00016160000000000002\n",
      "Epoch 1921, Loss: 0.5447843369257862, learning rate: 0.00016158\n",
      "Epoch 1922, Loss: 0.5447829856191293, learning rate: 0.00016156\n",
      "Epoch 1923, Loss: 0.5447816398902171, learning rate: 0.00016154\n",
      "Epoch 1924, Loss: 0.5447802948752861, learning rate: 0.00016152\n",
      "Epoch 1925, Loss: 0.5447789403052972, learning rate: 0.00016150000000000002\n",
      "Epoch 1926, Loss: 0.5447775886700811, learning rate: 0.00016148\n",
      "Epoch 1927, Loss: 0.5447762389890766, learning rate: 0.00016146000000000002\n",
      "Epoch 1928, Loss: 0.5447748922467808, learning rate: 0.00016144\n",
      "Epoch 1929, Loss: 0.5447735497271406, learning rate: 0.00016142\n",
      "Epoch 1930, Loss: 0.5447721925940745, learning rate: 0.00016140000000000002\n",
      "Epoch 1931, Loss: 0.5447708484202484, learning rate: 0.00016138\n",
      "Epoch 1932, Loss: 0.5447694995792541, learning rate: 0.00016136000000000001\n",
      "Epoch 1933, Loss: 0.5447681575739018, learning rate: 0.00016134\n",
      "Epoch 1934, Loss: 0.5447668039721052, learning rate: 0.00016132\n",
      "Epoch 1935, Loss: 0.5447654616587443, learning rate: 0.00016130000000000002\n",
      "Epoch 1936, Loss: 0.5447641079686398, learning rate: 0.00016128\n",
      "Epoch 1937, Loss: 0.5447627630999203, learning rate: 0.00016126\n",
      "Epoch 1938, Loss: 0.5447614192107166, learning rate: 0.00016124\n",
      "Epoch 1939, Loss: 0.5447600653554266, learning rate: 0.00016122\n",
      "Epoch 1940, Loss: 0.5447587242091577, learning rate: 0.00016120000000000002\n",
      "Epoch 1941, Loss: 0.544757379811769, learning rate: 0.00016118\n",
      "Epoch 1942, Loss: 0.5447560289108372, learning rate: 0.00016116\n",
      "Epoch 1943, Loss: 0.5447546824371493, learning rate: 0.00016114000000000002\n",
      "Epoch 1944, Loss: 0.5447533386301739, learning rate: 0.00016112\n",
      "Epoch 1945, Loss: 0.5447519891997257, learning rate: 0.0001611\n",
      "Epoch 1946, Loss: 0.5447506484394612, learning rate: 0.00016108\n",
      "Epoch 1947, Loss: 0.5447492993322836, learning rate: 0.00016106\n",
      "Epoch 1948, Loss: 0.5447479637994079, learning rate: 0.00016104000000000002\n",
      "Epoch 1949, Loss: 0.5447466092197215, learning rate: 0.00016102\n",
      "Epoch 1950, Loss: 0.5447452646344395, learning rate: 0.000161\n",
      "Epoch 1951, Loss: 0.5447439214675626, learning rate: 0.00016098000000000002\n",
      "Epoch 1952, Loss: 0.5447425779049228, learning rate: 0.00016096\n",
      "Epoch 1953, Loss: 0.5447412371063555, learning rate: 0.00016094000000000001\n",
      "Epoch 1954, Loss: 0.5447398884935526, learning rate: 0.00016092\n",
      "Epoch 1955, Loss: 0.5447385433883033, learning rate: 0.0001609\n",
      "Epoch 1956, Loss: 0.544737201291386, learning rate: 0.00016088000000000002\n",
      "Epoch 1957, Loss: 0.5447358542953389, learning rate: 0.00016086\n",
      "Epoch 1958, Loss: 0.5447345201924075, learning rate: 0.00016084\n",
      "Epoch 1959, Loss: 0.5447331688150955, learning rate: 0.00016082\n",
      "Epoch 1960, Loss: 0.5447318278157194, learning rate: 0.0001608\n",
      "Epoch 1961, Loss: 0.5447304899012252, learning rate: 0.00016078000000000002\n",
      "Epoch 1962, Loss: 0.5447291410865581, learning rate: 0.00016076\n",
      "Epoch 1963, Loss: 0.5447277993012297, learning rate: 0.00016074\n",
      "Epoch 1964, Loss: 0.5447264627792057, learning rate: 0.00016072\n",
      "Epoch 1965, Loss: 0.5447251169639279, learning rate: 0.0001607\n",
      "Epoch 1966, Loss: 0.5447237717536174, learning rate: 0.00016068\n",
      "Epoch 1967, Loss: 0.5447224336504702, learning rate: 0.00016066000000000002\n",
      "Epoch 1968, Loss: 0.544721093710086, learning rate: 0.00016064\n",
      "Epoch 1969, Loss: 0.5447197474932411, learning rate: 0.00016062\n",
      "Epoch 1970, Loss: 0.5447184094842688, learning rate: 0.0001606\n",
      "Epoch 1971, Loss: 0.5447170634045889, learning rate: 0.00016058\n",
      "Epoch 1972, Loss: 0.5447157285340378, learning rate: 0.00016056000000000002\n",
      "Epoch 1973, Loss: 0.5447143818372916, learning rate: 0.00016054\n",
      "Epoch 1974, Loss: 0.5447130458817805, learning rate: 0.00016052000000000001\n",
      "Epoch 1975, Loss: 0.5447117071041098, learning rate: 0.0001605\n",
      "Epoch 1976, Loss: 0.5447103615408563, learning rate: 0.00016048\n",
      "Epoch 1977, Loss: 0.5447090193669034, learning rate: 0.00016046000000000002\n",
      "Epoch 1978, Loss: 0.5447076836770637, learning rate: 0.00016044\n",
      "Epoch 1979, Loss: 0.5447063459245708, learning rate: 0.00016042\n",
      "Epoch 1980, Loss: 0.544705000344261, learning rate: 0.0001604\n",
      "Epoch 1981, Loss: 0.5447036622285257, learning rate: 0.00016038\n",
      "Epoch 1982, Loss: 0.5447023244282686, learning rate: 0.00016036000000000002\n",
      "Epoch 1983, Loss: 0.5447009866720736, learning rate: 0.00016034\n",
      "Epoch 1984, Loss: 0.5446996444271668, learning rate: 0.00016032\n",
      "Epoch 1985, Loss: 0.5446983039053261, learning rate: 0.0001603\n",
      "Epoch 1986, Loss: 0.5446969700589105, learning rate: 0.00016028\n",
      "Epoch 1987, Loss: 0.5446956310898454, learning rate: 0.00016026000000000001\n",
      "Epoch 1988, Loss: 0.544694287346899, learning rate: 0.00016024000000000002\n",
      "Epoch 1989, Loss: 0.5446929511681871, learning rate: 0.00016022\n",
      "Epoch 1990, Loss: 0.5446916141564582, learning rate: 0.0001602\n",
      "Epoch 1991, Loss: 0.5446902784902226, learning rate: 0.00016018\n",
      "Epoch 1992, Loss: 0.5446889382033392, learning rate: 0.00016016\n",
      "Epoch 1993, Loss: 0.544687604440747, learning rate: 0.00016014000000000002\n",
      "Epoch 1994, Loss: 0.544686264752043, learning rate: 0.00016012\n",
      "Epoch 1995, Loss: 0.5446849243815017, learning rate: 0.00016010000000000002\n",
      "Epoch 1996, Loss: 0.5446835905566316, learning rate: 0.00016008\n",
      "Epoch 1997, Loss: 0.5446822557795735, learning rate: 0.00016006\n",
      "Epoch 1998, Loss: 0.5446809177395563, learning rate: 0.00016004000000000002\n",
      "Epoch 1999, Loss: 0.5446795836645961, learning rate: 0.00016002\n",
      "Epoch 2000, Loss: 0.5446782446313679, learning rate: 0.00016\n",
      "Epoch 2001, Loss: 0.5446769080589534, learning rate: 0.00015998\n",
      "Epoch 2002, Loss: 0.5446755723770571, learning rate: 0.00015996\n",
      "Epoch 2003, Loss: 0.5446742377318677, learning rate: 0.00015994000000000002\n",
      "Epoch 2004, Loss: 0.5446729025596494, learning rate: 0.00015992\n",
      "Epoch 2005, Loss: 0.5446715659530171, learning rate: 0.0001599\n",
      "Epoch 2006, Loss: 0.544670232728027, learning rate: 0.00015988\n",
      "Epoch 2007, Loss: 0.5446688991321371, learning rate: 0.00015986\n",
      "Epoch 2008, Loss: 0.5446675606484901, learning rate: 0.00015984000000000001\n",
      "Epoch 2009, Loss: 0.5446662256790069, learning rate: 0.00015982\n",
      "Epoch 2010, Loss: 0.5446648943971826, learning rate: 0.0001598\n",
      "Epoch 2011, Loss: 0.5446635611838901, learning rate: 0.00015978000000000002\n",
      "Epoch 2012, Loss: 0.5446622260966936, learning rate: 0.00015976\n",
      "Epoch 2013, Loss: 0.544660889150058, learning rate: 0.00015974\n",
      "Epoch 2014, Loss: 0.5446595569901349, learning rate: 0.00015972\n",
      "Epoch 2015, Loss: 0.5446582267302714, learning rate: 0.0001597\n",
      "Epoch 2016, Loss: 0.5446568931253504, learning rate: 0.00015968000000000002\n",
      "Epoch 2017, Loss: 0.5446555541085587, learning rate: 0.00015966\n",
      "Epoch 2018, Loss: 0.5446542232186942, learning rate: 0.00015964\n",
      "Epoch 2019, Loss: 0.544652894099498, learning rate: 0.00015962000000000002\n",
      "Epoch 2020, Loss: 0.5446515584912673, learning rate: 0.0001596\n",
      "Epoch 2021, Loss: 0.5446502254666474, learning rate: 0.00015958000000000001\n",
      "Epoch 2022, Loss: 0.5446488965299767, learning rate: 0.00015956\n",
      "Epoch 2023, Loss: 0.5446475619522103, learning rate: 0.00015954\n",
      "Epoch 2024, Loss: 0.5446462274540627, learning rate: 0.00015952000000000002\n",
      "Epoch 2025, Loss: 0.5446448996173531, learning rate: 0.0001595\n",
      "Epoch 2026, Loss: 0.5446435676237047, learning rate: 0.00015948\n",
      "Epoch 2027, Loss: 0.5446422380959248, learning rate: 0.00015946\n",
      "Epoch 2028, Loss: 0.5446409046224744, learning rate: 0.00015944\n",
      "Epoch 2029, Loss: 0.5446395731094886, learning rate: 0.00015942000000000002\n",
      "Epoch 2030, Loss: 0.5446382483997884, learning rate: 0.0001594\n",
      "Epoch 2031, Loss: 0.5446369107984782, learning rate: 0.00015938\n",
      "Epoch 2032, Loss: 0.5446355782351842, learning rate: 0.00015936\n",
      "Epoch 2033, Loss: 0.5446342489515379, learning rate: 0.00015934\n",
      "Epoch 2034, Loss: 0.5446329237663373, learning rate: 0.00015932\n",
      "Epoch 2035, Loss: 0.5446315897169673, learning rate: 0.00015930000000000002\n",
      "Epoch 2036, Loss: 0.5446302585051803, learning rate: 0.00015928\n",
      "Epoch 2037, Loss: 0.5446289332752097, learning rate: 0.00015926\n",
      "Epoch 2038, Loss: 0.5446276014755802, learning rate: 0.00015924\n",
      "Epoch 2039, Loss: 0.5446262684729226, learning rate: 0.00015922\n",
      "Epoch 2040, Loss: 0.5446249445225366, learning rate: 0.00015920000000000002\n",
      "Epoch 2041, Loss: 0.544623610347331, learning rate: 0.00015918\n",
      "Epoch 2042, Loss: 0.5446222809821885, learning rate: 0.00015916000000000001\n",
      "Epoch 2043, Loss: 0.5446209537966131, learning rate: 0.00015914\n",
      "Epoch 2044, Loss: 0.5446196265918484, learning rate: 0.00015912\n",
      "Epoch 2045, Loss: 0.5446182947896201, learning rate: 0.00015910000000000002\n",
      "Epoch 2046, Loss: 0.5446169670276987, learning rate: 0.00015908\n",
      "Epoch 2047, Loss: 0.5446156388906428, learning rate: 0.00015906\n",
      "Epoch 2048, Loss: 0.5446143144112373, learning rate: 0.00015904\n",
      "Epoch 2049, Loss: 0.5446129865473903, learning rate: 0.00015902\n",
      "Epoch 2050, Loss: 0.5446116546604907, learning rate: 0.00015900000000000002\n",
      "Epoch 2051, Loss: 0.5446103289347886, learning rate: 0.00015898\n",
      "Epoch 2052, Loss: 0.5446090048532665, learning rate: 0.00015896\n",
      "Epoch 2053, Loss: 0.5446076733830196, learning rate: 0.00015894\n",
      "Epoch 2054, Loss: 0.5446063463482231, learning rate: 0.00015892\n",
      "Epoch 2055, Loss: 0.544605020048573, learning rate: 0.0001589\n",
      "Epoch 2056, Loss: 0.5446036983526727, learning rate: 0.00015888000000000002\n",
      "Epoch 2057, Loss: 0.5446023678627762, learning rate: 0.00015886\n",
      "Epoch 2058, Loss: 0.5446010390367279, learning rate: 0.00015884000000000002\n",
      "Epoch 2059, Loss: 0.5445997124895587, learning rate: 0.00015882\n",
      "Epoch 2060, Loss: 0.5445983876953338, learning rate: 0.0001588\n",
      "Epoch 2061, Loss: 0.5445970655936037, learning rate: 0.00015878000000000002\n",
      "Epoch 2062, Loss: 0.5445957357355005, learning rate: 0.00015876\n",
      "Epoch 2063, Loss: 0.5445944166958604, learning rate: 0.00015874000000000001\n",
      "Epoch 2064, Loss: 0.544593086342013, learning rate: 0.00015872\n",
      "Epoch 2065, Loss: 0.5445917602130761, learning rate: 0.0001587\n",
      "Epoch 2066, Loss: 0.5445904348490103, learning rate: 0.00015868000000000002\n",
      "Epoch 2067, Loss: 0.5445891154479795, learning rate: 0.00015866\n",
      "Epoch 2068, Loss: 0.5445877872730357, learning rate: 0.00015864\n",
      "Epoch 2069, Loss: 0.5445864603097288, learning rate: 0.00015862\n",
      "Epoch 2070, Loss: 0.5445851392617851, learning rate: 0.0001586\n",
      "Epoch 2071, Loss: 0.5445838210131925, learning rate: 0.00015858000000000002\n",
      "Epoch 2072, Loss: 0.5445824897025449, learning rate: 0.00015856\n",
      "Epoch 2073, Loss: 0.5445811673518898, learning rate: 0.00015854\n",
      "Epoch 2074, Loss: 0.5445798457025638, learning rate: 0.00015852\n",
      "Epoch 2075, Loss: 0.5445785207408995, learning rate: 0.0001585\n",
      "Epoch 2076, Loss: 0.5445771959997802, learning rate: 0.00015848000000000001\n",
      "Epoch 2077, Loss: 0.5445758730128292, learning rate: 0.00015846\n",
      "Epoch 2078, Loss: 0.544574554063333, learning rate: 0.00015844\n",
      "Epoch 2079, Loss: 0.5445732262622742, learning rate: 0.00015842000000000002\n",
      "Epoch 2080, Loss: 0.5445719090946222, learning rate: 0.0001584\n",
      "Epoch 2081, Loss: 0.544570587150429, learning rate: 0.00015838\n",
      "Epoch 2082, Loss: 0.5445692612525804, learning rate: 0.00015836000000000002\n",
      "Epoch 2083, Loss: 0.5445679392024508, learning rate: 0.00015834\n",
      "Epoch 2084, Loss: 0.544566616612626, learning rate: 0.00015832000000000002\n",
      "Epoch 2085, Loss: 0.5445653062930464, learning rate: 0.0001583\n",
      "Epoch 2086, Loss: 0.5445639737508847, learning rate: 0.00015828\n",
      "Epoch 2087, Loss: 0.5445626521709647, learning rate: 0.00015826000000000002\n",
      "Epoch 2088, Loss: 0.5445613330392912, learning rate: 0.00015824\n",
      "Epoch 2089, Loss: 0.544560011882789, learning rate: 0.00015822\n",
      "Epoch 2090, Loss: 0.5445586967385336, learning rate: 0.0001582\n",
      "Epoch 2091, Loss: 0.5445573718816978, learning rate: 0.00015818\n",
      "Epoch 2092, Loss: 0.5445560536260851, learning rate: 0.00015816000000000002\n",
      "Epoch 2093, Loss: 0.5445547302729867, learning rate: 0.00015814\n",
      "Epoch 2094, Loss: 0.5445534115719052, learning rate: 0.00015812\n",
      "Epoch 2095, Loss: 0.5445520949956177, learning rate: 0.0001581\n",
      "Epoch 2096, Loss: 0.5445507711392384, learning rate: 0.00015808\n",
      "Epoch 2097, Loss: 0.5445494516185289, learning rate: 0.00015806000000000001\n",
      "Epoch 2098, Loss: 0.5445481338846466, learning rate: 0.00015804\n",
      "Epoch 2099, Loss: 0.544546816259039, learning rate: 0.00015802\n",
      "Epoch 2100, Loss: 0.544545496247524, learning rate: 0.000158\n",
      "Epoch 2101, Loss: 0.5445441766641237, learning rate: 0.00015798\n",
      "Epoch 2102, Loss: 0.5445428609015753, learning rate: 0.00015796\n",
      "Epoch 2103, Loss: 0.5445415378150689, learning rate: 0.00015794000000000002\n",
      "Epoch 2104, Loss: 0.5445402237206196, learning rate: 0.00015792\n",
      "Epoch 2105, Loss: 0.5445389037817329, learning rate: 0.0001579\n",
      "Epoch 2106, Loss: 0.544537587672247, learning rate: 0.00015788\n",
      "Epoch 2107, Loss: 0.5445362659295105, learning rate: 0.00015786\n",
      "Epoch 2108, Loss: 0.5445349503500365, learning rate: 0.00015784000000000002\n",
      "Epoch 2109, Loss: 0.5445336350271511, learning rate: 0.00015782\n",
      "Epoch 2110, Loss: 0.5445323199846142, learning rate: 0.00015780000000000001\n",
      "Epoch 2111, Loss: 0.5445310022085823, learning rate: 0.00015778\n",
      "Epoch 2112, Loss: 0.544529681066704, learning rate: 0.00015776\n",
      "Epoch 2113, Loss: 0.5445283681722201, learning rate: 0.00015774000000000002\n",
      "Epoch 2114, Loss: 0.544527052520907, learning rate: 0.00015772\n",
      "Epoch 2115, Loss: 0.5445257331916254, learning rate: 0.0001577\n",
      "Epoch 2116, Loss: 0.5445244160669755, learning rate: 0.00015768\n",
      "Epoch 2117, Loss: 0.5445231023214794, learning rate: 0.00015766\n",
      "Epoch 2118, Loss: 0.544521786475919, learning rate: 0.00015764000000000002\n",
      "Epoch 2119, Loss: 0.5445204723056175, learning rate: 0.00015762\n",
      "Epoch 2120, Loss: 0.5445191597327325, learning rate: 0.0001576\n",
      "Epoch 2121, Loss: 0.544517841744046, learning rate: 0.00015758\n",
      "Epoch 2122, Loss: 0.5445165243281022, learning rate: 0.00015756\n",
      "Epoch 2123, Loss: 0.5445152139412768, learning rate: 0.00015754\n",
      "Epoch 2124, Loss: 0.5445138971130647, learning rate: 0.00015752000000000002\n",
      "Epoch 2125, Loss: 0.5445125817389773, learning rate: 0.0001575\n",
      "Epoch 2126, Loss: 0.5445112664788408, learning rate: 0.00015748000000000002\n",
      "Epoch 2127, Loss: 0.5445099549778635, learning rate: 0.00015746\n",
      "Epoch 2128, Loss: 0.5445086406026692, learning rate: 0.00015744\n",
      "Epoch 2129, Loss: 0.5445073271646733, learning rate: 0.00015742000000000002\n",
      "Epoch 2130, Loss: 0.5445060103778292, learning rate: 0.0001574\n",
      "Epoch 2131, Loss: 0.5445046974397252, learning rate: 0.00015738000000000001\n",
      "Epoch 2132, Loss: 0.5445033878050104, learning rate: 0.00015736\n",
      "Epoch 2133, Loss: 0.5445020738085488, learning rate: 0.00015734\n",
      "Epoch 2134, Loss: 0.5445007617647856, learning rate: 0.00015732000000000002\n",
      "Epoch 2135, Loss: 0.5444994462514715, learning rate: 0.0001573\n",
      "Epoch 2136, Loss: 0.544498135138377, learning rate: 0.00015728\n",
      "Epoch 2137, Loss: 0.544496820200575, learning rate: 0.00015726\n",
      "Epoch 2138, Loss: 0.544495508947002, learning rate: 0.00015724\n",
      "Epoch 2139, Loss: 0.544494199269718, learning rate: 0.00015722000000000002\n",
      "Epoch 2140, Loss: 0.5444928909272306, learning rate: 0.0001572\n",
      "Epoch 2141, Loss: 0.5444915721932133, learning rate: 0.00015718\n",
      "Epoch 2142, Loss: 0.5444902627965199, learning rate: 0.00015716\n",
      "Epoch 2143, Loss: 0.5444889479827282, learning rate: 0.00015714\n",
      "Epoch 2144, Loss: 0.5444876379192853, learning rate: 0.00015712000000000001\n",
      "Epoch 2145, Loss: 0.5444863259071668, learning rate: 0.0001571\n",
      "Epoch 2146, Loss: 0.5444850207412737, learning rate: 0.00015708\n",
      "Epoch 2147, Loss: 0.5444837055447534, learning rate: 0.00015706000000000002\n",
      "Epoch 2148, Loss: 0.5444823918979653, learning rate: 0.00015704\n",
      "Epoch 2149, Loss: 0.5444810862753693, learning rate: 0.00015702\n",
      "Epoch 2150, Loss: 0.5444797737444228, learning rate: 0.00015700000000000002\n",
      "Epoch 2151, Loss: 0.5444784633285616, learning rate: 0.00015698\n",
      "Epoch 2152, Loss: 0.5444771527705295, learning rate: 0.00015696000000000002\n",
      "Epoch 2153, Loss: 0.5444758409485174, learning rate: 0.00015694\n",
      "Epoch 2154, Loss: 0.5444745326042232, learning rate: 0.00015692\n",
      "Epoch 2155, Loss: 0.5444732229396718, learning rate: 0.00015690000000000002\n",
      "Epoch 2156, Loss: 0.544471917849118, learning rate: 0.00015688\n",
      "Epoch 2157, Loss: 0.5444706051232213, learning rate: 0.00015686\n",
      "Epoch 2158, Loss: 0.5444692915178639, learning rate: 0.00015684\n",
      "Epoch 2159, Loss: 0.5444679840052858, learning rate: 0.00015682\n",
      "Epoch 2160, Loss: 0.5444666785112666, learning rate: 0.00015680000000000002\n",
      "Epoch 2161, Loss: 0.5444653687562988, learning rate: 0.00015678\n",
      "Epoch 2162, Loss: 0.5444640567120093, learning rate: 0.00015676\n",
      "Epoch 2163, Loss: 0.5444627497158255, learning rate: 0.00015674\n",
      "Epoch 2164, Loss: 0.5444614419816287, learning rate: 0.00015672\n",
      "Epoch 2165, Loss: 0.5444601333838368, learning rate: 0.00015670000000000001\n",
      "Epoch 2166, Loss: 0.5444588247443435, learning rate: 0.00015668\n",
      "Epoch 2167, Loss: 0.5444575193289134, learning rate: 0.00015666\n",
      "Epoch 2168, Loss: 0.5444562181130357, learning rate: 0.00015664\n",
      "Epoch 2169, Loss: 0.5444549036689696, learning rate: 0.00015662\n",
      "Epoch 2170, Loss: 0.5444535927480404, learning rate: 0.0001566\n",
      "Epoch 2171, Loss: 0.5444522903245985, learning rate: 0.00015658000000000002\n",
      "Epoch 2172, Loss: 0.5444509808657196, learning rate: 0.00015656\n",
      "Epoch 2173, Loss: 0.544449681968749, learning rate: 0.00015654\n",
      "Epoch 2174, Loss: 0.5444483746273158, learning rate: 0.00015652\n",
      "Epoch 2175, Loss: 0.5444470640954865, learning rate: 0.0001565\n",
      "Epoch 2176, Loss: 0.5444457560237647, learning rate: 0.00015648000000000002\n",
      "Epoch 2177, Loss: 0.5444444527201453, learning rate: 0.00015646\n",
      "Epoch 2178, Loss: 0.5444431501239402, learning rate: 0.00015644\n",
      "Epoch 2179, Loss: 0.544441846224349, learning rate: 0.00015642\n",
      "Epoch 2180, Loss: 0.5444405371432343, learning rate: 0.0001564\n",
      "Epoch 2181, Loss: 0.5444392306524746, learning rate: 0.00015638000000000002\n",
      "Epoch 2182, Loss: 0.5444379259930439, learning rate: 0.00015636\n",
      "Epoch 2183, Loss: 0.5444366188527419, learning rate: 0.00015634\n",
      "Epoch 2184, Loss: 0.5444353219096784, learning rate: 0.00015632\n",
      "Epoch 2185, Loss: 0.5444340154864767, learning rate: 0.0001563\n",
      "Epoch 2186, Loss: 0.5444327098320205, learning rate: 0.00015628000000000001\n",
      "Epoch 2187, Loss: 0.5444314026322935, learning rate: 0.00015626000000000003\n",
      "Epoch 2188, Loss: 0.5444301012982181, learning rate: 0.00015624\n",
      "Epoch 2189, Loss: 0.5444287991309043, learning rate: 0.00015622\n",
      "Epoch 2190, Loss: 0.5444274984414194, learning rate: 0.0001562\n",
      "Epoch 2191, Loss: 0.5444261909426462, learning rate: 0.00015618\n",
      "Epoch 2192, Loss: 0.5444248885991437, learning rate: 0.00015616000000000002\n",
      "Epoch 2193, Loss: 0.5444235853535334, learning rate: 0.00015614\n",
      "Epoch 2194, Loss: 0.5444222800372999, learning rate: 0.00015612000000000002\n",
      "Epoch 2195, Loss: 0.544420978396847, learning rate: 0.0001561\n",
      "Epoch 2196, Loss: 0.5444196788707342, learning rate: 0.00015608\n",
      "Epoch 2197, Loss: 0.5444183740666231, learning rate: 0.00015606000000000002\n",
      "Epoch 2198, Loss: 0.5444170722008769, learning rate: 0.00015604\n",
      "Epoch 2199, Loss: 0.5444157712716177, learning rate: 0.00015602000000000001\n",
      "Epoch 2200, Loss: 0.5444144685227394, learning rate: 0.000156\n",
      "Epoch 2201, Loss: 0.5444131718969333, learning rate: 0.00015598\n",
      "Epoch 2202, Loss: 0.5444118693391726, learning rate: 0.00015596000000000002\n",
      "Epoch 2203, Loss: 0.5444105642327112, learning rate: 0.00015594\n",
      "Epoch 2204, Loss: 0.5444092652700552, learning rate: 0.00015592\n",
      "Epoch 2205, Loss: 0.5444079637019834, learning rate: 0.0001559\n",
      "Epoch 2206, Loss: 0.5444066603620933, learning rate: 0.00015588\n",
      "Epoch 2207, Loss: 0.5444053682137071, learning rate: 0.00015586000000000002\n",
      "Epoch 2208, Loss: 0.5444040630803078, learning rate: 0.00015584\n",
      "Epoch 2209, Loss: 0.5444027654262245, learning rate: 0.00015582\n",
      "Epoch 2210, Loss: 0.5444014623318135, learning rate: 0.0001558\n",
      "Epoch 2211, Loss: 0.5444001608097984, learning rate: 0.00015578\n",
      "Epoch 2212, Loss: 0.5443988628384877, learning rate: 0.00015576\n",
      "Epoch 2213, Loss: 0.5443975638371882, learning rate: 0.00015574\n",
      "Epoch 2214, Loss: 0.5443962621999678, learning rate: 0.00015572\n",
      "Epoch 2215, Loss: 0.5443949697177908, learning rate: 0.00015570000000000002\n",
      "Epoch 2216, Loss: 0.5443936671647738, learning rate: 0.00015568\n",
      "Epoch 2217, Loss: 0.5443923667051056, learning rate: 0.00015566\n",
      "Epoch 2218, Loss: 0.544391071139411, learning rate: 0.00015564000000000002\n",
      "Epoch 2219, Loss: 0.5443897717140362, learning rate: 0.00015562\n",
      "Epoch 2220, Loss: 0.5443884715013875, learning rate: 0.00015560000000000001\n",
      "Epoch 2221, Loss: 0.5443871751864549, learning rate: 0.00015558\n",
      "Epoch 2222, Loss: 0.5443858740954056, learning rate: 0.00015556\n",
      "Epoch 2223, Loss: 0.5443845820052117, learning rate: 0.00015554000000000002\n",
      "Epoch 2224, Loss: 0.5443832832827601, learning rate: 0.00015552\n",
      "Epoch 2225, Loss: 0.5443819865648587, learning rate: 0.0001555\n",
      "Epoch 2226, Loss: 0.5443806850395536, learning rate: 0.00015548\n",
      "Epoch 2227, Loss: 0.5443793907940903, learning rate: 0.00015546\n",
      "Epoch 2228, Loss: 0.5443780926493931, learning rate: 0.00015544000000000002\n",
      "Epoch 2229, Loss: 0.544376797139398, learning rate: 0.00015542\n",
      "Epoch 2230, Loss: 0.5443755033421173, learning rate: 0.0001554\n",
      "Epoch 2231, Loss: 0.5443742055053973, learning rate: 0.00015538\n",
      "Epoch 2232, Loss: 0.5443729049089896, learning rate: 0.00015536\n",
      "Epoch 2233, Loss: 0.5443716179542463, learning rate: 0.00015534000000000001\n",
      "Epoch 2234, Loss: 0.5443703184706261, learning rate: 0.00015532\n",
      "Epoch 2235, Loss: 0.5443690213092204, learning rate: 0.0001553\n",
      "Epoch 2236, Loss: 0.5443677235214169, learning rate: 0.00015528\n",
      "Epoch 2237, Loss: 0.5443664284669496, learning rate: 0.00015526\n",
      "Epoch 2238, Loss: 0.5443651347372469, learning rate: 0.00015524\n",
      "Epoch 2239, Loss: 0.5443638407899689, learning rate: 0.00015522000000000002\n",
      "Epoch 2240, Loss: 0.5443625482251218, learning rate: 0.0001552\n",
      "Epoch 2241, Loss: 0.5443612539316846, learning rate: 0.00015518000000000002\n",
      "Epoch 2242, Loss: 0.5443599592945587, learning rate: 0.00015516\n",
      "Epoch 2243, Loss: 0.5443586611947729, learning rate: 0.00015514\n",
      "Epoch 2244, Loss: 0.5443573694606957, learning rate: 0.00015512000000000002\n",
      "Epoch 2245, Loss: 0.5443560758938882, learning rate: 0.0001551\n",
      "Epoch 2246, Loss: 0.5443547811166676, learning rate: 0.00015508\n",
      "Epoch 2247, Loss: 0.5443534931244937, learning rate: 0.00015506\n",
      "Epoch 2248, Loss: 0.5443521973542135, learning rate: 0.00015504\n",
      "Epoch 2249, Loss: 0.5443509048917566, learning rate: 0.00015502000000000002\n",
      "Epoch 2250, Loss: 0.5443496120548429, learning rate: 0.000155\n",
      "Epoch 2251, Loss: 0.5443483198160597, learning rate: 0.00015498\n",
      "Epoch 2252, Loss: 0.5443470251492224, learning rate: 0.00015496\n",
      "Epoch 2253, Loss: 0.5443457345527023, learning rate: 0.00015494\n",
      "Epoch 2254, Loss: 0.5443444482682489, learning rate: 0.00015492000000000001\n",
      "Epoch 2255, Loss: 0.5443431513339475, learning rate: 0.00015490000000000002\n",
      "Epoch 2256, Loss: 0.5443418574949583, learning rate: 0.00015488\n",
      "Epoch 2257, Loss: 0.5443405717198085, learning rate: 0.00015486\n",
      "Epoch 2258, Loss: 0.5443392799388753, learning rate: 0.00015484\n",
      "Epoch 2259, Loss: 0.5443379866651793, learning rate: 0.00015482\n",
      "Epoch 2260, Loss: 0.5443366954046037, learning rate: 0.00015480000000000002\n",
      "Epoch 2261, Loss: 0.5443354061788823, learning rate: 0.00015478\n",
      "Epoch 2262, Loss: 0.544334112150104, learning rate: 0.00015476000000000002\n",
      "Epoch 2263, Loss: 0.5443328208432062, learning rate: 0.00015474\n",
      "Epoch 2264, Loss: 0.5443315340261572, learning rate: 0.00015472\n",
      "Epoch 2265, Loss: 0.5443302451381958, learning rate: 0.00015470000000000002\n",
      "Epoch 2266, Loss: 0.5443289541728001, learning rate: 0.00015468\n",
      "Epoch 2267, Loss: 0.5443276629955992, learning rate: 0.00015466\n",
      "Epoch 2268, Loss: 0.5443263784827205, learning rate: 0.00015464\n",
      "Epoch 2269, Loss: 0.5443250868700745, learning rate: 0.00015462\n",
      "Epoch 2270, Loss: 0.5443237960045838, learning rate: 0.00015460000000000002\n",
      "Epoch 2271, Loss: 0.5443225062618, learning rate: 0.00015458\n",
      "Epoch 2272, Loss: 0.5443212179144646, learning rate: 0.00015456\n",
      "Epoch 2273, Loss: 0.5443199301299113, learning rate: 0.00015454\n",
      "Epoch 2274, Loss: 0.5443186397330455, learning rate: 0.00015452\n",
      "Epoch 2275, Loss: 0.5443173533154383, learning rate: 0.00015450000000000001\n",
      "Epoch 2276, Loss: 0.5443160652092047, learning rate: 0.00015448\n",
      "Epoch 2277, Loss: 0.5443147772277076, learning rate: 0.00015446\n",
      "Epoch 2278, Loss: 0.5443134859878053, learning rate: 0.00015444000000000002\n",
      "Epoch 2279, Loss: 0.5443122002707756, learning rate: 0.00015442\n",
      "Epoch 2280, Loss: 0.5443109168392402, learning rate: 0.0001544\n",
      "Epoch 2281, Loss: 0.5443096268026403, learning rate: 0.00015438000000000002\n",
      "Epoch 2282, Loss: 0.5443083431667004, learning rate: 0.00015436\n",
      "Epoch 2283, Loss: 0.5443070541030254, learning rate: 0.00015434000000000002\n",
      "Epoch 2284, Loss: 0.544305770343124, learning rate: 0.00015432\n",
      "Epoch 2285, Loss: 0.5443044782858986, learning rate: 0.0001543\n",
      "Epoch 2286, Loss: 0.5443031930319499, learning rate: 0.00015428000000000002\n",
      "Epoch 2287, Loss: 0.5443019086080579, learning rate: 0.00015426\n",
      "Epoch 2288, Loss: 0.5443006192442563, learning rate: 0.00015424000000000001\n",
      "Epoch 2289, Loss: 0.5442993370571032, learning rate: 0.00015422\n",
      "Epoch 2290, Loss: 0.5442980513389165, learning rate: 0.0001542\n",
      "Epoch 2291, Loss: 0.5442967679538436, learning rate: 0.00015418000000000002\n",
      "Epoch 2292, Loss: 0.5442954771187579, learning rate: 0.00015416\n",
      "Epoch 2293, Loss: 0.544294193718584, learning rate: 0.00015414\n",
      "Epoch 2294, Loss: 0.5442929118093073, learning rate: 0.00015412\n",
      "Epoch 2295, Loss: 0.5442916274565868, learning rate: 0.0001541\n",
      "Epoch 2296, Loss: 0.5442903410250698, learning rate: 0.00015408000000000002\n",
      "Epoch 2297, Loss: 0.544289053946111, learning rate: 0.00015406\n",
      "Epoch 2298, Loss: 0.5442877748303697, learning rate: 0.00015404\n",
      "Epoch 2299, Loss: 0.5442864861161766, learning rate: 0.00015402\n",
      "Epoch 2300, Loss: 0.5442852048751832, learning rate: 0.000154\n",
      "Epoch 2301, Loss: 0.5442839217113596, learning rate: 0.00015398\n",
      "Epoch 2302, Loss: 0.5442826386995878, learning rate: 0.00015396\n",
      "Epoch 2303, Loss: 0.5442813527165181, learning rate: 0.00015394\n",
      "Epoch 2304, Loss: 0.544280068495621, learning rate: 0.00015392\n",
      "Epoch 2305, Loss: 0.5442787878838219, learning rate: 0.0001539\n",
      "Epoch 2306, Loss: 0.5442775065249598, learning rate: 0.00015388\n",
      "Epoch 2307, Loss: 0.5442762228114366, learning rate: 0.00015386000000000002\n",
      "Epoch 2308, Loss: 0.5442749410974337, learning rate: 0.00015384\n",
      "Epoch 2309, Loss: 0.5442736570381059, learning rate: 0.00015382000000000001\n",
      "Epoch 2310, Loss: 0.544272376590253, learning rate: 0.0001538\n",
      "Epoch 2311, Loss: 0.5442710905462632, learning rate: 0.00015378\n",
      "Epoch 2312, Loss: 0.5442698123199635, learning rate: 0.00015376000000000002\n",
      "Epoch 2313, Loss: 0.5442685311153179, learning rate: 0.00015374\n",
      "Epoch 2314, Loss: 0.5442672452628936, learning rate: 0.00015372\n",
      "Epoch 2315, Loss: 0.5442659717780938, learning rate: 0.0001537\n",
      "Epoch 2316, Loss: 0.5442646883506984, learning rate: 0.00015368\n",
      "Epoch 2317, Loss: 0.5442634069833048, learning rate: 0.00015366000000000002\n",
      "Epoch 2318, Loss: 0.5442621221484168, learning rate: 0.00015364\n",
      "Epoch 2319, Loss: 0.5442608453689878, learning rate: 0.00015362\n",
      "Epoch 2320, Loss: 0.5442595637129402, learning rate: 0.0001536\n",
      "Epoch 2321, Loss: 0.5442582792389036, learning rate: 0.00015358\n",
      "Epoch 2322, Loss: 0.5442570030187005, learning rate: 0.00015356000000000001\n",
      "Epoch 2323, Loss: 0.5442557196131007, learning rate: 0.00015354000000000002\n",
      "Epoch 2324, Loss: 0.5442544404701883, learning rate: 0.00015352\n",
      "Epoch 2325, Loss: 0.5442531622139183, learning rate: 0.0001535\n",
      "Epoch 2326, Loss: 0.5442518880271765, learning rate: 0.00015348\n",
      "Epoch 2327, Loss: 0.5442506040010913, learning rate: 0.00015346\n",
      "Epoch 2328, Loss: 0.5442493247048814, learning rate: 0.00015344000000000002\n",
      "Epoch 2329, Loss: 0.5442480491441926, learning rate: 0.00015342\n",
      "Epoch 2330, Loss: 0.5442467711411019, learning rate: 0.00015340000000000002\n",
      "Epoch 2331, Loss: 0.5442454903024474, learning rate: 0.00015338\n",
      "Epoch 2332, Loss: 0.5442442096315757, learning rate: 0.00015336\n",
      "Epoch 2333, Loss: 0.5442429361110573, learning rate: 0.00015334000000000002\n",
      "Epoch 2334, Loss: 0.5442416540007886, learning rate: 0.00015332\n",
      "Epoch 2335, Loss: 0.5442403774862742, learning rate: 0.0001533\n",
      "Epoch 2336, Loss: 0.5442390999022848, learning rate: 0.00015328\n",
      "Epoch 2337, Loss: 0.5442378209651568, learning rate: 0.00015326\n",
      "Epoch 2338, Loss: 0.5442365427479858, learning rate: 0.00015324000000000002\n",
      "Epoch 2339, Loss: 0.5442352668789812, learning rate: 0.00015322\n",
      "Epoch 2340, Loss: 0.5442339895659986, learning rate: 0.0001532\n",
      "Epoch 2341, Loss: 0.5442327149255012, learning rate: 0.00015318\n",
      "Epoch 2342, Loss: 0.5442314431381704, learning rate: 0.00015316\n",
      "Epoch 2343, Loss: 0.5442301657470096, learning rate: 0.00015314000000000001\n",
      "Epoch 2344, Loss: 0.5442288873101674, learning rate: 0.00015312\n",
      "Epoch 2345, Loss: 0.5442276080381635, learning rate: 0.0001531\n",
      "Epoch 2346, Loss: 0.5442263389645522, learning rate: 0.00015308000000000002\n",
      "Epoch 2347, Loss: 0.5442250577502381, learning rate: 0.00015306\n",
      "Epoch 2348, Loss: 0.5442237837828875, learning rate: 0.00015304\n",
      "Epoch 2349, Loss: 0.5442225151465386, learning rate: 0.00015302000000000002\n",
      "Epoch 2350, Loss: 0.544221234476444, learning rate: 0.000153\n",
      "Epoch 2351, Loss: 0.5442199636578939, learning rate: 0.00015298000000000002\n",
      "Epoch 2352, Loss: 0.5442186834229042, learning rate: 0.00015296\n",
      "Epoch 2353, Loss: 0.5442174097257555, learning rate: 0.00015294\n",
      "Epoch 2354, Loss: 0.5442161392459948, learning rate: 0.00015292000000000002\n",
      "Epoch 2355, Loss: 0.5442148640272079, learning rate: 0.0001529\n",
      "Epoch 2356, Loss: 0.5442135935344625, learning rate: 0.00015288\n",
      "Epoch 2357, Loss: 0.5442123155741474, learning rate: 0.00015286\n",
      "Epoch 2358, Loss: 0.5442110434813152, learning rate: 0.00015284\n",
      "Epoch 2359, Loss: 0.5442097746163348, learning rate: 0.00015282000000000002\n",
      "Epoch 2360, Loss: 0.5442084977152776, learning rate: 0.0001528\n",
      "Epoch 2361, Loss: 0.5442072227476163, learning rate: 0.00015278\n",
      "Epoch 2362, Loss: 0.5442059526961937, learning rate: 0.00015276\n",
      "Epoch 2363, Loss: 0.544204682825529, learning rate: 0.00015274\n",
      "Epoch 2364, Loss: 0.5442034036613338, learning rate: 0.00015272000000000002\n",
      "Epoch 2365, Loss: 0.5442021360707079, learning rate: 0.0001527\n",
      "Epoch 2366, Loss: 0.5442008604669695, learning rate: 0.00015268\n",
      "Epoch 2367, Loss: 0.5441995888795388, learning rate: 0.00015266\n",
      "Epoch 2368, Loss: 0.5441983243867642, learning rate: 0.00015264\n",
      "Epoch 2369, Loss: 0.5441970553130456, learning rate: 0.00015262\n",
      "Epoch 2370, Loss: 0.5441957777297266, learning rate: 0.00015260000000000002\n",
      "Epoch 2371, Loss: 0.5441945090802764, learning rate: 0.00015258\n",
      "Epoch 2372, Loss: 0.5441932411961624, learning rate: 0.00015256\n",
      "Epoch 2373, Loss: 0.5441919686902515, learning rate: 0.00015254\n",
      "Epoch 2374, Loss: 0.5441906993290258, learning rate: 0.00015252\n",
      "Epoch 2375, Loss: 0.5441894299995935, learning rate: 0.00015250000000000002\n",
      "Epoch 2376, Loss: 0.5441881666624546, learning rate: 0.00015248\n",
      "Epoch 2377, Loss: 0.544186896028661, learning rate: 0.00015246000000000001\n",
      "Epoch 2378, Loss: 0.5441856215763546, learning rate: 0.00015244\n",
      "Epoch 2379, Loss: 0.5441843567775708, learning rate: 0.00015242\n",
      "Epoch 2380, Loss: 0.544183089411578, learning rate: 0.00015240000000000002\n",
      "Epoch 2381, Loss: 0.5441818216277248, learning rate: 0.00015238\n",
      "Epoch 2382, Loss: 0.5441805491232965, learning rate: 0.00015236\n",
      "Epoch 2383, Loss: 0.5441792846548543, learning rate: 0.00015234\n",
      "Epoch 2384, Loss: 0.5441780133761783, learning rate: 0.00015232\n",
      "Epoch 2385, Loss: 0.5441767499891763, learning rate: 0.00015230000000000002\n",
      "Epoch 2386, Loss: 0.5441754804015807, learning rate: 0.00015228\n",
      "Epoch 2387, Loss: 0.544174214132906, learning rate: 0.00015226\n",
      "Epoch 2388, Loss: 0.5441729481705462, learning rate: 0.00015224\n",
      "Epoch 2389, Loss: 0.5441716824035503, learning rate: 0.00015222\n",
      "Epoch 2390, Loss: 0.5441704131321101, learning rate: 0.0001522\n",
      "Epoch 2391, Loss: 0.5441691450815687, learning rate: 0.00015218000000000002\n",
      "Epoch 2392, Loss: 0.5441678773188393, learning rate: 0.00015216\n",
      "Epoch 2393, Loss: 0.544166614378664, learning rate: 0.00015214000000000002\n",
      "Epoch 2394, Loss: 0.5441653532752545, learning rate: 0.00015212\n",
      "Epoch 2395, Loss: 0.5441640886976646, learning rate: 0.0001521\n",
      "Epoch 2396, Loss: 0.5441628158300359, learning rate: 0.00015208000000000002\n",
      "Epoch 2397, Loss: 0.5441615568546813, learning rate: 0.00015206\n",
      "Epoch 2398, Loss: 0.5441602870235475, learning rate: 0.00015204000000000001\n",
      "Epoch 2399, Loss: 0.5441590222556644, learning rate: 0.00015202\n",
      "Epoch 2400, Loss: 0.5441577554131981, learning rate: 0.000152\n",
      "Epoch 2401, Loss: 0.5441564917793303, learning rate: 0.00015198000000000002\n",
      "Epoch 2402, Loss: 0.5441552326673056, learning rate: 0.00015196\n",
      "Epoch 2403, Loss: 0.5441539636945413, learning rate: 0.00015194\n",
      "Epoch 2404, Loss: 0.5441527039932732, learning rate: 0.00015192\n",
      "Epoch 2405, Loss: 0.54415143893366, learning rate: 0.0001519\n",
      "Epoch 2406, Loss: 0.5441501708113805, learning rate: 0.00015188000000000002\n",
      "Epoch 2407, Loss: 0.5441489072333721, learning rate: 0.00015186\n",
      "Epoch 2408, Loss: 0.5441476483587985, learning rate: 0.00015184\n",
      "Epoch 2409, Loss: 0.5441463878608919, learning rate: 0.00015182\n",
      "Epoch 2410, Loss: 0.5441451183779669, learning rate: 0.0001518\n",
      "Epoch 2411, Loss: 0.5441438557882787, learning rate: 0.00015178000000000001\n",
      "Epoch 2412, Loss: 0.5441425913698189, learning rate: 0.00015176000000000002\n",
      "Epoch 2413, Loss: 0.5441413318398844, learning rate: 0.00015174\n",
      "Epoch 2414, Loss: 0.5441400749749356, learning rate: 0.00015172000000000002\n",
      "Epoch 2415, Loss: 0.5441388113354596, learning rate: 0.0001517\n",
      "Epoch 2416, Loss: 0.5441375434865746, learning rate: 0.00015168\n",
      "Epoch 2417, Loss: 0.5441362854879502, learning rate: 0.00015166000000000002\n",
      "Epoch 2418, Loss: 0.5441350217640598, learning rate: 0.00015164\n",
      "Epoch 2419, Loss: 0.5441337608159601, learning rate: 0.00015162000000000002\n",
      "Epoch 2420, Loss: 0.5441324993436596, learning rate: 0.0001516\n",
      "Epoch 2421, Loss: 0.544131239735995, learning rate: 0.00015158\n",
      "Epoch 2422, Loss: 0.5441299767604716, learning rate: 0.00015156000000000002\n",
      "Epoch 2423, Loss: 0.5441287164598453, learning rate: 0.00015154\n",
      "Epoch 2424, Loss: 0.5441274581791447, learning rate: 0.00015152\n",
      "Epoch 2425, Loss: 0.5441261945766557, learning rate: 0.0001515\n",
      "Epoch 2426, Loss: 0.5441249379150096, learning rate: 0.00015148\n",
      "Epoch 2427, Loss: 0.5441236748764516, learning rate: 0.00015146000000000002\n",
      "Epoch 2428, Loss: 0.544122416295491, learning rate: 0.00015144\n",
      "Epoch 2429, Loss: 0.544121158361487, learning rate: 0.00015142\n",
      "Epoch 2430, Loss: 0.5441198973254886, learning rate: 0.0001514\n",
      "Epoch 2431, Loss: 0.5441186381941655, learning rate: 0.00015138\n",
      "Epoch 2432, Loss: 0.5441173852325889, learning rate: 0.00015136000000000001\n",
      "Epoch 2433, Loss: 0.5441161178539645, learning rate: 0.00015134\n",
      "Epoch 2434, Loss: 0.5441148593852341, learning rate: 0.00015132\n",
      "Epoch 2435, Loss: 0.5441136000561386, learning rate: 0.0001513\n",
      "Epoch 2436, Loss: 0.5441123429583146, learning rate: 0.00015128\n",
      "Epoch 2437, Loss: 0.5441110898489091, learning rate: 0.00015126\n",
      "Epoch 2438, Loss: 0.5441098265912118, learning rate: 0.00015124000000000002\n",
      "Epoch 2439, Loss: 0.5441085668151191, learning rate: 0.00015122\n",
      "Epoch 2440, Loss: 0.544107315480344, learning rate: 0.0001512\n",
      "Epoch 2441, Loss: 0.5441060533318438, learning rate: 0.00015118\n",
      "Epoch 2442, Loss: 0.5441047967793272, learning rate: 0.00015116\n",
      "Epoch 2443, Loss: 0.5441035427309721, learning rate: 0.00015114000000000002\n",
      "Epoch 2444, Loss: 0.5441022845388431, learning rate: 0.00015112\n",
      "Epoch 2445, Loss: 0.544101032493053, learning rate: 0.00015110000000000001\n",
      "Epoch 2446, Loss: 0.5440997679471766, learning rate: 0.00015108\n",
      "Epoch 2447, Loss: 0.5440985122422795, learning rate: 0.00015106\n",
      "Epoch 2448, Loss: 0.5440972541772648, learning rate: 0.00015104000000000002\n",
      "Epoch 2449, Loss: 0.544095997304728, learning rate: 0.00015102\n",
      "Epoch 2450, Loss: 0.5440947453310861, learning rate: 0.000151\n",
      "Epoch 2451, Loss: 0.5440934904520628, learning rate: 0.00015098\n",
      "Epoch 2452, Loss: 0.5440922319837819, learning rate: 0.00015096\n",
      "Epoch 2453, Loss: 0.544090977545558, learning rate: 0.00015094000000000002\n",
      "Epoch 2454, Loss: 0.5440897259689865, learning rate: 0.00015092\n",
      "Epoch 2455, Loss: 0.5440884688325752, learning rate: 0.0001509\n",
      "Epoch 2456, Loss: 0.5440872130200011, learning rate: 0.00015088\n",
      "Epoch 2457, Loss: 0.5440859574134789, learning rate: 0.00015086\n",
      "Epoch 2458, Loss: 0.5440847114185606, learning rate: 0.00015084\n",
      "Epoch 2459, Loss: 0.5440834461141908, learning rate: 0.00015082000000000002\n",
      "Epoch 2460, Loss: 0.544082192150871, learning rate: 0.0001508\n",
      "Epoch 2461, Loss: 0.5440809373464541, learning rate: 0.00015078000000000002\n",
      "Epoch 2462, Loss: 0.5440796856477501, learning rate: 0.00015076\n",
      "Epoch 2463, Loss: 0.5440784389494409, learning rate: 0.00015074\n",
      "Epoch 2464, Loss: 0.5440771775945157, learning rate: 0.00015072000000000002\n",
      "Epoch 2465, Loss: 0.5440759232499129, learning rate: 0.0001507\n",
      "Epoch 2466, Loss: 0.5440746699865576, learning rate: 0.00015068000000000001\n",
      "Epoch 2467, Loss: 0.5440734233737872, learning rate: 0.00015066\n",
      "Epoch 2468, Loss: 0.544072169121341, learning rate: 0.00015064\n",
      "Epoch 2469, Loss: 0.5440709145813764, learning rate: 0.00015062000000000002\n",
      "Epoch 2470, Loss: 0.544069663693957, learning rate: 0.0001506\n",
      "Epoch 2471, Loss: 0.5440684119387053, learning rate: 0.00015058\n",
      "Epoch 2472, Loss: 0.5440671576452091, learning rate: 0.00015056\n",
      "Epoch 2473, Loss: 0.5440659022253822, learning rate: 0.00015054\n",
      "Epoch 2474, Loss: 0.5440646506750085, learning rate: 0.00015052000000000002\n",
      "Epoch 2475, Loss: 0.5440633998238592, learning rate: 0.0001505\n",
      "Epoch 2476, Loss: 0.5440621507466632, learning rate: 0.00015048\n",
      "Epoch 2477, Loss: 0.5440609030318754, learning rate: 0.00015046\n",
      "Epoch 2478, Loss: 0.5440596485942517, learning rate: 0.00015044\n",
      "Epoch 2479, Loss: 0.5440583943778331, learning rate: 0.00015042\n",
      "Epoch 2480, Loss: 0.5440571487123282, learning rate: 0.00015040000000000002\n",
      "Epoch 2481, Loss: 0.544055896123354, learning rate: 0.00015038\n",
      "Epoch 2482, Loss: 0.5440546445958315, learning rate: 0.00015036000000000002\n",
      "Epoch 2483, Loss: 0.5440533931816699, learning rate: 0.00015034\n",
      "Epoch 2484, Loss: 0.544052152331055, learning rate: 0.00015032\n",
      "Epoch 2485, Loss: 0.5440508966452647, learning rate: 0.00015030000000000002\n",
      "Epoch 2486, Loss: 0.5440496424696079, learning rate: 0.00015028\n",
      "Epoch 2487, Loss: 0.5440483933658375, learning rate: 0.00015026000000000001\n",
      "Epoch 2488, Loss: 0.544047145408698, learning rate: 0.00015024\n",
      "Epoch 2489, Loss: 0.5440458953422441, learning rate: 0.00015022\n",
      "Epoch 2490, Loss: 0.5440446541690976, learning rate: 0.00015020000000000002\n",
      "Epoch 2491, Loss: 0.5440433998969098, learning rate: 0.00015018\n",
      "Epoch 2492, Loss: 0.5440421526357446, learning rate: 0.00015016\n",
      "Epoch 2493, Loss: 0.5440409028219263, learning rate: 0.00015014\n",
      "Epoch 2494, Loss: 0.5440396505072265, learning rate: 0.00015012\n",
      "Epoch 2495, Loss: 0.544038404346702, learning rate: 0.00015010000000000002\n",
      "Epoch 2496, Loss: 0.5440371649248288, learning rate: 0.00015008\n",
      "Epoch 2497, Loss: 0.5440359074518243, learning rate: 0.00015006\n",
      "Epoch 2498, Loss: 0.5440346597242576, learning rate: 0.00015004\n",
      "Epoch 2499, Loss: 0.544033415938119, learning rate: 0.00015002\n",
      "Epoch 2500, Loss: 0.544032169174557, learning rate: 0.00015000000000000001\n",
      "Epoch 2501, Loss: 0.5440309223441122, learning rate: 0.00014998000000000002\n",
      "Epoch 2502, Loss: 0.5440296795919081, learning rate: 0.00014996\n",
      "Epoch 2503, Loss: 0.5440284285688677, learning rate: 0.00014994\n",
      "Epoch 2504, Loss: 0.5440271777697141, learning rate: 0.00014992\n",
      "Epoch 2505, Loss: 0.5440259319619032, learning rate: 0.0001499\n",
      "Epoch 2506, Loss: 0.5440246880782752, learning rate: 0.00014988000000000002\n",
      "Epoch 2507, Loss: 0.544023449242544, learning rate: 0.00014986\n",
      "Epoch 2508, Loss: 0.5440221954890164, learning rate: 0.00014984\n",
      "Epoch 2509, Loss: 0.5440209477725054, learning rate: 0.00014982\n",
      "Epoch 2510, Loss: 0.5440197078492874, learning rate: 0.0001498\n",
      "Epoch 2511, Loss: 0.5440184556955974, learning rate: 0.00014978000000000002\n",
      "Epoch 2512, Loss: 0.5440172137270001, learning rate: 0.00014976\n",
      "Epoch 2513, Loss: 0.5440159724786449, learning rate: 0.00014974\n",
      "Epoch 2514, Loss: 0.544014730480414, learning rate: 0.00014972\n",
      "Epoch 2515, Loss: 0.5440134779435737, learning rate: 0.0001497\n",
      "Epoch 2516, Loss: 0.5440122371468512, learning rate: 0.00014968000000000002\n",
      "Epoch 2517, Loss: 0.5440109897542929, learning rate: 0.00014966\n",
      "Epoch 2518, Loss: 0.5440097508027881, learning rate: 0.00014964\n",
      "Epoch 2519, Loss: 0.5440085012376578, learning rate: 0.00014962\n",
      "Epoch 2520, Loss: 0.5440072569507478, learning rate: 0.0001496\n",
      "Epoch 2521, Loss: 0.5440060214109109, learning rate: 0.00014958000000000001\n",
      "Epoch 2522, Loss: 0.5440047710083175, learning rate: 0.00014956\n",
      "Epoch 2523, Loss: 0.5440035313726094, learning rate: 0.00014954\n",
      "Epoch 2524, Loss: 0.5440022868095589, learning rate: 0.00014952\n",
      "Epoch 2525, Loss: 0.5440010440054095, learning rate: 0.0001495\n",
      "Epoch 2526, Loss: 0.5439998011927641, learning rate: 0.00014948\n",
      "Epoch 2527, Loss: 0.5439985650501988, learning rate: 0.00014946\n",
      "Epoch 2528, Loss: 0.5439973157649075, learning rate: 0.00014944\n",
      "Epoch 2529, Loss: 0.5439960710640467, learning rate: 0.00014942000000000002\n",
      "Epoch 2530, Loss: 0.5439948337832302, learning rate: 0.0001494\n",
      "Epoch 2531, Loss: 0.5439935941137752, learning rate: 0.00014938\n",
      "Epoch 2532, Loss: 0.5439923512642809, learning rate: 0.00014936\n",
      "Epoch 2533, Loss: 0.5439911138889064, learning rate: 0.00014934\n",
      "Epoch 2534, Loss: 0.5439898685657474, learning rate: 0.00014932000000000001\n",
      "Epoch 2535, Loss: 0.5439886236907404, learning rate: 0.0001493\n",
      "Epoch 2536, Loss: 0.5439873816259458, learning rate: 0.00014928\n",
      "Epoch 2537, Loss: 0.5439861446224789, learning rate: 0.00014926000000000002\n",
      "Epoch 2538, Loss: 0.5439849025607898, learning rate: 0.00014924\n",
      "Epoch 2539, Loss: 0.5439836648095445, learning rate: 0.00014922\n",
      "Epoch 2540, Loss: 0.5439824213327575, learning rate: 0.0001492\n",
      "Epoch 2541, Loss: 0.5439811879672648, learning rate: 0.00014918\n",
      "Epoch 2542, Loss: 0.5439799475309048, learning rate: 0.00014916000000000002\n",
      "Epoch 2543, Loss: 0.5439787031749973, learning rate: 0.00014914\n",
      "Epoch 2544, Loss: 0.5439774642026292, learning rate: 0.00014912\n",
      "Epoch 2545, Loss: 0.5439762245862605, learning rate: 0.0001491\n",
      "Epoch 2546, Loss: 0.5439749907413071, learning rate: 0.00014908\n",
      "Epoch 2547, Loss: 0.5439737462474479, learning rate: 0.00014906\n",
      "Epoch 2548, Loss: 0.5439725061557621, learning rate: 0.00014904\n",
      "Epoch 2549, Loss: 0.5439712651260014, learning rate: 0.00014902\n",
      "Epoch 2550, Loss: 0.5439700319067873, learning rate: 0.00014900000000000002\n",
      "Epoch 2551, Loss: 0.5439688001851669, learning rate: 0.00014898\n",
      "Epoch 2552, Loss: 0.5439675530003879, learning rate: 0.00014896\n",
      "Epoch 2553, Loss: 0.5439663118266168, learning rate: 0.00014894000000000002\n",
      "Epoch 2554, Loss: 0.5439650776481665, learning rate: 0.00014892\n",
      "Epoch 2555, Loss: 0.5439638399740441, learning rate: 0.00014890000000000001\n",
      "Epoch 2556, Loss: 0.5439626028993712, learning rate: 0.00014888\n",
      "Epoch 2557, Loss: 0.5439613634766671, learning rate: 0.00014886\n",
      "Epoch 2558, Loss: 0.5439601298983413, learning rate: 0.00014884000000000002\n",
      "Epoch 2559, Loss: 0.5439588898702015, learning rate: 0.00014882\n",
      "Epoch 2560, Loss: 0.5439576608707272, learning rate: 0.0001488\n",
      "Epoch 2561, Loss: 0.5439564167331163, learning rate: 0.00014878\n",
      "Epoch 2562, Loss: 0.5439551831134065, learning rate: 0.00014876\n",
      "Epoch 2563, Loss: 0.543953945276443, learning rate: 0.00014874000000000002\n",
      "Epoch 2564, Loss: 0.5439527068436849, learning rate: 0.00014872\n",
      "Epoch 2565, Loss: 0.5439514747194322, learning rate: 0.0001487\n",
      "Epoch 2566, Loss: 0.5439502412567571, learning rate: 0.00014868000000000002\n",
      "Epoch 2567, Loss: 0.5439490027078523, learning rate: 0.00014866\n",
      "Epoch 2568, Loss: 0.543947764750719, learning rate: 0.00014864\n",
      "Epoch 2569, Loss: 0.543946526363626, learning rate: 0.00014862\n",
      "Epoch 2570, Loss: 0.5439452929374022, learning rate: 0.0001486\n",
      "Epoch 2571, Loss: 0.5439440549521524, learning rate: 0.00014858000000000002\n",
      "Epoch 2572, Loss: 0.5439428236738564, learning rate: 0.00014856\n",
      "Epoch 2573, Loss: 0.5439415864666647, learning rate: 0.00014854\n",
      "Epoch 2574, Loss: 0.5439403561881071, learning rate: 0.00014852000000000002\n",
      "Epoch 2575, Loss: 0.5439391209790283, learning rate: 0.0001485\n",
      "Epoch 2576, Loss: 0.5439378876382129, learning rate: 0.00014848000000000001\n",
      "Epoch 2577, Loss: 0.5439366558719843, learning rate: 0.00014846\n",
      "Epoch 2578, Loss: 0.5439354202633748, learning rate: 0.00014844\n",
      "Epoch 2579, Loss: 0.5439341870004412, learning rate: 0.00014842000000000002\n",
      "Epoch 2580, Loss: 0.5439329515785046, learning rate: 0.0001484\n",
      "Epoch 2581, Loss: 0.543931720759392, learning rate: 0.00014838\n",
      "Epoch 2582, Loss: 0.5439304846883941, learning rate: 0.00014836\n",
      "Epoch 2583, Loss: 0.5439292497622121, learning rate: 0.00014834\n",
      "Epoch 2584, Loss: 0.5439280160997874, learning rate: 0.00014832000000000002\n",
      "Epoch 2585, Loss: 0.5439267866138112, learning rate: 0.0001483\n",
      "Epoch 2586, Loss: 0.5439255524602001, learning rate: 0.00014828\n",
      "Epoch 2587, Loss: 0.5439243255934121, learning rate: 0.00014826000000000002\n",
      "Epoch 2588, Loss: 0.5439230893025856, learning rate: 0.00014824\n",
      "Epoch 2589, Loss: 0.5439218561461244, learning rate: 0.00014822000000000001\n",
      "Epoch 2590, Loss: 0.5439206321143725, learning rate: 0.00014820000000000002\n",
      "Epoch 2591, Loss: 0.5439193943030779, learning rate: 0.00014818\n",
      "Epoch 2592, Loss: 0.5439181698798475, learning rate: 0.00014816000000000002\n",
      "Epoch 2593, Loss: 0.5439169307731687, learning rate: 0.00014814\n",
      "Epoch 2594, Loss: 0.5439157024213627, learning rate: 0.00014812\n",
      "Epoch 2595, Loss: 0.5439144714068769, learning rate: 0.00014810000000000002\n",
      "Epoch 2596, Loss: 0.5439132390028871, learning rate: 0.00014808\n",
      "Epoch 2597, Loss: 0.5439120067393495, learning rate: 0.00014806000000000002\n",
      "Epoch 2598, Loss: 0.5439107824311055, learning rate: 0.00014804\n",
      "Epoch 2599, Loss: 0.5439095474553082, learning rate: 0.00014802\n",
      "Epoch 2600, Loss: 0.5439083208490896, learning rate: 0.00014800000000000002\n",
      "Epoch 2601, Loss: 0.5439070943818869, learning rate: 0.00014798\n",
      "Epoch 2602, Loss: 0.5439058614902766, learning rate: 0.00014796\n",
      "Epoch 2603, Loss: 0.5439046304354843, learning rate: 0.00014794\n",
      "Epoch 2604, Loss: 0.5439033984487843, learning rate: 0.00014792\n",
      "Epoch 2605, Loss: 0.543902172911527, learning rate: 0.00014790000000000002\n",
      "Epoch 2606, Loss: 0.5439009412103748, learning rate: 0.00014788\n",
      "Epoch 2607, Loss: 0.54389971423733, learning rate: 0.00014786\n",
      "Epoch 2608, Loss: 0.5438984856049325, learning rate: 0.00014784\n",
      "Epoch 2609, Loss: 0.5438972587801347, learning rate: 0.00014782\n",
      "Epoch 2610, Loss: 0.5438960283068367, learning rate: 0.00014780000000000001\n",
      "Epoch 2611, Loss: 0.5438948070605625, learning rate: 0.00014778000000000003\n",
      "Epoch 2612, Loss: 0.5438935730303691, learning rate: 0.00014776\n",
      "Epoch 2613, Loss: 0.5438923484694115, learning rate: 0.00014774\n",
      "Epoch 2614, Loss: 0.5438911175032791, learning rate: 0.00014772\n",
      "Epoch 2615, Loss: 0.5438898922693912, learning rate: 0.0001477\n",
      "Epoch 2616, Loss: 0.5438886649357465, learning rate: 0.00014768000000000002\n",
      "Epoch 2617, Loss: 0.5438874357079052, learning rate: 0.00014766\n",
      "Epoch 2618, Loss: 0.5438862088803489, learning rate: 0.00014764\n",
      "Epoch 2619, Loss: 0.5438849900150261, learning rate: 0.00014762\n",
      "Epoch 2620, Loss: 0.543883755869013, learning rate: 0.0001476\n",
      "Epoch 2621, Loss: 0.5438825272552825, learning rate: 0.00014758000000000002\n",
      "Epoch 2622, Loss: 0.5438813013554362, learning rate: 0.00014756\n",
      "Epoch 2623, Loss: 0.5438800833029002, learning rate: 0.00014754\n",
      "Epoch 2624, Loss: 0.5438788504862527, learning rate: 0.00014752\n",
      "Epoch 2625, Loss: 0.5438776242256859, learning rate: 0.0001475\n",
      "Epoch 2626, Loss: 0.5438764009871805, learning rate: 0.00014748000000000002\n",
      "Epoch 2627, Loss: 0.5438751747361307, learning rate: 0.00014746000000000003\n",
      "Epoch 2628, Loss: 0.5438739576055703, learning rate: 0.00014744\n",
      "Epoch 2629, Loss: 0.54387272424632, learning rate: 0.00014742\n",
      "Epoch 2630, Loss: 0.5438715019397875, learning rate: 0.0001474\n",
      "Epoch 2631, Loss: 0.5438702768623997, learning rate: 0.00014738000000000002\n",
      "Epoch 2632, Loss: 0.5438690509809816, learning rate: 0.00014736000000000003\n",
      "Epoch 2633, Loss: 0.5438678309437497, learning rate: 0.00014734\n",
      "Epoch 2634, Loss: 0.5438666101991024, learning rate: 0.00014732\n",
      "Epoch 2635, Loss: 0.5438653794118767, learning rate: 0.0001473\n",
      "Epoch 2636, Loss: 0.5438641563883012, learning rate: 0.00014728\n",
      "Epoch 2637, Loss: 0.5438629339495672, learning rate: 0.00014726000000000002\n",
      "Epoch 2638, Loss: 0.5438617120140264, learning rate: 0.00014724\n",
      "Epoch 2639, Loss: 0.543860484771278, learning rate: 0.00014722\n",
      "Epoch 2640, Loss: 0.5438592598540162, learning rate: 0.0001472\n",
      "Epoch 2641, Loss: 0.5438580431590686, learning rate: 0.00014718\n",
      "Epoch 2642, Loss: 0.543856818088808, learning rate: 0.00014716000000000002\n",
      "Epoch 2643, Loss: 0.5438555983643278, learning rate: 0.00014714\n",
      "Epoch 2644, Loss: 0.5438543710834077, learning rate: 0.00014712000000000001\n",
      "Epoch 2645, Loss: 0.5438531476838122, learning rate: 0.0001471\n",
      "Epoch 2646, Loss: 0.5438519345553602, learning rate: 0.00014708\n",
      "Epoch 2647, Loss: 0.543850705772365, learning rate: 0.00014706000000000002\n",
      "Epoch 2648, Loss: 0.5438494813975822, learning rate: 0.00014704\n",
      "Epoch 2649, Loss: 0.5438482654070683, learning rate: 0.00014702\n",
      "Epoch 2650, Loss: 0.5438470416215794, learning rate: 0.000147\n",
      "Epoch 2651, Loss: 0.5438458216113699, learning rate: 0.00014698\n",
      "Epoch 2652, Loss: 0.5438446031931051, learning rate: 0.00014696000000000002\n",
      "Epoch 2653, Loss: 0.5438433831418871, learning rate: 0.00014694\n",
      "Epoch 2654, Loss: 0.543842158487838, learning rate: 0.00014692\n",
      "Epoch 2655, Loss: 0.5438409383856365, learning rate: 0.0001469\n",
      "Epoch 2656, Loss: 0.5438397221054989, learning rate: 0.00014688\n",
      "Epoch 2657, Loss: 0.5438385015828756, learning rate: 0.00014686\n",
      "Epoch 2658, Loss: 0.5438372772732194, learning rate: 0.00014684\n",
      "Epoch 2659, Loss: 0.5438360573696692, learning rate: 0.00014682\n",
      "Epoch 2660, Loss: 0.543834843923437, learning rate: 0.0001468\n",
      "Epoch 2661, Loss: 0.5438336205564309, learning rate: 0.00014678\n",
      "Epoch 2662, Loss: 0.5438324081845805, learning rate: 0.00014676\n",
      "Epoch 2663, Loss: 0.543831181799982, learning rate: 0.00014674\n",
      "Epoch 2664, Loss: 0.5438299616956594, learning rate: 0.00014672\n",
      "Epoch 2665, Loss: 0.5438287444755172, learning rate: 0.00014670000000000002\n",
      "Epoch 2666, Loss: 0.5438275332337529, learning rate: 0.00014668\n",
      "Epoch 2667, Loss: 0.5438263061695269, learning rate: 0.00014666\n",
      "Epoch 2668, Loss: 0.543825090594884, learning rate: 0.00014664000000000002\n",
      "Epoch 2669, Loss: 0.5438238732927907, learning rate: 0.00014662\n",
      "Epoch 2670, Loss: 0.5438226533766425, learning rate: 0.0001466\n",
      "Epoch 2671, Loss: 0.5438214374233672, learning rate: 0.00014658\n",
      "Epoch 2672, Loss: 0.5438202247804258, learning rate: 0.00014656\n",
      "Epoch 2673, Loss: 0.543819007006614, learning rate: 0.00014654000000000002\n",
      "Epoch 2674, Loss: 0.5438177858458741, learning rate: 0.00014652\n",
      "Epoch 2675, Loss: 0.5438165689052182, learning rate: 0.0001465\n",
      "Epoch 2676, Loss: 0.54381535935648, learning rate: 0.00014648\n",
      "Epoch 2677, Loss: 0.5438141363281727, learning rate: 0.00014646\n",
      "Epoch 2678, Loss: 0.5438129210629321, learning rate: 0.00014644000000000001\n",
      "Epoch 2679, Loss: 0.5438117049430827, learning rate: 0.00014642\n",
      "Epoch 2680, Loss: 0.5438104907511505, learning rate: 0.0001464\n",
      "Epoch 2681, Loss: 0.5438092788291949, learning rate: 0.00014638000000000002\n",
      "Epoch 2682, Loss: 0.5438080577175046, learning rate: 0.00014636\n",
      "Epoch 2683, Loss: 0.543806844844192, learning rate: 0.00014634\n",
      "Epoch 2684, Loss: 0.5438056304441589, learning rate: 0.00014632\n",
      "Epoch 2685, Loss: 0.5438044134505181, learning rate: 0.0001463\n",
      "Epoch 2686, Loss: 0.5438032047030646, learning rate: 0.00014628000000000002\n",
      "Epoch 2687, Loss: 0.5438019826747383, learning rate: 0.00014626\n",
      "Epoch 2688, Loss: 0.5438007685194683, learning rate: 0.00014624\n",
      "Epoch 2689, Loss: 0.5437995592498648, learning rate: 0.00014622000000000002\n",
      "Epoch 2690, Loss: 0.5437983444387436, learning rate: 0.0001462\n",
      "Epoch 2691, Loss: 0.5437971258899958, learning rate: 0.00014618\n",
      "Epoch 2692, Loss: 0.5437959208833165, learning rate: 0.00014616\n",
      "Epoch 2693, Loss: 0.5437947061535391, learning rate: 0.00014614\n",
      "Epoch 2694, Loss: 0.5437934883701858, learning rate: 0.00014612000000000002\n",
      "Epoch 2695, Loss: 0.5437922755695275, learning rate: 0.0001461\n",
      "Epoch 2696, Loss: 0.5437910602991844, learning rate: 0.00014608\n",
      "Epoch 2697, Loss: 0.5437898528453485, learning rate: 0.00014606\n",
      "Epoch 2698, Loss: 0.5437886365049753, learning rate: 0.00014604\n",
      "Epoch 2699, Loss: 0.5437874322231968, learning rate: 0.00014602000000000001\n",
      "Epoch 2700, Loss: 0.5437862123672571, learning rate: 0.000146\n",
      "Epoch 2701, Loss: 0.5437850022195745, learning rate: 0.00014598\n",
      "Epoch 2702, Loss: 0.5437837976400003, learning rate: 0.00014596000000000002\n",
      "Epoch 2703, Loss: 0.5437825798476157, learning rate: 0.00014594\n",
      "Epoch 2704, Loss: 0.5437813673644456, learning rate: 0.00014592\n",
      "Epoch 2705, Loss: 0.5437801608512175, learning rate: 0.00014590000000000002\n",
      "Epoch 2706, Loss: 0.5437789450847728, learning rate: 0.00014588\n",
      "Epoch 2707, Loss: 0.543777737754996, learning rate: 0.00014586000000000002\n",
      "Epoch 2708, Loss: 0.5437765280510802, learning rate: 0.00014584\n",
      "Epoch 2709, Loss: 0.5437753206016518, learning rate: 0.00014582\n",
      "Epoch 2710, Loss: 0.5437741050990288, learning rate: 0.00014580000000000002\n",
      "Epoch 2711, Loss: 0.5437728965604803, learning rate: 0.00014578\n",
      "Epoch 2712, Loss: 0.5437716841137316, learning rate: 0.00014576000000000001\n",
      "Epoch 2713, Loss: 0.543770477077789, learning rate: 0.00014574\n",
      "Epoch 2714, Loss: 0.5437692668098437, learning rate: 0.00014572\n",
      "Epoch 2715, Loss: 0.5437680665807759, learning rate: 0.00014570000000000002\n",
      "Epoch 2716, Loss: 0.5437668472958804, learning rate: 0.00014568\n",
      "Epoch 2717, Loss: 0.5437656370030367, learning rate: 0.00014566\n",
      "Epoch 2718, Loss: 0.543764432665482, learning rate: 0.00014564000000000002\n",
      "Epoch 2719, Loss: 0.5437632243408965, learning rate: 0.00014562\n",
      "Epoch 2720, Loss: 0.5437620156037549, learning rate: 0.00014560000000000002\n",
      "Epoch 2721, Loss: 0.5437608065384445, learning rate: 0.00014558\n",
      "Epoch 2722, Loss: 0.5437595987964267, learning rate: 0.00014556\n",
      "Epoch 2723, Loss: 0.5437583891725567, learning rate: 0.00014554000000000002\n",
      "Epoch 2724, Loss: 0.5437571786240383, learning rate: 0.00014552\n",
      "Epoch 2725, Loss: 0.5437559666177983, learning rate: 0.0001455\n",
      "Epoch 2726, Loss: 0.5437547606714873, learning rate: 0.00014548000000000002\n",
      "Epoch 2727, Loss: 0.5437535565618947, learning rate: 0.00014546\n",
      "Epoch 2728, Loss: 0.5437523459549549, learning rate: 0.00014544000000000002\n",
      "Epoch 2729, Loss: 0.5437511390300294, learning rate: 0.00014542\n",
      "Epoch 2730, Loss: 0.5437499283803815, learning rate: 0.0001454\n",
      "Epoch 2731, Loss: 0.54374871972633, learning rate: 0.00014538000000000002\n",
      "Epoch 2732, Loss: 0.5437475147284516, learning rate: 0.00014536\n",
      "Epoch 2733, Loss: 0.5437463102498414, learning rate: 0.00014534000000000001\n",
      "Epoch 2734, Loss: 0.5437451017744196, learning rate: 0.00014532\n",
      "Epoch 2735, Loss: 0.5437438939007676, learning rate: 0.0001453\n",
      "Epoch 2736, Loss: 0.5437426908060359, learning rate: 0.00014528000000000002\n",
      "Epoch 2737, Loss: 0.5437414838259109, learning rate: 0.00014526\n",
      "Epoch 2738, Loss: 0.5437402754023372, learning rate: 0.00014524\n",
      "Epoch 2739, Loss: 0.5437390740220663, learning rate: 0.00014522\n",
      "Epoch 2740, Loss: 0.5437378655329659, learning rate: 0.0001452\n",
      "Epoch 2741, Loss: 0.5437366586310213, learning rate: 0.00014518000000000002\n",
      "Epoch 2742, Loss: 0.543735453683966, learning rate: 0.00014516000000000003\n",
      "Epoch 2743, Loss: 0.5437342488192215, learning rate: 0.00014514\n",
      "Epoch 2744, Loss: 0.5437330423687416, learning rate: 0.00014512\n",
      "Epoch 2745, Loss: 0.5437318372591353, learning rate: 0.0001451\n",
      "Epoch 2746, Loss: 0.5437306339336452, learning rate: 0.00014508000000000001\n",
      "Epoch 2747, Loss: 0.5437294334630465, learning rate: 0.00014506000000000002\n",
      "Epoch 2748, Loss: 0.5437282249681579, learning rate: 0.00014504\n",
      "Epoch 2749, Loss: 0.5437270189456167, learning rate: 0.00014502\n",
      "Epoch 2750, Loss: 0.5437258171750985, learning rate: 0.000145\n",
      "Epoch 2751, Loss: 0.5437246140808867, learning rate: 0.00014498\n",
      "Epoch 2752, Loss: 0.5437234152157836, learning rate: 0.00014496000000000002\n",
      "Epoch 2753, Loss: 0.5437222086989079, learning rate: 0.00014494\n",
      "Epoch 2754, Loss: 0.5437210045365964, learning rate: 0.00014492\n",
      "Epoch 2755, Loss: 0.5437198017773345, learning rate: 0.0001449\n",
      "Epoch 2756, Loss: 0.5437185960352907, learning rate: 0.00014488\n",
      "Epoch 2757, Loss: 0.5437173946411286, learning rate: 0.00014486000000000002\n",
      "Epoch 2758, Loss: 0.5437161945116636, learning rate: 0.00014484\n",
      "Epoch 2759, Loss: 0.543714995042035, learning rate: 0.00014482\n",
      "Epoch 2760, Loss: 0.5437137909720036, learning rate: 0.0001448\n",
      "Epoch 2761, Loss: 0.5437125942178798, learning rate: 0.00014478\n",
      "Epoch 2762, Loss: 0.5437113871933202, learning rate: 0.00014476000000000002\n",
      "Epoch 2763, Loss: 0.5437101846920649, learning rate: 0.00014474000000000003\n",
      "Epoch 2764, Loss: 0.5437089836620246, learning rate: 0.00014472\n",
      "Epoch 2765, Loss: 0.5437077856459015, learning rate: 0.0001447\n",
      "Epoch 2766, Loss: 0.5437065804021912, learning rate: 0.00014468\n",
      "Epoch 2767, Loss: 0.543705381490329, learning rate: 0.00014466000000000001\n",
      "Epoch 2768, Loss: 0.5437041786404231, learning rate: 0.00014464000000000002\n",
      "Epoch 2769, Loss: 0.5437029773789358, learning rate: 0.00014462\n",
      "Epoch 2770, Loss: 0.5437017793522595, learning rate: 0.0001446\n",
      "Epoch 2771, Loss: 0.5437005819605312, learning rate: 0.00014458\n",
      "Epoch 2772, Loss: 0.5436993789426905, learning rate: 0.00014456\n",
      "Epoch 2773, Loss: 0.5436981811230953, learning rate: 0.00014454000000000002\n",
      "Epoch 2774, Loss: 0.5436969845755636, learning rate: 0.00014452\n",
      "Epoch 2775, Loss: 0.5436957852554868, learning rate: 0.0001445\n",
      "Epoch 2776, Loss: 0.5436945790582548, learning rate: 0.00014448\n",
      "Epoch 2777, Loss: 0.5436933825827407, learning rate: 0.00014446\n",
      "Epoch 2778, Loss: 0.543692188656974, learning rate: 0.00014444000000000002\n",
      "Epoch 2779, Loss: 0.5436909853484311, learning rate: 0.00014442\n",
      "Epoch 2780, Loss: 0.5436897857612483, learning rate: 0.0001444\n",
      "Epoch 2781, Loss: 0.5436885864770042, learning rate: 0.00014438\n",
      "Epoch 2782, Loss: 0.5436873945224215, learning rate: 0.00014436\n",
      "Epoch 2783, Loss: 0.543686193591863, learning rate: 0.00014434000000000002\n",
      "Epoch 2784, Loss: 0.5436849964443039, learning rate: 0.00014432\n",
      "Epoch 2785, Loss: 0.5436837962291999, learning rate: 0.0001443\n",
      "Epoch 2786, Loss: 0.5436825995249313, learning rate: 0.00014428\n",
      "Epoch 2787, Loss: 0.5436814033409078, learning rate: 0.00014426\n",
      "Epoch 2788, Loss: 0.5436802080709572, learning rate: 0.00014424000000000001\n",
      "Epoch 2789, Loss: 0.5436790092375644, learning rate: 0.00014422\n",
      "Epoch 2790, Loss: 0.5436778085823566, learning rate: 0.0001442\n",
      "Epoch 2791, Loss: 0.5436766131269432, learning rate: 0.00014418\n",
      "Epoch 2792, Loss: 0.543675421556317, learning rate: 0.00014416\n",
      "Epoch 2793, Loss: 0.5436742247049128, learning rate: 0.00014414\n",
      "Epoch 2794, Loss: 0.5436730296197951, learning rate: 0.00014412\n",
      "Epoch 2795, Loss: 0.5436718267347227, learning rate: 0.0001441\n",
      "Epoch 2796, Loss: 0.5436706352337846, learning rate: 0.00014408000000000002\n",
      "Epoch 2797, Loss: 0.5436694399961333, learning rate: 0.00014406\n",
      "Epoch 2798, Loss: 0.5436682469452547, learning rate: 0.00014404\n",
      "Epoch 2799, Loss: 0.5436670455520811, learning rate: 0.00014402\n",
      "Epoch 2800, Loss: 0.5436658526886008, learning rate: 0.000144\n",
      "Epoch 2801, Loss: 0.5436646562960089, learning rate: 0.00014398000000000001\n",
      "Epoch 2802, Loss: 0.5436634637600158, learning rate: 0.00014396\n",
      "Epoch 2803, Loss: 0.5436622733122045, learning rate: 0.00014394\n",
      "Epoch 2804, Loss: 0.5436610741923706, learning rate: 0.00014392000000000002\n",
      "Epoch 2805, Loss: 0.5436598822947223, learning rate: 0.0001439\n",
      "Epoch 2806, Loss: 0.5436586879860282, learning rate: 0.00014388\n",
      "Epoch 2807, Loss: 0.543657496159683, learning rate: 0.00014386\n",
      "Epoch 2808, Loss: 0.5436563034357602, learning rate: 0.00014384\n",
      "Epoch 2809, Loss: 0.5436551092114978, learning rate: 0.00014382000000000002\n",
      "Epoch 2810, Loss: 0.5436539125856054, learning rate: 0.0001438\n",
      "Epoch 2811, Loss: 0.5436527191355074, learning rate: 0.00014378\n",
      "Epoch 2812, Loss: 0.5436515258615203, learning rate: 0.00014376\n",
      "Epoch 2813, Loss: 0.543650336443558, learning rate: 0.00014374\n",
      "Epoch 2814, Loss: 0.543649145343256, learning rate: 0.00014372\n",
      "Epoch 2815, Loss: 0.5436479551807151, learning rate: 0.0001437\n",
      "Epoch 2816, Loss: 0.5436467581035602, learning rate: 0.00014368\n",
      "Epoch 2817, Loss: 0.543645565926188, learning rate: 0.00014366000000000002\n",
      "Epoch 2818, Loss: 0.5436443727842127, learning rate: 0.00014364\n",
      "Epoch 2819, Loss: 0.5436431829380546, learning rate: 0.00014362\n",
      "Epoch 2820, Loss: 0.5436419953194751, learning rate: 0.0001436\n",
      "Epoch 2821, Loss: 0.5436408041970926, learning rate: 0.00014358\n",
      "Epoch 2822, Loss: 0.5436396116073396, learning rate: 0.00014356000000000001\n",
      "Epoch 2823, Loss: 0.5436384160980621, learning rate: 0.00014354\n",
      "Epoch 2824, Loss: 0.5436372274974361, learning rate: 0.00014352\n",
      "Epoch 2825, Loss: 0.5436360413651733, learning rate: 0.00014350000000000002\n",
      "Epoch 2826, Loss: 0.543634848252738, learning rate: 0.00014348\n",
      "Epoch 2827, Loss: 0.5436336561208805, learning rate: 0.00014346\n",
      "Epoch 2828, Loss: 0.5436324687488047, learning rate: 0.00014344\n",
      "Epoch 2829, Loss: 0.5436312800205813, learning rate: 0.00014342\n",
      "Epoch 2830, Loss: 0.5436300887994755, learning rate: 0.00014340000000000002\n",
      "Epoch 2831, Loss: 0.543628898659628, learning rate: 0.00014338\n",
      "Epoch 2832, Loss: 0.5436277088603758, learning rate: 0.00014336\n",
      "Epoch 2833, Loss: 0.5436265184022318, learning rate: 0.00014334000000000002\n",
      "Epoch 2834, Loss: 0.5436253265550106, learning rate: 0.00014332\n",
      "Epoch 2835, Loss: 0.543624138351222, learning rate: 0.00014330000000000001\n",
      "Epoch 2836, Loss: 0.5436229583353304, learning rate: 0.00014328\n",
      "Epoch 2837, Loss: 0.5436217646465503, learning rate: 0.00014326\n",
      "Epoch 2838, Loss: 0.5436205779797167, learning rate: 0.00014324000000000002\n",
      "Epoch 2839, Loss: 0.5436193863781205, learning rate: 0.00014322\n",
      "Epoch 2840, Loss: 0.5436181974619996, learning rate: 0.0001432\n",
      "Epoch 2841, Loss: 0.5436170070682986, learning rate: 0.00014318000000000002\n",
      "Epoch 2842, Loss: 0.5436158198194502, learning rate: 0.00014316\n",
      "Epoch 2843, Loss: 0.5436146341905471, learning rate: 0.00014314000000000002\n",
      "Epoch 2844, Loss: 0.5436134499159808, learning rate: 0.00014312\n",
      "Epoch 2845, Loss: 0.543612259811772, learning rate: 0.0001431\n",
      "Epoch 2846, Loss: 0.5436110686133678, learning rate: 0.00014308000000000002\n",
      "Epoch 2847, Loss: 0.5436098831227388, learning rate: 0.00014306\n",
      "Epoch 2848, Loss: 0.5436086980751857, learning rate: 0.00014304\n",
      "Epoch 2849, Loss: 0.5436075137434928, learning rate: 0.00014302\n",
      "Epoch 2850, Loss: 0.5436063243336792, learning rate: 0.000143\n",
      "Epoch 2851, Loss: 0.5436051431365712, learning rate: 0.00014298000000000002\n",
      "Epoch 2852, Loss: 0.5436039565754744, learning rate: 0.00014296\n",
      "Epoch 2853, Loss: 0.5436027700181035, learning rate: 0.00014294\n",
      "Epoch 2854, Loss: 0.5436015819709381, learning rate: 0.00014292000000000002\n",
      "Epoch 2855, Loss: 0.5436004020096676, learning rate: 0.0001429\n",
      "Epoch 2856, Loss: 0.5435992136447791, learning rate: 0.00014288000000000001\n",
      "Epoch 2857, Loss: 0.5435980303336967, learning rate: 0.00014286\n",
      "Epoch 2858, Loss: 0.543596840321804, learning rate: 0.00014284\n",
      "Epoch 2859, Loss: 0.5435956566228162, learning rate: 0.00014282000000000002\n",
      "Epoch 2860, Loss: 0.5435944773521091, learning rate: 0.0001428\n",
      "Epoch 2861, Loss: 0.5435932881355894, learning rate: 0.00014278\n",
      "Epoch 2862, Loss: 0.5435921075147315, learning rate: 0.00014276000000000002\n",
      "Epoch 2863, Loss: 0.5435909305577008, learning rate: 0.00014274\n",
      "Epoch 2864, Loss: 0.5435897393229453, learning rate: 0.00014272000000000002\n",
      "Epoch 2865, Loss: 0.5435885550271607, learning rate: 0.0001427\n",
      "Epoch 2866, Loss: 0.5435873766157401, learning rate: 0.00014268\n",
      "Epoch 2867, Loss: 0.5435861879937973, learning rate: 0.00014266000000000002\n",
      "Epoch 2868, Loss: 0.543585004929698, learning rate: 0.00014264\n",
      "Epoch 2869, Loss: 0.5435838258207193, learning rate: 0.00014262\n",
      "Epoch 2870, Loss: 0.5435826411625586, learning rate: 0.0001426\n",
      "Epoch 2871, Loss: 0.5435814621969005, learning rate: 0.00014258\n",
      "Epoch 2872, Loss: 0.5435802794046823, learning rate: 0.00014256000000000002\n",
      "Epoch 2873, Loss: 0.5435790957821671, learning rate: 0.00014254\n",
      "Epoch 2874, Loss: 0.5435779118749368, learning rate: 0.00014252\n",
      "Epoch 2875, Loss: 0.5435767347926204, learning rate: 0.0001425\n",
      "Epoch 2876, Loss: 0.5435755501584477, learning rate: 0.00014248\n",
      "Epoch 2877, Loss: 0.5435743716570518, learning rate: 0.00014246000000000002\n",
      "Epoch 2878, Loss: 0.5435731860118833, learning rate: 0.00014244000000000003\n",
      "Epoch 2879, Loss: 0.5435720083033754, learning rate: 0.00014242\n",
      "Epoch 2880, Loss: 0.5435708244771147, learning rate: 0.0001424\n",
      "Epoch 2881, Loss: 0.5435696473550139, learning rate: 0.00014238\n",
      "Epoch 2882, Loss: 0.5435684676044722, learning rate: 0.00014236\n",
      "Epoch 2883, Loss: 0.5435672883398504, learning rate: 0.00014234000000000002\n",
      "Epoch 2884, Loss: 0.5435661022872961, learning rate: 0.00014232\n",
      "Epoch 2885, Loss: 0.5435649239979472, learning rate: 0.0001423\n",
      "Epoch 2886, Loss: 0.5435637449588436, learning rate: 0.00014228\n",
      "Epoch 2887, Loss: 0.5435625689450643, learning rate: 0.00014226\n",
      "Epoch 2888, Loss: 0.5435613868700088, learning rate: 0.00014224000000000002\n",
      "Epoch 2889, Loss: 0.5435602115563539, learning rate: 0.00014222\n",
      "Epoch 2890, Loss: 0.543559026042356, learning rate: 0.0001422\n",
      "Epoch 2891, Loss: 0.5435578496623078, learning rate: 0.00014218\n",
      "Epoch 2892, Loss: 0.5435566676296946, learning rate: 0.00014216\n",
      "Epoch 2893, Loss: 0.5435554912753895, learning rate: 0.00014214000000000002\n",
      "Epoch 2894, Loss: 0.5435543167160681, learning rate: 0.00014212\n",
      "Epoch 2895, Loss: 0.5435531342841007, learning rate: 0.0001421\n",
      "Epoch 2896, Loss: 0.5435519618873813, learning rate: 0.00014208\n",
      "Epoch 2897, Loss: 0.5435507852292287, learning rate: 0.00014206\n",
      "Epoch 2898, Loss: 0.5435496041563568, learning rate: 0.00014204000000000002\n",
      "Epoch 2899, Loss: 0.5435484221301904, learning rate: 0.00014202000000000003\n",
      "Epoch 2900, Loss: 0.5435472500957135, learning rate: 0.000142\n",
      "Epoch 2901, Loss: 0.5435460685217779, learning rate: 0.00014198\n",
      "Epoch 2902, Loss: 0.543544894930755, learning rate: 0.00014196\n",
      "Epoch 2903, Loss: 0.54354371533579, learning rate: 0.00014194\n",
      "Epoch 2904, Loss: 0.5435425396766764, learning rate: 0.00014192000000000002\n",
      "Epoch 2905, Loss: 0.5435413665953505, learning rate: 0.0001419\n",
      "Epoch 2906, Loss: 0.5435401936325365, learning rate: 0.00014188\n",
      "Epoch 2907, Loss: 0.5435390131967308, learning rate: 0.00014186\n",
      "Epoch 2908, Loss: 0.5435378338386234, learning rate: 0.00014184\n",
      "Epoch 2909, Loss: 0.543536666210142, learning rate: 0.00014182000000000002\n",
      "Epoch 2910, Loss: 0.5435354870084791, learning rate: 0.0001418\n",
      "Epoch 2911, Loss: 0.5435343094549785, learning rate: 0.00014178\n",
      "Epoch 2912, Loss: 0.5435331329308395, learning rate: 0.00014176\n",
      "Epoch 2913, Loss: 0.5435319577026971, learning rate: 0.00014174\n",
      "Epoch 2914, Loss: 0.5435307861910947, learning rate: 0.00014172000000000002\n",
      "Epoch 2915, Loss: 0.5435296094883392, learning rate: 0.0001417\n",
      "Epoch 2916, Loss: 0.5435284359437956, learning rate: 0.00014168\n",
      "Epoch 2917, Loss: 0.5435272588916721, learning rate: 0.00014166\n",
      "Epoch 2918, Loss: 0.5435260921040597, learning rate: 0.00014164\n",
      "Epoch 2919, Loss: 0.5435249127123365, learning rate: 0.00014162000000000002\n",
      "Epoch 2920, Loss: 0.543523743648445, learning rate: 0.0001416\n",
      "Epoch 2921, Loss: 0.5435225634774146, learning rate: 0.00014158\n",
      "Epoch 2922, Loss: 0.5435213931301873, learning rate: 0.00014156\n",
      "Epoch 2923, Loss: 0.5435202232822097, learning rate: 0.00014154\n",
      "Epoch 2924, Loss: 0.5435190505691424, learning rate: 0.00014152000000000001\n",
      "Epoch 2925, Loss: 0.5435178737874897, learning rate: 0.0001415\n",
      "Epoch 2926, Loss: 0.5435166970905316, learning rate: 0.00014148\n",
      "Epoch 2927, Loss: 0.5435155281013445, learning rate: 0.00014146\n",
      "Epoch 2928, Loss: 0.5435143584119553, learning rate: 0.00014144\n",
      "Epoch 2929, Loss: 0.5435131805343002, learning rate: 0.00014142\n",
      "Epoch 2930, Loss: 0.543512010006536, learning rate: 0.0001414\n",
      "Epoch 2931, Loss: 0.5435108375036861, learning rate: 0.00014138\n",
      "Epoch 2932, Loss: 0.5435096642616217, learning rate: 0.00014136000000000002\n",
      "Epoch 2933, Loss: 0.5435085014662894, learning rate: 0.00014134\n",
      "Epoch 2934, Loss: 0.5435073250804917, learning rate: 0.00014132\n",
      "Epoch 2935, Loss: 0.5435061547549747, learning rate: 0.0001413\n",
      "Epoch 2936, Loss: 0.5435049814942939, learning rate: 0.00014128\n",
      "Epoch 2937, Loss: 0.5435038108916522, learning rate: 0.00014126\n",
      "Epoch 2938, Loss: 0.5435026409078728, learning rate: 0.00014124\n",
      "Epoch 2939, Loss: 0.5435014739433077, learning rate: 0.00014122\n",
      "Epoch 2940, Loss: 0.5435003039216578, learning rate: 0.00014120000000000002\n",
      "Epoch 2941, Loss: 0.5434991329896475, learning rate: 0.00014118\n",
      "Epoch 2942, Loss: 0.5434979628134893, learning rate: 0.00014116\n",
      "Epoch 2943, Loss: 0.5434967974417728, learning rate: 0.00014114\n",
      "Epoch 2944, Loss: 0.5434956215162352, learning rate: 0.00014112\n",
      "Epoch 2945, Loss: 0.5434944556866496, learning rate: 0.00014110000000000001\n",
      "Epoch 2946, Loss: 0.5434932833097504, learning rate: 0.00014108\n",
      "Epoch 2947, Loss: 0.5434921151353349, learning rate: 0.00014106\n",
      "Epoch 2948, Loss: 0.5434909522050078, learning rate: 0.00014104000000000002\n",
      "Epoch 2949, Loss: 0.5434897836745174, learning rate: 0.00014102\n",
      "Epoch 2950, Loss: 0.5434886119331207, learning rate: 0.000141\n",
      "Epoch 2951, Loss: 0.5434874466818019, learning rate: 0.00014098\n",
      "Epoch 2952, Loss: 0.5434862746342398, learning rate: 0.00014096\n",
      "Epoch 2953, Loss: 0.5434851080083539, learning rate: 0.00014094000000000002\n",
      "Epoch 2954, Loss: 0.5434839400923732, learning rate: 0.00014092\n",
      "Epoch 2955, Loss: 0.5434827746030058, learning rate: 0.0001409\n",
      "Epoch 2956, Loss: 0.5434816081981524, learning rate: 0.00014088000000000002\n",
      "Epoch 2957, Loss: 0.5434804425134933, learning rate: 0.00014086\n",
      "Epoch 2958, Loss: 0.543479273973098, learning rate: 0.00014084000000000001\n",
      "Epoch 2959, Loss: 0.5434781103719748, learning rate: 0.00014082\n",
      "Epoch 2960, Loss: 0.5434769453263079, learning rate: 0.0001408\n",
      "Epoch 2961, Loss: 0.5434757804285223, learning rate: 0.00014078000000000002\n",
      "Epoch 2962, Loss: 0.5434746100052638, learning rate: 0.00014076\n",
      "Epoch 2963, Loss: 0.5434734449285352, learning rate: 0.00014074\n",
      "Epoch 2964, Loss: 0.5434722768471063, learning rate: 0.00014072\n",
      "Epoch 2965, Loss: 0.543471114904933, learning rate: 0.0001407\n",
      "Epoch 2966, Loss: 0.5434699456713691, learning rate: 0.00014068000000000002\n",
      "Epoch 2967, Loss: 0.5434687817350011, learning rate: 0.00014066\n",
      "Epoch 2968, Loss: 0.5434676171066135, learning rate: 0.00014064\n",
      "Epoch 2969, Loss: 0.5434664542620036, learning rate: 0.00014062000000000002\n",
      "Epoch 2970, Loss: 0.5434652903324642, learning rate: 0.0001406\n",
      "Epoch 2971, Loss: 0.5434641214255783, learning rate: 0.00014058\n",
      "Epoch 2972, Loss: 0.5434629625740343, learning rate: 0.00014056\n",
      "Epoch 2973, Loss: 0.5434617981940214, learning rate: 0.00014054\n",
      "Epoch 2974, Loss: 0.5434606372740487, learning rate: 0.00014052000000000002\n",
      "Epoch 2975, Loss: 0.5434594702275861, learning rate: 0.0001405\n",
      "Epoch 2976, Loss: 0.5434583061734947, learning rate: 0.00014048\n",
      "Epoch 2977, Loss: 0.5434571402738329, learning rate: 0.00014046000000000002\n",
      "Epoch 2978, Loss: 0.543455976641692, learning rate: 0.00014044\n",
      "Epoch 2979, Loss: 0.5434548151952986, learning rate: 0.00014042000000000001\n",
      "Epoch 2980, Loss: 0.5434536599506261, learning rate: 0.0001404\n",
      "Epoch 2981, Loss: 0.5434524923084866, learning rate: 0.00014038\n",
      "Epoch 2982, Loss: 0.5434513294213714, learning rate: 0.00014036000000000002\n",
      "Epoch 2983, Loss: 0.5434501631830511, learning rate: 0.00014034\n",
      "Epoch 2984, Loss: 0.5434490001734713, learning rate: 0.00014032\n",
      "Epoch 2985, Loss: 0.5434478406229103, learning rate: 0.00014030000000000002\n",
      "Epoch 2986, Loss: 0.5434466841114901, learning rate: 0.00014028\n",
      "Epoch 2987, Loss: 0.5434455171727673, learning rate: 0.00014026000000000002\n",
      "Epoch 2988, Loss: 0.5434443579217269, learning rate: 0.00014024\n",
      "Epoch 2989, Loss: 0.543443191835637, learning rate: 0.00014022\n",
      "Epoch 2990, Loss: 0.5434420286342818, learning rate: 0.00014020000000000002\n",
      "Epoch 2991, Loss: 0.5434408670713724, learning rate: 0.00014018\n",
      "Epoch 2992, Loss: 0.5434397049682829, learning rate: 0.00014016\n",
      "Epoch 2993, Loss: 0.5434385430500432, learning rate: 0.00014014000000000002\n",
      "Epoch 2994, Loss: 0.5434373826921861, learning rate: 0.00014012\n",
      "Epoch 2995, Loss: 0.5434362273291308, learning rate: 0.00014010000000000002\n",
      "Epoch 2996, Loss: 0.5434350674890701, learning rate: 0.00014008\n",
      "Epoch 2997, Loss: 0.543433906005371, learning rate: 0.00014006\n",
      "Epoch 2998, Loss: 0.543432750895017, learning rate: 0.00014004000000000002\n",
      "Epoch 2999, Loss: 0.5434315792687189, learning rate: 0.00014002\n",
      "Epoch 3000, Loss: 0.5434304185378881, learning rate: 0.00014000000000000001\n",
      "Epoch 3001, Loss: 0.5434292574553851, learning rate: 0.00013998\n",
      "Epoch 3002, Loss: 0.5434280997428248, learning rate: 0.00013996\n",
      "Epoch 3003, Loss: 0.5434269449835809, learning rate: 0.00013994000000000002\n",
      "Epoch 3004, Loss: 0.5434257839463762, learning rate: 0.00013992\n",
      "Epoch 3005, Loss: 0.5434246246981193, learning rate: 0.0001399\n",
      "Epoch 3006, Loss: 0.5434234698395357, learning rate: 0.00013988\n",
      "Epoch 3007, Loss: 0.5434223018012792, learning rate: 0.00013986\n",
      "Epoch 3008, Loss: 0.5434211420841166, learning rate: 0.00013984000000000002\n",
      "Epoch 3009, Loss: 0.5434199859408307, learning rate: 0.00013982\n",
      "Epoch 3010, Loss: 0.5434188340681244, learning rate: 0.0001398\n",
      "Epoch 3011, Loss: 0.5434176693371477, learning rate: 0.00013978\n",
      "Epoch 3012, Loss: 0.543416510157018, learning rate: 0.00013976\n",
      "Epoch 3013, Loss: 0.5434153565948839, learning rate: 0.00013974000000000001\n",
      "Epoch 3014, Loss: 0.5434142025507869, learning rate: 0.00013972000000000002\n",
      "Epoch 3015, Loss: 0.5434130365817944, learning rate: 0.0001397\n",
      "Epoch 3016, Loss: 0.5434118796447915, learning rate: 0.00013968\n",
      "Epoch 3017, Loss: 0.5434107292884005, learning rate: 0.00013966\n",
      "Epoch 3018, Loss: 0.5434095704361438, learning rate: 0.00013964\n",
      "Epoch 3019, Loss: 0.5434084082204954, learning rate: 0.00013962000000000002\n",
      "Epoch 3020, Loss: 0.5434072497802513, learning rate: 0.0001396\n",
      "Epoch 3021, Loss: 0.5434060916625583, learning rate: 0.00013958\n",
      "Epoch 3022, Loss: 0.5434049402173922, learning rate: 0.00013956\n",
      "Epoch 3023, Loss: 0.5434037848932936, learning rate: 0.00013954\n",
      "Epoch 3024, Loss: 0.5434026280432909, learning rate: 0.00013952000000000002\n",
      "Epoch 3025, Loss: 0.543401479623444, learning rate: 0.0001395\n",
      "Epoch 3026, Loss: 0.5434003137575991, learning rate: 0.00013947999999999999\n",
      "Epoch 3027, Loss: 0.5433991576596037, learning rate: 0.00013946\n",
      "Epoch 3028, Loss: 0.5433980005720075, learning rate: 0.00013944\n",
      "Epoch 3029, Loss: 0.5433968464607364, learning rate: 0.00013942000000000002\n",
      "Epoch 3030, Loss: 0.5433956969398814, learning rate: 0.00013940000000000003\n",
      "Epoch 3031, Loss: 0.5433945392172174, learning rate: 0.00013938\n",
      "Epoch 3032, Loss: 0.5433933877609389, learning rate: 0.00013936\n",
      "Epoch 3033, Loss: 0.5433922305026031, learning rate: 0.00013934\n",
      "Epoch 3034, Loss: 0.5433910734336541, learning rate: 0.00013932000000000001\n",
      "Epoch 3035, Loss: 0.5433899180101216, learning rate: 0.00013930000000000002\n",
      "Epoch 3036, Loss: 0.5433887642679006, learning rate: 0.00013928\n",
      "Epoch 3037, Loss: 0.5433876167077438, learning rate: 0.00013926\n",
      "Epoch 3038, Loss: 0.543386457526525, learning rate: 0.00013924\n",
      "Epoch 3039, Loss: 0.5433853153273699, learning rate: 0.00013922\n",
      "Epoch 3040, Loss: 0.5433841496161353, learning rate: 0.00013920000000000002\n",
      "Epoch 3041, Loss: 0.5433829988309399, learning rate: 0.00013918\n",
      "Epoch 3042, Loss: 0.5433818507612308, learning rate: 0.00013916\n",
      "Epoch 3043, Loss: 0.5433806963416413, learning rate: 0.00013914\n",
      "Epoch 3044, Loss: 0.543379540232408, learning rate: 0.00013912\n",
      "Epoch 3045, Loss: 0.5433783963338403, learning rate: 0.00013910000000000002\n",
      "Epoch 3046, Loss: 0.5433772348574523, learning rate: 0.00013908\n",
      "Epoch 3047, Loss: 0.5433760817089884, learning rate: 0.00013906000000000001\n",
      "Epoch 3048, Loss: 0.543374931886138, learning rate: 0.00013904\n",
      "Epoch 3049, Loss: 0.5433737899659528, learning rate: 0.00013902\n",
      "Epoch 3050, Loss: 0.5433726291490438, learning rate: 0.00013900000000000002\n",
      "Epoch 3051, Loss: 0.543371487250774, learning rate: 0.00013898\n",
      "Epoch 3052, Loss: 0.5433703248715019, learning rate: 0.00013895999999999998\n",
      "Epoch 3053, Loss: 0.5433691740527427, learning rate: 0.00013894\n",
      "Epoch 3054, Loss: 0.5433680220052669, learning rate: 0.00013892\n",
      "Epoch 3055, Loss: 0.5433668732025472, learning rate: 0.00013890000000000002\n",
      "Epoch 3056, Loss: 0.5433657306580328, learning rate: 0.00013888\n",
      "Epoch 3057, Loss: 0.543364573284409, learning rate: 0.00013886\n",
      "Epoch 3058, Loss: 0.5433634306890583, learning rate: 0.00013884\n",
      "Epoch 3059, Loss: 0.5433622726177213, learning rate: 0.00013882\n",
      "Epoch 3060, Loss: 0.5433611201197642, learning rate: 0.0001388\n",
      "Epoch 3061, Loss: 0.5433599727956846, learning rate: 0.00013878\n",
      "Epoch 3062, Loss: 0.5433588279676296, learning rate: 0.00013876\n",
      "Epoch 3063, Loss: 0.5433576754535129, learning rate: 0.00013874\n",
      "Epoch 3064, Loss: 0.5433565330447555, learning rate: 0.00013872\n",
      "Epoch 3065, Loss: 0.5433553744568037, learning rate: 0.0001387\n",
      "Epoch 3066, Loss: 0.5433542279041098, learning rate: 0.00013868000000000002\n",
      "Epoch 3067, Loss: 0.5433530788384519, learning rate: 0.00013866\n",
      "Epoch 3068, Loss: 0.543351934410521, learning rate: 0.00013864\n",
      "Epoch 3069, Loss: 0.5433507837672287, learning rate: 0.00013862\n",
      "Epoch 3070, Loss: 0.5433496434989066, learning rate: 0.0001386\n",
      "Epoch 3071, Loss: 0.5433484863683653, learning rate: 0.00013858000000000002\n",
      "Epoch 3072, Loss: 0.5433473365620949, learning rate: 0.00013856\n",
      "Epoch 3073, Loss: 0.5433461872414318, learning rate: 0.00013854\n",
      "Epoch 3074, Loss: 0.5433450437472497, learning rate: 0.00013852\n",
      "Epoch 3075, Loss: 0.543343905209027, learning rate: 0.0001385\n",
      "Epoch 3076, Loss: 0.5433427480834885, learning rate: 0.00013848000000000002\n",
      "Epoch 3077, Loss: 0.5433416003277005, learning rate: 0.00013846\n",
      "Epoch 3078, Loss: 0.5433404641565687, learning rate: 0.00013844\n",
      "Epoch 3079, Loss: 0.5433393130879034, learning rate: 0.00013842\n",
      "Epoch 3080, Loss: 0.5433381640676447, learning rate: 0.0001384\n",
      "Epoch 3081, Loss: 0.5433370193110324, learning rate: 0.00013838\n",
      "Epoch 3082, Loss: 0.5433358727258827, learning rate: 0.00013836000000000002\n",
      "Epoch 3083, Loss: 0.5433347300820186, learning rate: 0.00013834\n",
      "Epoch 3084, Loss: 0.5433335782965011, learning rate: 0.00013832\n",
      "Epoch 3085, Loss: 0.5433324304027329, learning rate: 0.0001383\n",
      "Epoch 3086, Loss: 0.5433312865502308, learning rate: 0.00013828\n",
      "Epoch 3087, Loss: 0.5433301490592266, learning rate: 0.00013826000000000002\n",
      "Epoch 3088, Loss: 0.5433289962758503, learning rate: 0.00013824\n",
      "Epoch 3089, Loss: 0.5433278495858671, learning rate: 0.00013822\n",
      "Epoch 3090, Loss: 0.5433267039620905, learning rate: 0.0001382\n",
      "Epoch 3091, Loss: 0.5433255641825442, learning rate: 0.00013818\n",
      "Epoch 3092, Loss: 0.5433244259153271, learning rate: 0.00013816000000000002\n",
      "Epoch 3093, Loss: 0.5433232724008089, learning rate: 0.00013814\n",
      "Epoch 3094, Loss: 0.5433221390018411, learning rate: 0.00013812\n",
      "Epoch 3095, Loss: 0.5433209830253196, learning rate: 0.0001381\n",
      "Epoch 3096, Loss: 0.5433198524358966, learning rate: 0.00013808\n",
      "Epoch 3097, Loss: 0.5433186971184627, learning rate: 0.00013806000000000002\n",
      "Epoch 3098, Loss: 0.5433175542331978, learning rate: 0.00013804\n",
      "Epoch 3099, Loss: 0.5433164186323547, learning rate: 0.00013802\n",
      "Epoch 3100, Loss: 0.5433152712912653, learning rate: 0.000138\n",
      "Epoch 3101, Loss: 0.5433141283125842, learning rate: 0.00013798\n",
      "Epoch 3102, Loss: 0.5433129898687981, learning rate: 0.00013796000000000001\n",
      "Epoch 3103, Loss: 0.5433118403525333, learning rate: 0.00013794000000000002\n",
      "Epoch 3104, Loss: 0.5433107003221672, learning rate: 0.00013792\n",
      "Epoch 3105, Loss: 0.5433095522465867, learning rate: 0.0001379\n",
      "Epoch 3106, Loss: 0.5433084097818502, learning rate: 0.00013788\n",
      "Epoch 3107, Loss: 0.543307271105443, learning rate: 0.00013786\n",
      "Epoch 3108, Loss: 0.543306137095822, learning rate: 0.00013784000000000002\n",
      "Epoch 3109, Loss: 0.5433049851369481, learning rate: 0.00013782\n",
      "Epoch 3110, Loss: 0.5433038429672963, learning rate: 0.00013780000000000002\n",
      "Epoch 3111, Loss: 0.5433027117147324, learning rate: 0.00013778\n",
      "Epoch 3112, Loss: 0.5433015616930894, learning rate: 0.00013776\n",
      "Epoch 3113, Loss: 0.5433004195185569, learning rate: 0.00013774000000000002\n",
      "Epoch 3114, Loss: 0.5432992774731679, learning rate: 0.00013772\n",
      "Epoch 3115, Loss: 0.5432981484802544, learning rate: 0.0001377\n",
      "Epoch 3116, Loss: 0.5432969986206487, learning rate: 0.00013768\n",
      "Epoch 3117, Loss: 0.5432958679992376, learning rate: 0.00013766\n",
      "Epoch 3118, Loss: 0.5432947184441383, learning rate: 0.00013764000000000002\n",
      "Epoch 3119, Loss: 0.5432935858098707, learning rate: 0.00013762000000000003\n",
      "Epoch 3120, Loss: 0.5432924384285592, learning rate: 0.0001376\n",
      "Epoch 3121, Loss: 0.5432912954323318, learning rate: 0.00013758\n",
      "Epoch 3122, Loss: 0.5432901573333755, learning rate: 0.00013756\n",
      "Epoch 3123, Loss: 0.5432890235335794, learning rate: 0.00013754000000000001\n",
      "Epoch 3124, Loss: 0.5432878860816003, learning rate: 0.00013752000000000003\n",
      "Epoch 3125, Loss: 0.5432867499007, learning rate: 0.0001375\n",
      "Epoch 3126, Loss: 0.5432856024798461, learning rate: 0.00013748\n",
      "Epoch 3127, Loss: 0.5432844724334572, learning rate: 0.00013746\n",
      "Epoch 3128, Loss: 0.5432833275623233, learning rate: 0.00013744\n",
      "Epoch 3129, Loss: 0.5432821856799342, learning rate: 0.00013742000000000002\n",
      "Epoch 3130, Loss: 0.5432810466915232, learning rate: 0.0001374\n",
      "Epoch 3131, Loss: 0.5432799216664775, learning rate: 0.00013738000000000002\n",
      "Epoch 3132, Loss: 0.5432787720953601, learning rate: 0.00013736\n",
      "Epoch 3133, Loss: 0.5432776327023624, learning rate: 0.00013734\n",
      "Epoch 3134, Loss: 0.543276504084205, learning rate: 0.00013732000000000002\n",
      "Epoch 3135, Loss: 0.5432753630572276, learning rate: 0.0001373\n",
      "Epoch 3136, Loss: 0.543274231124625, learning rate: 0.00013728000000000001\n",
      "Epoch 3137, Loss: 0.5432730885680735, learning rate: 0.00013726\n",
      "Epoch 3138, Loss: 0.5432719611688556, learning rate: 0.00013724\n",
      "Epoch 3139, Loss: 0.543270812420483, learning rate: 0.00013722000000000002\n",
      "Epoch 3140, Loss: 0.5432696768327735, learning rate: 0.00013720000000000003\n",
      "Epoch 3141, Loss: 0.5432685467189374, learning rate: 0.00013718\n",
      "Epoch 3142, Loss: 0.5432674051289583, learning rate: 0.00013716\n",
      "Epoch 3143, Loss: 0.5432662746228791, learning rate: 0.00013714\n",
      "Epoch 3144, Loss: 0.5432651387941002, learning rate: 0.00013712000000000002\n",
      "Epoch 3145, Loss: 0.5432639965977798, learning rate: 0.00013710000000000003\n",
      "Epoch 3146, Loss: 0.5432628615465583, learning rate: 0.00013708\n",
      "Epoch 3147, Loss: 0.5432617312093109, learning rate: 0.00013706\n",
      "Epoch 3148, Loss: 0.5432605970151484, learning rate: 0.00013704\n",
      "Epoch 3149, Loss: 0.5432594596821375, learning rate: 0.00013702\n",
      "Epoch 3150, Loss: 0.5432583269418945, learning rate: 0.00013700000000000002\n",
      "Epoch 3151, Loss: 0.543257191492719, learning rate: 0.00013698\n",
      "Epoch 3152, Loss: 0.5432560562993396, learning rate: 0.00013696\n",
      "Epoch 3153, Loss: 0.5432549227032757, learning rate: 0.00013694\n",
      "Epoch 3154, Loss: 0.5432537909945884, learning rate: 0.00013692\n",
      "Epoch 3155, Loss: 0.5432526521262686, learning rate: 0.00013690000000000002\n",
      "Epoch 3156, Loss: 0.5432515197058279, learning rate: 0.00013688000000000003\n",
      "Epoch 3157, Loss: 0.5432503816950378, learning rate: 0.00013686\n",
      "Epoch 3158, Loss: 0.5432492481290033, learning rate: 0.00013684\n",
      "Epoch 3159, Loss: 0.5432481167811021, learning rate: 0.00013682\n",
      "Epoch 3160, Loss: 0.5432469938534399, learning rate: 0.00013680000000000002\n",
      "Epoch 3161, Loss: 0.5432458521614429, learning rate: 0.00013678000000000003\n",
      "Epoch 3162, Loss: 0.5432447210094552, learning rate: 0.00013675999999999998\n",
      "Epoch 3163, Loss: 0.5432435908470608, learning rate: 0.00013674\n",
      "Epoch 3164, Loss: 0.543242457427036, learning rate: 0.00013672\n",
      "Epoch 3165, Loss: 0.5432413230897517, learning rate: 0.00013670000000000002\n",
      "Epoch 3166, Loss: 0.5432401990555036, learning rate: 0.00013668000000000003\n",
      "Epoch 3167, Loss: 0.5432390584852854, learning rate: 0.00013665999999999998\n",
      "Epoch 3168, Loss: 0.543237927489936, learning rate: 0.00013664\n",
      "Epoch 3169, Loss: 0.5432367957880859, learning rate: 0.00013662\n",
      "Epoch 3170, Loss: 0.543235663601667, learning rate: 0.0001366\n",
      "Epoch 3171, Loss: 0.5432345359771467, learning rate: 0.00013658000000000002\n",
      "Epoch 3172, Loss: 0.5432334036385789, learning rate: 0.00013656\n",
      "Epoch 3173, Loss: 0.5432322733843318, learning rate: 0.00013654\n",
      "Epoch 3174, Loss: 0.5432311433609675, learning rate: 0.00013652\n",
      "Epoch 3175, Loss: 0.5432300114657823, learning rate: 0.0001365\n",
      "Epoch 3176, Loss: 0.5432288793487265, learning rate: 0.00013648000000000002\n",
      "Epoch 3177, Loss: 0.5432277498264958, learning rate: 0.00013646\n",
      "Epoch 3178, Loss: 0.5432266173746964, learning rate: 0.00013644\n",
      "Epoch 3179, Loss: 0.5432254877498303, learning rate: 0.00013642\n",
      "Epoch 3180, Loss: 0.543224358847807, learning rate: 0.0001364\n",
      "Epoch 3181, Loss: 0.5432232320958716, learning rate: 0.00013638000000000002\n",
      "Epoch 3182, Loss: 0.5432221040200051, learning rate: 0.00013636\n",
      "Epoch 3183, Loss: 0.5432209727262427, learning rate: 0.00013633999999999999\n",
      "Epoch 3184, Loss: 0.543219842910624, learning rate: 0.00013632\n",
      "Epoch 3185, Loss: 0.5432187164397769, learning rate: 0.0001363\n",
      "Epoch 3186, Loss: 0.5432175861175524, learning rate: 0.00013628000000000002\n",
      "Epoch 3187, Loss: 0.5432164622571668, learning rate: 0.00013626\n",
      "Epoch 3188, Loss: 0.5432153323050835, learning rate: 0.00013624\n",
      "Epoch 3189, Loss: 0.5432142007403215, learning rate: 0.00013622\n",
      "Epoch 3190, Loss: 0.5432130723958029, learning rate: 0.0001362\n",
      "Epoch 3191, Loss: 0.5432119431513284, learning rate: 0.00013618000000000001\n",
      "Epoch 3192, Loss: 0.5432108259982409, learning rate: 0.00013616\n",
      "Epoch 3193, Loss: 0.5432096907153485, learning rate: 0.00013614\n",
      "Epoch 3194, Loss: 0.5432085695651072, learning rate: 0.00013612\n",
      "Epoch 3195, Loss: 0.5432074384359651, learning rate: 0.0001361\n",
      "Epoch 3196, Loss: 0.5432063145366152, learning rate: 0.00013608\n",
      "Epoch 3197, Loss: 0.5432051861012034, learning rate: 0.00013606\n",
      "Epoch 3198, Loss: 0.5432040569176971, learning rate: 0.00013604\n",
      "Epoch 3199, Loss: 0.5432029327897698, learning rate: 0.00013602\n",
      "Epoch 3200, Loss: 0.5432018050987161, learning rate: 0.000136\n",
      "Epoch 3201, Loss: 0.5432006766745927, learning rate: 0.00013598\n",
      "Epoch 3202, Loss: 0.5431995570433794, learning rate: 0.00013596000000000002\n",
      "Epoch 3203, Loss: 0.5431984296992421, learning rate: 0.00013594\n",
      "Epoch 3204, Loss: 0.5431973015730105, learning rate: 0.00013591999999999999\n",
      "Epoch 3205, Loss: 0.5431961773234216, learning rate: 0.0001359\n",
      "Epoch 3206, Loss: 0.543195048668289, learning rate: 0.00013588\n",
      "Epoch 3207, Loss: 0.5431939246708823, learning rate: 0.00013586000000000002\n",
      "Epoch 3208, Loss: 0.543192805702648, learning rate: 0.00013584\n",
      "Epoch 3209, Loss: 0.5431916776989615, learning rate: 0.00013582\n",
      "Epoch 3210, Loss: 0.5431905547008049, learning rate: 0.0001358\n",
      "Epoch 3211, Loss: 0.5431894292978886, learning rate: 0.00013578\n",
      "Epoch 3212, Loss: 0.5431883097828558, learning rate: 0.00013576000000000001\n",
      "Epoch 3213, Loss: 0.5431871823257042, learning rate: 0.00013574\n",
      "Epoch 3214, Loss: 0.5431860549361626, learning rate: 0.00013572\n",
      "Epoch 3215, Loss: 0.5431849312798539, learning rate: 0.0001357\n",
      "Epoch 3216, Loss: 0.5431838091072441, learning rate: 0.00013568\n",
      "Epoch 3217, Loss: 0.5431826897688049, learning rate: 0.00013566\n",
      "Epoch 3218, Loss: 0.5431815716215427, learning rate: 0.00013564000000000002\n",
      "Epoch 3219, Loss: 0.5431804395448018, learning rate: 0.00013562\n",
      "Epoch 3220, Loss: 0.5431793155148161, learning rate: 0.0001356\n",
      "Epoch 3221, Loss: 0.5431781954392653, learning rate: 0.00013558\n",
      "Epoch 3222, Loss: 0.543177079343572, learning rate: 0.00013556\n",
      "Epoch 3223, Loss: 0.5431759548852996, learning rate: 0.00013554000000000002\n",
      "Epoch 3224, Loss: 0.543174833689893, learning rate: 0.00013552\n",
      "Epoch 3225, Loss: 0.5431737070992443, learning rate: 0.00013550000000000001\n",
      "Epoch 3226, Loss: 0.5431725854200802, learning rate: 0.00013548\n",
      "Epoch 3227, Loss: 0.5431714635125531, learning rate: 0.00013546\n",
      "Epoch 3228, Loss: 0.5431703491997821, learning rate: 0.00013544000000000002\n",
      "Epoch 3229, Loss: 0.5431692218204827, learning rate: 0.00013542\n",
      "Epoch 3230, Loss: 0.5431681038291382, learning rate: 0.0001354\n",
      "Epoch 3231, Loss: 0.5431669846926873, learning rate: 0.00013538\n",
      "Epoch 3232, Loss: 0.5431658673406178, learning rate: 0.00013536\n",
      "Epoch 3233, Loss: 0.5431647401701594, learning rate: 0.00013534000000000002\n",
      "Epoch 3234, Loss: 0.5431636249668674, learning rate: 0.00013532\n",
      "Epoch 3235, Loss: 0.5431624998269062, learning rate: 0.0001353\n",
      "Epoch 3236, Loss: 0.5431613795028103, learning rate: 0.00013528\n",
      "Epoch 3237, Loss: 0.5431602632243716, learning rate: 0.00013526\n",
      "Epoch 3238, Loss: 0.5431591455411979, learning rate: 0.00013524\n",
      "Epoch 3239, Loss: 0.5431580208081876, learning rate: 0.00013522000000000002\n",
      "Epoch 3240, Loss: 0.5431569024457045, learning rate: 0.0001352\n",
      "Epoch 3241, Loss: 0.5431557818964423, learning rate: 0.00013518\n",
      "Epoch 3242, Loss: 0.5431546709171767, learning rate: 0.00013516\n",
      "Epoch 3243, Loss: 0.5431535450808317, learning rate: 0.00013514\n",
      "Epoch 3244, Loss: 0.5431524272337324, learning rate: 0.00013512000000000002\n",
      "Epoch 3245, Loss: 0.5431513062768918, learning rate: 0.0001351\n",
      "Epoch 3246, Loss: 0.5431501897861584, learning rate: 0.00013508000000000001\n",
      "Epoch 3247, Loss: 0.5431490775301946, learning rate: 0.00013506\n",
      "Epoch 3248, Loss: 0.543147962073705, learning rate: 0.00013504\n",
      "Epoch 3249, Loss: 0.5431468368223272, learning rate: 0.00013502000000000002\n",
      "Epoch 3250, Loss: 0.543145718600439, learning rate: 0.000135\n",
      "Epoch 3251, Loss: 0.5431446075855059, learning rate: 0.00013498\n",
      "Epoch 3252, Loss: 0.543143488562514, learning rate: 0.00013496\n",
      "Epoch 3253, Loss: 0.5431423704294229, learning rate: 0.00013494\n",
      "Epoch 3254, Loss: 0.5431412562326863, learning rate: 0.00013492000000000002\n",
      "Epoch 3255, Loss: 0.5431401340875395, learning rate: 0.00013490000000000003\n",
      "Epoch 3256, Loss: 0.5431390183453432, learning rate: 0.00013488\n",
      "Epoch 3257, Loss: 0.5431379005005643, learning rate: 0.00013486\n",
      "Epoch 3258, Loss: 0.5431367895995421, learning rate: 0.00013484\n",
      "Epoch 3259, Loss: 0.5431356696778833, learning rate: 0.00013482000000000001\n",
      "Epoch 3260, Loss: 0.5431345524183135, learning rate: 0.00013480000000000002\n",
      "Epoch 3261, Loss: 0.5431334366084534, learning rate: 0.00013478\n",
      "Epoch 3262, Loss: 0.543132322352486, learning rate: 0.00013476000000000002\n",
      "Epoch 3263, Loss: 0.5431312106932229, learning rate: 0.00013474\n",
      "Epoch 3264, Loss: 0.5431300975142166, learning rate: 0.00013472\n",
      "Epoch 3265, Loss: 0.5431289762260832, learning rate: 0.00013470000000000002\n",
      "Epoch 3266, Loss: 0.5431278660304625, learning rate: 0.00013468\n",
      "Epoch 3267, Loss: 0.5431267482974693, learning rate: 0.00013466000000000002\n",
      "Epoch 3268, Loss: 0.5431256390172925, learning rate: 0.00013464\n",
      "Epoch 3269, Loss: 0.5431245180602167, learning rate: 0.00013462\n",
      "Epoch 3270, Loss: 0.5431234046198921, learning rate: 0.00013460000000000002\n",
      "Epoch 3271, Loss: 0.5431222884536281, learning rate: 0.00013458000000000003\n",
      "Epoch 3272, Loss: 0.543121177167088, learning rate: 0.00013456\n",
      "Epoch 3273, Loss: 0.5431200654323302, learning rate: 0.00013454\n",
      "Epoch 3274, Loss: 0.5431189522753314, learning rate: 0.00013452\n",
      "Epoch 3275, Loss: 0.5431178366212637, learning rate: 0.00013450000000000002\n",
      "Epoch 3276, Loss: 0.5431167256384416, learning rate: 0.00013448000000000003\n",
      "Epoch 3277, Loss: 0.5431156100625787, learning rate: 0.00013446\n",
      "Epoch 3278, Loss: 0.5431144964693927, learning rate: 0.00013444\n",
      "Epoch 3279, Loss: 0.5431133827763943, learning rate: 0.00013442\n",
      "Epoch 3280, Loss: 0.5431122766793194, learning rate: 0.00013440000000000001\n",
      "Epoch 3281, Loss: 0.5431111586666576, learning rate: 0.00013438000000000002\n",
      "Epoch 3282, Loss: 0.543110046885974, learning rate: 0.00013436\n",
      "Epoch 3283, Loss: 0.5431089338899492, learning rate: 0.00013434\n",
      "Epoch 3284, Loss: 0.5431078270655746, learning rate: 0.00013432\n",
      "Epoch 3285, Loss: 0.5431067126390359, learning rate: 0.0001343\n",
      "Epoch 3286, Loss: 0.5431056034316151, learning rate: 0.00013428000000000002\n",
      "Epoch 3287, Loss: 0.5431044904213814, learning rate: 0.00013426\n",
      "Epoch 3288, Loss: 0.5431033781089394, learning rate: 0.00013424\n",
      "Epoch 3289, Loss: 0.5431022696677829, learning rate: 0.00013422\n",
      "Epoch 3290, Loss: 0.5431011583964777, learning rate: 0.0001342\n",
      "Epoch 3291, Loss: 0.5431000467282399, learning rate: 0.00013418000000000002\n",
      "Epoch 3292, Loss: 0.5430989346781586, learning rate: 0.00013416000000000003\n",
      "Epoch 3293, Loss: 0.5430978249516266, learning rate: 0.00013413999999999999\n",
      "Epoch 3294, Loss: 0.5430967193971347, learning rate: 0.00013412\n",
      "Epoch 3295, Loss: 0.5430956046064154, learning rate: 0.0001341\n",
      "Epoch 3296, Loss: 0.5430944954868535, learning rate: 0.00013408000000000002\n",
      "Epoch 3297, Loss: 0.5430933838355902, learning rate: 0.00013406000000000003\n",
      "Epoch 3298, Loss: 0.5430922792773731, learning rate: 0.00013403999999999998\n",
      "Epoch 3299, Loss: 0.543091166990226, learning rate: 0.00013402\n",
      "Epoch 3300, Loss: 0.5430900591784993, learning rate: 0.000134\n",
      "Epoch 3301, Loss: 0.5430889502130868, learning rate: 0.00013398000000000001\n",
      "Epoch 3302, Loss: 0.5430878416624241, learning rate: 0.00013396000000000003\n",
      "Epoch 3303, Loss: 0.5430867359856993, learning rate: 0.00013394\n",
      "Epoch 3304, Loss: 0.5430856271087533, learning rate: 0.00013392\n",
      "Epoch 3305, Loss: 0.5430845148634883, learning rate: 0.0001339\n",
      "Epoch 3306, Loss: 0.5430834069974517, learning rate: 0.00013388\n",
      "Epoch 3307, Loss: 0.5430822976164245, learning rate: 0.00013386000000000002\n",
      "Epoch 3308, Loss: 0.543081190769141, learning rate: 0.00013384\n",
      "Epoch 3309, Loss: 0.5430800895266534, learning rate: 0.00013382\n",
      "Epoch 3310, Loss: 0.5430789806038101, learning rate: 0.0001338\n",
      "Epoch 3311, Loss: 0.5430778702346837, learning rate: 0.00013378\n",
      "Epoch 3312, Loss: 0.5430767644030808, learning rate: 0.00013376000000000002\n",
      "Epoch 3313, Loss: 0.5430756555082303, learning rate: 0.00013374\n",
      "Epoch 3314, Loss: 0.5430745502622628, learning rate: 0.00013372\n",
      "Epoch 3315, Loss: 0.5430734442252059, learning rate: 0.0001337\n",
      "Epoch 3316, Loss: 0.543072336460312, learning rate: 0.00013368\n",
      "Epoch 3317, Loss: 0.5430712289686733, learning rate: 0.00013366000000000002\n",
      "Epoch 3318, Loss: 0.5430701242429653, learning rate: 0.00013364\n",
      "Epoch 3319, Loss: 0.5430690253327751, learning rate: 0.00013361999999999998\n",
      "Epoch 3320, Loss: 0.5430679199376119, learning rate: 0.0001336\n",
      "Epoch 3321, Loss: 0.5430668124512984, learning rate: 0.00013358\n",
      "Epoch 3322, Loss: 0.5430657105131279, learning rate: 0.00013356000000000002\n",
      "Epoch 3323, Loss: 0.5430646055445474, learning rate: 0.00013354\n",
      "Epoch 3324, Loss: 0.543063498031146, learning rate: 0.00013352\n",
      "Epoch 3325, Loss: 0.5430623936093889, learning rate: 0.0001335\n",
      "Epoch 3326, Loss: 0.5430612886848953, learning rate: 0.00013348\n",
      "Epoch 3327, Loss: 0.5430601850526896, learning rate: 0.00013346\n",
      "Epoch 3328, Loss: 0.5430590807387887, learning rate: 0.00013344\n",
      "Epoch 3329, Loss: 0.543057977692186, learning rate: 0.00013342\n",
      "Epoch 3330, Loss: 0.5430568707638668, learning rate: 0.0001334\n",
      "Epoch 3331, Loss: 0.5430557690681908, learning rate: 0.00013338\n",
      "Epoch 3332, Loss: 0.5430546632188203, learning rate: 0.00013336\n",
      "Epoch 3333, Loss: 0.5430535613476692, learning rate: 0.00013334000000000002\n",
      "Epoch 3334, Loss: 0.5430524586046888, learning rate: 0.00013332\n",
      "Epoch 3335, Loss: 0.5430513545158203, learning rate: 0.0001333\n",
      "Epoch 3336, Loss: 0.5430502541766675, learning rate: 0.00013328\n",
      "Epoch 3337, Loss: 0.543049150276315, learning rate: 0.00013326\n",
      "Epoch 3338, Loss: 0.543048048890457, learning rate: 0.00013324000000000002\n",
      "Epoch 3339, Loss: 0.543046941280446, learning rate: 0.00013322\n",
      "Epoch 3340, Loss: 0.5430458456817004, learning rate: 0.0001332\n",
      "Epoch 3341, Loss: 0.5430447386890588, learning rate: 0.00013318\n",
      "Epoch 3342, Loss: 0.5430436422084793, learning rate: 0.00013316\n",
      "Epoch 3343, Loss: 0.5430425375899209, learning rate: 0.00013314000000000002\n",
      "Epoch 3344, Loss: 0.5430414379932804, learning rate: 0.00013312\n",
      "Epoch 3345, Loss: 0.543040336667751, learning rate: 0.0001331\n",
      "Epoch 3346, Loss: 0.5430392349692192, learning rate: 0.00013308\n",
      "Epoch 3347, Loss: 0.543038133988803, learning rate: 0.00013306\n",
      "Epoch 3348, Loss: 0.5430370332424928, learning rate: 0.00013304000000000001\n",
      "Epoch 3349, Loss: 0.5430359316990301, learning rate: 0.00013302\n",
      "Epoch 3350, Loss: 0.5430348325850591, learning rate: 0.000133\n",
      "Epoch 3351, Loss: 0.5430337319124174, learning rate: 0.00013298\n",
      "Epoch 3352, Loss: 0.543032633651624, learning rate: 0.00013296\n",
      "Epoch 3353, Loss: 0.5430315380411145, learning rate: 0.00013294\n",
      "Epoch 3354, Loss: 0.5430304318357839, learning rate: 0.00013292000000000002\n",
      "Epoch 3355, Loss: 0.5430293318144611, learning rate: 0.0001329\n",
      "Epoch 3356, Loss: 0.543028233050711, learning rate: 0.00013288\n",
      "Epoch 3357, Loss: 0.5430271377872218, learning rate: 0.00013286\n",
      "Epoch 3358, Loss: 0.5430260353648841, learning rate: 0.00013284\n",
      "Epoch 3359, Loss: 0.543024932517919, learning rate: 0.00013282000000000002\n",
      "Epoch 3360, Loss: 0.5430238430514769, learning rate: 0.0001328\n",
      "Epoch 3361, Loss: 0.5430227431027168, learning rate: 0.00013278\n",
      "Epoch 3362, Loss: 0.5430216399814058, learning rate: 0.00013276\n",
      "Epoch 3363, Loss: 0.5430205425243396, learning rate: 0.00013274\n",
      "Epoch 3364, Loss: 0.5430194449442349, learning rate: 0.00013272000000000002\n",
      "Epoch 3365, Loss: 0.5430183472404566, learning rate: 0.0001327\n",
      "Epoch 3366, Loss: 0.5430172535150923, learning rate: 0.00013268\n",
      "Epoch 3367, Loss: 0.5430161569191334, learning rate: 0.00013266\n",
      "Epoch 3368, Loss: 0.5430150553312406, learning rate: 0.00013264\n",
      "Epoch 3369, Loss: 0.5430139584804703, learning rate: 0.00013262000000000001\n",
      "Epoch 3370, Loss: 0.543012860587931, learning rate: 0.00013260000000000002\n",
      "Epoch 3371, Loss: 0.5430117677221171, learning rate: 0.00013258\n",
      "Epoch 3372, Loss: 0.5430106665444823, learning rate: 0.00013256\n",
      "Epoch 3373, Loss: 0.5430095700946036, learning rate: 0.00013254\n",
      "Epoch 3374, Loss: 0.5430084767244996, learning rate: 0.00013252\n",
      "Epoch 3375, Loss: 0.5430073786685959, learning rate: 0.00013250000000000002\n",
      "Epoch 3376, Loss: 0.5430062837251443, learning rate: 0.00013248\n",
      "Epoch 3377, Loss: 0.5430051879693579, learning rate: 0.00013246000000000002\n",
      "Epoch 3378, Loss: 0.5430040926209966, learning rate: 0.00013244\n",
      "Epoch 3379, Loss: 0.5430029964371433, learning rate: 0.00013242\n",
      "Epoch 3380, Loss: 0.5430018995015335, learning rate: 0.00013240000000000002\n",
      "Epoch 3381, Loss: 0.5430008091967755, learning rate: 0.00013238\n",
      "Epoch 3382, Loss: 0.5429997114641871, learning rate: 0.00013236\n",
      "Epoch 3383, Loss: 0.5429986202922993, learning rate: 0.00013234\n",
      "Epoch 3384, Loss: 0.5429975229140885, learning rate: 0.00013232\n",
      "Epoch 3385, Loss: 0.5429964310245889, learning rate: 0.00013230000000000002\n",
      "Epoch 3386, Loss: 0.5429953325227648, learning rate: 0.00013228\n",
      "Epoch 3387, Loss: 0.5429942382941251, learning rate: 0.00013226\n",
      "Epoch 3388, Loss: 0.542993144627408, learning rate: 0.00013224\n",
      "Epoch 3389, Loss: 0.5429920568273422, learning rate: 0.00013222\n",
      "Epoch 3390, Loss: 0.542990957545227, learning rate: 0.00013220000000000001\n",
      "Epoch 3391, Loss: 0.542989867470081, learning rate: 0.00013218000000000003\n",
      "Epoch 3392, Loss: 0.542988772590421, learning rate: 0.00013216\n",
      "Epoch 3393, Loss: 0.5429876813614282, learning rate: 0.00013214\n",
      "Epoch 3394, Loss: 0.5429865839429302, learning rate: 0.00013212\n",
      "Epoch 3395, Loss: 0.5429854951672506, learning rate: 0.0001321\n",
      "Epoch 3396, Loss: 0.542984400895147, learning rate: 0.00013208000000000002\n",
      "Epoch 3397, Loss: 0.5429833059930373, learning rate: 0.00013206\n",
      "Epoch 3398, Loss: 0.5429822130573976, learning rate: 0.00013204000000000002\n",
      "Epoch 3399, Loss: 0.5429811230505687, learning rate: 0.00013202\n",
      "Epoch 3400, Loss: 0.5429800337392213, learning rate: 0.000132\n",
      "Epoch 3401, Loss: 0.5429789451745786, learning rate: 0.00013198000000000002\n",
      "Epoch 3402, Loss: 0.5429778515829539, learning rate: 0.00013196\n",
      "Epoch 3403, Loss: 0.5429767589729955, learning rate: 0.00013194000000000001\n",
      "Epoch 3404, Loss: 0.54297566713854, learning rate: 0.00013192\n",
      "Epoch 3405, Loss: 0.5429745721935156, learning rate: 0.0001319\n",
      "Epoch 3406, Loss: 0.5429734838194339, learning rate: 0.00013188000000000002\n",
      "Epoch 3407, Loss: 0.5429723956923374, learning rate: 0.00013186000000000003\n",
      "Epoch 3408, Loss: 0.5429713022814424, learning rate: 0.00013184\n",
      "Epoch 3409, Loss: 0.5429702091285462, learning rate: 0.00013182\n",
      "Epoch 3410, Loss: 0.5429691263220199, learning rate: 0.0001318\n",
      "Epoch 3411, Loss: 0.5429680382760168, learning rate: 0.00013178000000000002\n",
      "Epoch 3412, Loss: 0.5429669466544892, learning rate: 0.00013176000000000003\n",
      "Epoch 3413, Loss: 0.5429658548381312, learning rate: 0.00013174\n",
      "Epoch 3414, Loss: 0.5429647713636517, learning rate: 0.00013172\n",
      "Epoch 3415, Loss: 0.5429636798409004, learning rate: 0.0001317\n",
      "Epoch 3416, Loss: 0.5429625898355305, learning rate: 0.00013168\n",
      "Epoch 3417, Loss: 0.5429614964394326, learning rate: 0.00013166000000000002\n",
      "Epoch 3418, Loss: 0.5429604083244061, learning rate: 0.00013164\n",
      "Epoch 3419, Loss: 0.5429593214961459, learning rate: 0.00013162\n",
      "Epoch 3420, Loss: 0.542958230510105, learning rate: 0.0001316\n",
      "Epoch 3421, Loss: 0.5429571453310776, learning rate: 0.00013158\n",
      "Epoch 3422, Loss: 0.5429560626164313, learning rate: 0.00013156000000000002\n",
      "Epoch 3423, Loss: 0.5429549691815934, learning rate: 0.00013154\n",
      "Epoch 3424, Loss: 0.5429538789315472, learning rate: 0.00013152\n",
      "Epoch 3425, Loss: 0.5429527975626988, learning rate: 0.0001315\n",
      "Epoch 3426, Loss: 0.5429517100354909, learning rate: 0.00013148\n",
      "Epoch 3427, Loss: 0.5429506198632488, learning rate: 0.00013146000000000002\n",
      "Epoch 3428, Loss: 0.5429495317972098, learning rate: 0.00013144000000000003\n",
      "Epoch 3429, Loss: 0.5429484471748405, learning rate: 0.00013141999999999999\n",
      "Epoch 3430, Loss: 0.542947363027193, learning rate: 0.0001314\n",
      "Epoch 3431, Loss: 0.5429462731542871, learning rate: 0.00013138\n",
      "Epoch 3432, Loss: 0.5429451856487275, learning rate: 0.00013136000000000002\n",
      "Epoch 3433, Loss: 0.5429441009434656, learning rate: 0.00013134000000000003\n",
      "Epoch 3434, Loss: 0.5429430169156607, learning rate: 0.00013131999999999998\n",
      "Epoch 3435, Loss: 0.5429419340943987, learning rate: 0.0001313\n",
      "Epoch 3436, Loss: 0.5429408465279388, learning rate: 0.00013128\n",
      "Epoch 3437, Loss: 0.5429397632005533, learning rate: 0.00013126000000000001\n",
      "Epoch 3438, Loss: 0.5429386748136072, learning rate: 0.00013124000000000002\n",
      "Epoch 3439, Loss: 0.5429375886964979, learning rate: 0.00013122\n",
      "Epoch 3440, Loss: 0.5429365033845063, learning rate: 0.0001312\n",
      "Epoch 3441, Loss: 0.5429354218745108, learning rate: 0.00013118\n",
      "Epoch 3442, Loss: 0.5429343426329281, learning rate: 0.00013116\n",
      "Epoch 3443, Loss: 0.542933253222309, learning rate: 0.00013114000000000002\n",
      "Epoch 3444, Loss: 0.542932165685078, learning rate: 0.00013112\n",
      "Epoch 3445, Loss: 0.5429310852501055, learning rate: 0.0001311\n",
      "Epoch 3446, Loss: 0.5429300038594019, learning rate: 0.00013108\n",
      "Epoch 3447, Loss: 0.5429289166770767, learning rate: 0.00013106\n",
      "Epoch 3448, Loss: 0.5429278356644026, learning rate: 0.00013104000000000002\n",
      "Epoch 3449, Loss: 0.5429267496399088, learning rate: 0.00013102\n",
      "Epoch 3450, Loss: 0.5429256651350032, learning rate: 0.00013099999999999999\n",
      "Epoch 3451, Loss: 0.5429245843037219, learning rate: 0.00013098\n",
      "Epoch 3452, Loss: 0.5429235017918755, learning rate: 0.00013096\n",
      "Epoch 3453, Loss: 0.5429224249037773, learning rate: 0.00013094000000000002\n",
      "Epoch 3454, Loss: 0.5429213355990159, learning rate: 0.00013092\n",
      "Epoch 3455, Loss: 0.5429202523348472, learning rate: 0.0001309\n",
      "Epoch 3456, Loss: 0.5429191768348082, learning rate: 0.00013088\n",
      "Epoch 3457, Loss: 0.5429180951204376, learning rate: 0.00013086\n",
      "Epoch 3458, Loss: 0.5429170092776476, learning rate: 0.00013084000000000001\n",
      "Epoch 3459, Loss: 0.542915926738253, learning rate: 0.00013082\n",
      "Epoch 3460, Loss: 0.5429148458883788, learning rate: 0.0001308\n",
      "Epoch 3461, Loss: 0.5429137662568044, learning rate: 0.00013078\n",
      "Epoch 3462, Loss: 0.5429126846230066, learning rate: 0.00013076\n",
      "Epoch 3463, Loss: 0.5429116066399959, learning rate: 0.00013074\n",
      "Epoch 3464, Loss: 0.5429105215904579, learning rate: 0.00013072\n",
      "Epoch 3465, Loss: 0.5429094409818107, learning rate: 0.0001307\n",
      "Epoch 3466, Loss: 0.5429083649646422, learning rate: 0.00013068\n",
      "Epoch 3467, Loss: 0.5429072834646246, learning rate: 0.00013066\n",
      "Epoch 3468, Loss: 0.542906201298291, learning rate: 0.00013064\n",
      "Epoch 3469, Loss: 0.5429051218615234, learning rate: 0.00013062000000000002\n",
      "Epoch 3470, Loss: 0.5429040413738127, learning rate: 0.0001306\n",
      "Epoch 3471, Loss: 0.5429029625489817, learning rate: 0.00013057999999999999\n",
      "Epoch 3472, Loss: 0.5429018859473752, learning rate: 0.00013056\n",
      "Epoch 3473, Loss: 0.5429008057963006, learning rate: 0.00013054\n",
      "Epoch 3474, Loss: 0.5428997259726255, learning rate: 0.00013052000000000002\n",
      "Epoch 3475, Loss: 0.5428986447979333, learning rate: 0.0001305\n",
      "Epoch 3476, Loss: 0.5428975679213309, learning rate: 0.00013048\n",
      "Epoch 3477, Loss: 0.5428964880221917, learning rate: 0.00013046\n",
      "Epoch 3478, Loss: 0.5428954155146627, learning rate: 0.00013044\n",
      "Epoch 3479, Loss: 0.5428943333751491, learning rate: 0.00013042000000000002\n",
      "Epoch 3480, Loss: 0.5428932588737178, learning rate: 0.0001304\n",
      "Epoch 3481, Loss: 0.5428921790984986, learning rate: 0.00013038\n",
      "Epoch 3482, Loss: 0.5428910989417804, learning rate: 0.00013036\n",
      "Epoch 3483, Loss: 0.5428900235178633, learning rate: 0.00013034\n",
      "Epoch 3484, Loss: 0.5428889444359257, learning rate: 0.00013032\n",
      "Epoch 3485, Loss: 0.5428878709010344, learning rate: 0.00013030000000000002\n",
      "Epoch 3486, Loss: 0.5428867894202833, learning rate: 0.00013028\n",
      "Epoch 3487, Loss: 0.5428857167012321, learning rate: 0.00013026\n",
      "Epoch 3488, Loss: 0.5428846367377108, learning rate: 0.00013024\n",
      "Epoch 3489, Loss: 0.5428835628816912, learning rate: 0.00013022\n",
      "Epoch 3490, Loss: 0.542882486950155, learning rate: 0.00013020000000000002\n",
      "Epoch 3491, Loss: 0.5428814144106169, learning rate: 0.00013018\n",
      "Epoch 3492, Loss: 0.5428803346593615, learning rate: 0.00013016000000000001\n",
      "Epoch 3493, Loss: 0.5428792628966674, learning rate: 0.00013014\n",
      "Epoch 3494, Loss: 0.5428781837226274, learning rate: 0.00013012\n",
      "Epoch 3495, Loss: 0.5428771115516109, learning rate: 0.00013010000000000002\n",
      "Epoch 3496, Loss: 0.5428760314268593, learning rate: 0.00013008\n",
      "Epoch 3497, Loss: 0.5428749575437579, learning rate: 0.00013006\n",
      "Epoch 3498, Loss: 0.5428738818908009, learning rate: 0.00013004\n",
      "Epoch 3499, Loss: 0.542872810389117, learning rate: 0.00013002\n",
      "Epoch 3500, Loss: 0.5428717351261871, learning rate: 0.00013000000000000002\n",
      "Epoch 3501, Loss: 0.5428706584099787, learning rate: 0.00012998\n",
      "Epoch 3502, Loss: 0.5428695873539932, learning rate: 0.00012996\n",
      "Epoch 3503, Loss: 0.5428685141456122, learning rate: 0.00012994\n",
      "Epoch 3504, Loss: 0.5428674375526276, learning rate: 0.00012992\n",
      "Epoch 3505, Loss: 0.5428663630595694, learning rate: 0.0001299\n",
      "Epoch 3506, Loss: 0.5428652910446432, learning rate: 0.00012988000000000002\n",
      "Epoch 3507, Loss: 0.5428642182388193, learning rate: 0.00012986\n",
      "Epoch 3508, Loss: 0.5428631452914202, learning rate: 0.00012984\n",
      "Epoch 3509, Loss: 0.5428620710513593, learning rate: 0.00012982\n",
      "Epoch 3510, Loss: 0.5428610011905979, learning rate: 0.0001298\n",
      "Epoch 3511, Loss: 0.5428599321591879, learning rate: 0.00012978000000000002\n",
      "Epoch 3512, Loss: 0.5428588525058041, learning rate: 0.00012976\n",
      "Epoch 3513, Loss: 0.5428577813244099, learning rate: 0.00012974000000000001\n",
      "Epoch 3514, Loss: 0.5428567127408128, learning rate: 0.00012972\n",
      "Epoch 3515, Loss: 0.5428556406468867, learning rate: 0.0001297\n",
      "Epoch 3516, Loss: 0.5428545646624382, learning rate: 0.00012968000000000002\n",
      "Epoch 3517, Loss: 0.5428534930400236, learning rate: 0.00012966\n",
      "Epoch 3518, Loss: 0.5428524222813855, learning rate: 0.00012964\n",
      "Epoch 3519, Loss: 0.5428513533072861, learning rate: 0.00012962\n",
      "Epoch 3520, Loss: 0.5428502824471301, learning rate: 0.0001296\n",
      "Epoch 3521, Loss: 0.5428492134722451, learning rate: 0.00012958000000000002\n",
      "Epoch 3522, Loss: 0.5428481416815475, learning rate: 0.00012956000000000003\n",
      "Epoch 3523, Loss: 0.5428470674415473, learning rate: 0.00012954\n",
      "Epoch 3524, Loss: 0.5428459969160978, learning rate: 0.00012952\n",
      "Epoch 3525, Loss: 0.5428449305914035, learning rate: 0.0001295\n",
      "Epoch 3526, Loss: 0.5428438563136933, learning rate: 0.00012948000000000001\n",
      "Epoch 3527, Loss: 0.5428427905893378, learning rate: 0.00012946000000000002\n",
      "Epoch 3528, Loss: 0.5428417218822355, learning rate: 0.00012944\n",
      "Epoch 3529, Loss: 0.5428406509656444, learning rate: 0.00012942000000000002\n",
      "Epoch 3530, Loss: 0.5428395789489173, learning rate: 0.0001294\n",
      "Epoch 3531, Loss: 0.5428385116778842, learning rate: 0.00012938\n",
      "Epoch 3532, Loss: 0.5428374411783539, learning rate: 0.00012936000000000002\n",
      "Epoch 3533, Loss: 0.5428363774717966, learning rate: 0.00012934\n",
      "Epoch 3534, Loss: 0.5428353051206268, learning rate: 0.00012932000000000002\n",
      "Epoch 3535, Loss: 0.5428342379791453, learning rate: 0.0001293\n",
      "Epoch 3536, Loss: 0.5428331702215189, learning rate: 0.00012928\n",
      "Epoch 3537, Loss: 0.5428320988569058, learning rate: 0.00012926000000000002\n",
      "Epoch 3538, Loss: 0.5428310350736375, learning rate: 0.00012924\n",
      "Epoch 3539, Loss: 0.542829972239121, learning rate: 0.00012922\n",
      "Epoch 3540, Loss: 0.5428288976495584, learning rate: 0.0001292\n",
      "Epoch 3541, Loss: 0.5428278289835066, learning rate: 0.00012918\n",
      "Epoch 3542, Loss: 0.5428267638214946, learning rate: 0.00012916000000000002\n",
      "Epoch 3543, Loss: 0.5428256939309205, learning rate: 0.00012914000000000003\n",
      "Epoch 3544, Loss: 0.5428246316350593, learning rate: 0.00012912\n",
      "Epoch 3545, Loss: 0.5428235689396188, learning rate: 0.0001291\n",
      "Epoch 3546, Loss: 0.5428224939916347, learning rate: 0.00012908\n",
      "Epoch 3547, Loss: 0.5428214276532347, learning rate: 0.00012906000000000001\n",
      "Epoch 3548, Loss: 0.5428203663672218, learning rate: 0.00012904000000000002\n",
      "Epoch 3549, Loss: 0.5428192970582205, learning rate: 0.00012902\n",
      "Epoch 3550, Loss: 0.5428182358491558, learning rate: 0.000129\n",
      "Epoch 3551, Loss: 0.5428171706269724, learning rate: 0.00012898\n",
      "Epoch 3552, Loss: 0.5428161029006061, learning rate: 0.00012896\n",
      "Epoch 3553, Loss: 0.5428150399382722, learning rate: 0.00012894000000000002\n",
      "Epoch 3554, Loss: 0.5428139703470101, learning rate: 0.00012892\n",
      "Epoch 3555, Loss: 0.5428129062189412, learning rate: 0.0001289\n",
      "Epoch 3556, Loss: 0.542811840983887, learning rate: 0.00012888\n",
      "Epoch 3557, Loss: 0.5428107757522926, learning rate: 0.00012886\n",
      "Epoch 3558, Loss: 0.5428097181344854, learning rate: 0.00012884000000000002\n",
      "Epoch 3559, Loss: 0.5428086479648513, learning rate: 0.00012882000000000003\n",
      "Epoch 3560, Loss: 0.5428075849000517, learning rate: 0.0001288\n",
      "Epoch 3561, Loss: 0.5428065223638985, learning rate: 0.00012878\n",
      "Epoch 3562, Loss: 0.5428054573297425, learning rate: 0.00012876\n",
      "Epoch 3563, Loss: 0.5428043954351647, learning rate: 0.00012874000000000002\n",
      "Epoch 3564, Loss: 0.5428033330011013, learning rate: 0.00012872000000000003\n",
      "Epoch 3565, Loss: 0.5428022752083539, learning rate: 0.00012869999999999998\n",
      "Epoch 3566, Loss: 0.542801207910463, learning rate: 0.00012868\n",
      "Epoch 3567, Loss: 0.5428001424574836, learning rate: 0.00012866\n",
      "Epoch 3568, Loss: 0.5427990796652938, learning rate: 0.00012864000000000002\n",
      "Epoch 3569, Loss: 0.5427980218086127, learning rate: 0.00012862000000000003\n",
      "Epoch 3570, Loss: 0.5427969591100814, learning rate: 0.00012859999999999998\n",
      "Epoch 3571, Loss: 0.5427958986877968, learning rate: 0.00012858\n",
      "Epoch 3572, Loss: 0.5427948344988414, learning rate: 0.00012856\n",
      "Epoch 3573, Loss: 0.5427937722286094, learning rate: 0.00012854\n",
      "Epoch 3574, Loss: 0.5427927129286817, learning rate: 0.00012852000000000002\n",
      "Epoch 3575, Loss: 0.5427916502655095, learning rate: 0.0001285\n",
      "Epoch 3576, Loss: 0.5427905932540256, learning rate: 0.00012848\n",
      "Epoch 3577, Loss: 0.5427895317007673, learning rate: 0.00012846\n",
      "Epoch 3578, Loss: 0.5427884677258085, learning rate: 0.00012844\n",
      "Epoch 3579, Loss: 0.542787407371122, learning rate: 0.00012842000000000002\n",
      "Epoch 3580, Loss: 0.5427863475341617, learning rate: 0.0001284\n",
      "Epoch 3581, Loss: 0.5427852926119108, learning rate: 0.00012838\n",
      "Epoch 3582, Loss: 0.5427842275638864, learning rate: 0.00012836\n",
      "Epoch 3583, Loss: 0.5427831665817249, learning rate: 0.00012834\n",
      "Epoch 3584, Loss: 0.542782112800805, learning rate: 0.00012832000000000002\n",
      "Epoch 3585, Loss: 0.5427810490701932, learning rate: 0.0001283\n",
      "Epoch 3586, Loss: 0.5427799929271555, learning rate: 0.00012827999999999998\n",
      "Epoch 3587, Loss: 0.5427789314951026, learning rate: 0.00012826\n",
      "Epoch 3588, Loss: 0.5427778741206705, learning rate: 0.00012824\n",
      "Epoch 3589, Loss: 0.5427768181341545, learning rate: 0.00012822000000000002\n",
      "Epoch 3590, Loss: 0.5427757557858571, learning rate: 0.0001282\n",
      "Epoch 3591, Loss: 0.5427746956874313, learning rate: 0.00012818\n",
      "Epoch 3592, Loss: 0.5427736426846497, learning rate: 0.00012816\n",
      "Epoch 3593, Loss: 0.5427725826942785, learning rate: 0.00012814\n",
      "Epoch 3594, Loss: 0.5427715240156693, learning rate: 0.00012812\n",
      "Epoch 3595, Loss: 0.5427704645247966, learning rate: 0.0001281\n",
      "Epoch 3596, Loss: 0.5427694094584619, learning rate: 0.00012808\n",
      "Epoch 3597, Loss: 0.5427683517777986, learning rate: 0.00012806\n",
      "Epoch 3598, Loss: 0.54276730252318, learning rate: 0.00012804\n",
      "Epoch 3599, Loss: 0.5427662383203785, learning rate: 0.00012802\n",
      "Epoch 3600, Loss: 0.5427651801446691, learning rate: 0.00012800000000000002\n",
      "Epoch 3601, Loss: 0.5427641272030304, learning rate: 0.00012798\n",
      "Epoch 3602, Loss: 0.5427630746140991, learning rate: 0.00012796\n",
      "Epoch 3603, Loss: 0.5427620148553279, learning rate: 0.00012794\n",
      "Epoch 3604, Loss: 0.5427609568202535, learning rate: 0.00012792\n",
      "Epoch 3605, Loss: 0.5427599039802061, learning rate: 0.00012790000000000002\n",
      "Epoch 3606, Loss: 0.542758845688892, learning rate: 0.00012788\n",
      "Epoch 3607, Loss: 0.5427577917295713, learning rate: 0.00012785999999999999\n",
      "Epoch 3608, Loss: 0.5427567401696716, learning rate: 0.00012784\n",
      "Epoch 3609, Loss: 0.5427556812986338, learning rate: 0.00012782\n",
      "Epoch 3610, Loss: 0.5427546258839323, learning rate: 0.00012780000000000002\n",
      "Epoch 3611, Loss: 0.5427535771380124, learning rate: 0.00012778\n",
      "Epoch 3612, Loss: 0.5427525175187401, learning rate: 0.00012776\n",
      "Epoch 3613, Loss: 0.5427514621469264, learning rate: 0.00012774\n",
      "Epoch 3614, Loss: 0.5427504109023373, learning rate: 0.00012772\n",
      "Epoch 3615, Loss: 0.5427493565947703, learning rate: 0.00012770000000000001\n",
      "Epoch 3616, Loss: 0.542748301767084, learning rate: 0.00012768\n",
      "Epoch 3617, Loss: 0.5427472472761432, learning rate: 0.00012766\n",
      "Epoch 3618, Loss: 0.5427461946905443, learning rate: 0.00012764\n",
      "Epoch 3619, Loss: 0.5427451405340523, learning rate: 0.00012762\n",
      "Epoch 3620, Loss: 0.5427440872899023, learning rate: 0.0001276\n",
      "Epoch 3621, Loss: 0.5427430421421245, learning rate: 0.00012758000000000002\n",
      "Epoch 3622, Loss: 0.5427419823561321, learning rate: 0.00012756\n",
      "Epoch 3623, Loss: 0.5427409291768994, learning rate: 0.00012754\n",
      "Epoch 3624, Loss: 0.5427398786969082, learning rate: 0.00012752\n",
      "Epoch 3625, Loss: 0.5427388305680206, learning rate: 0.0001275\n",
      "Epoch 3626, Loss: 0.5427377731591267, learning rate: 0.00012748000000000002\n",
      "Epoch 3627, Loss: 0.542736719594825, learning rate: 0.00012746\n",
      "Epoch 3628, Loss: 0.5427356721130726, learning rate: 0.00012744\n",
      "Epoch 3629, Loss: 0.5427346180038497, learning rate: 0.00012742\n",
      "Epoch 3630, Loss: 0.542733567959367, learning rate: 0.0001274\n",
      "Epoch 3631, Loss: 0.5427325188048582, learning rate: 0.00012738000000000002\n",
      "Epoch 3632, Loss: 0.5427314645812898, learning rate: 0.00012736\n",
      "Epoch 3633, Loss: 0.5427304127645656, learning rate: 0.00012734\n",
      "Epoch 3634, Loss: 0.5427293681862728, learning rate: 0.00012732\n",
      "Epoch 3635, Loss: 0.5427283132934215, learning rate: 0.0001273\n",
      "Epoch 3636, Loss: 0.5427272616169123, learning rate: 0.00012728000000000001\n",
      "Epoch 3637, Loss: 0.5427262111313369, learning rate: 0.00012726000000000003\n",
      "Epoch 3638, Loss: 0.542725161979515, learning rate: 0.00012724\n",
      "Epoch 3639, Loss: 0.5427241176170363, learning rate: 0.00012722\n",
      "Epoch 3640, Loss: 0.5427230619348706, learning rate: 0.0001272\n",
      "Epoch 3641, Loss: 0.5427220128922764, learning rate: 0.00012718\n",
      "Epoch 3642, Loss: 0.5427209679293565, learning rate: 0.00012716000000000002\n",
      "Epoch 3643, Loss: 0.5427199157447313, learning rate: 0.00012714\n",
      "Epoch 3644, Loss: 0.5427188644979706, learning rate: 0.00012712\n",
      "Epoch 3645, Loss: 0.5427178155799819, learning rate: 0.0001271\n",
      "Epoch 3646, Loss: 0.5427167701752996, learning rate: 0.00012708\n",
      "Epoch 3647, Loss: 0.5427157209611541, learning rate: 0.00012706000000000002\n",
      "Epoch 3648, Loss: 0.5427146757669155, learning rate: 0.00012704\n",
      "Epoch 3649, Loss: 0.5427136242606941, learning rate: 0.00012702000000000001\n",
      "Epoch 3650, Loss: 0.5427125748985631, learning rate: 0.000127\n",
      "Epoch 3651, Loss: 0.5427115288257812, learning rate: 0.00012698\n",
      "Epoch 3652, Loss: 0.5427104794433277, learning rate: 0.00012696000000000002\n",
      "Epoch 3653, Loss: 0.5427094362294614, learning rate: 0.00012694\n",
      "Epoch 3654, Loss: 0.5427083851765029, learning rate: 0.00012692\n",
      "Epoch 3655, Loss: 0.5427073392106693, learning rate: 0.0001269\n",
      "Epoch 3656, Loss: 0.5427062955517563, learning rate: 0.00012688\n",
      "Epoch 3657, Loss: 0.5427052449320042, learning rate: 0.00012686000000000002\n",
      "Epoch 3658, Loss: 0.542704197407952, learning rate: 0.00012684000000000003\n",
      "Epoch 3659, Loss: 0.5427031512779381, learning rate: 0.00012682\n",
      "Epoch 3660, Loss: 0.5427021089093027, learning rate: 0.0001268\n",
      "Epoch 3661, Loss: 0.5427010583325588, learning rate: 0.00012678\n",
      "Epoch 3662, Loss: 0.5427000141708176, learning rate: 0.00012676\n",
      "Epoch 3663, Loss: 0.5426989674996153, learning rate: 0.00012674000000000002\n",
      "Epoch 3664, Loss: 0.5426979232075212, learning rate: 0.00012672\n",
      "Epoch 3665, Loss: 0.5426968766745418, learning rate: 0.00012670000000000002\n",
      "Epoch 3666, Loss: 0.5426958377812466, learning rate: 0.00012668\n",
      "Epoch 3667, Loss: 0.5426947873222483, learning rate: 0.00012666\n",
      "Epoch 3668, Loss: 0.5426937417449591, learning rate: 0.00012664000000000002\n",
      "Epoch 3669, Loss: 0.5426927000781888, learning rate: 0.00012662\n",
      "Epoch 3670, Loss: 0.5426916542098094, learning rate: 0.00012660000000000001\n",
      "Epoch 3671, Loss: 0.5426906090047674, learning rate: 0.00012658\n",
      "Epoch 3672, Loss: 0.5426895635462924, learning rate: 0.00012656\n",
      "Epoch 3673, Loss: 0.5426885194318374, learning rate: 0.00012654000000000002\n",
      "Epoch 3674, Loss: 0.54268747758575, learning rate: 0.00012652000000000003\n",
      "Epoch 3675, Loss: 0.542686434376652, learning rate: 0.0001265\n",
      "Epoch 3676, Loss: 0.54268539271084, learning rate: 0.00012648\n",
      "Epoch 3677, Loss: 0.5426843454536987, learning rate: 0.00012646\n",
      "Epoch 3678, Loss: 0.5426833018947448, learning rate: 0.00012644000000000002\n",
      "Epoch 3679, Loss: 0.5426822594495357, learning rate: 0.00012642000000000003\n",
      "Epoch 3680, Loss: 0.5426812203189052, learning rate: 0.0001264\n",
      "Epoch 3681, Loss: 0.5426801744593337, learning rate: 0.00012638\n",
      "Epoch 3682, Loss: 0.5426791310320754, learning rate: 0.00012636\n",
      "Epoch 3683, Loss: 0.5426780893590473, learning rate: 0.00012634\n",
      "Epoch 3684, Loss: 0.5426770473363853, learning rate: 0.00012632000000000002\n",
      "Epoch 3685, Loss: 0.5426760049413367, learning rate: 0.0001263\n",
      "Epoch 3686, Loss: 0.5426749670313398, learning rate: 0.00012628\n",
      "Epoch 3687, Loss: 0.5426739224291884, learning rate: 0.00012626\n",
      "Epoch 3688, Loss: 0.5426728802085931, learning rate: 0.00012624\n",
      "Epoch 3689, Loss: 0.5426718409963847, learning rate: 0.00012622000000000002\n",
      "Epoch 3690, Loss: 0.542670797887968, learning rate: 0.0001262\n",
      "Epoch 3691, Loss: 0.5426697579406699, learning rate: 0.00012618\n",
      "Epoch 3692, Loss: 0.5426687156211378, learning rate: 0.00012616\n",
      "Epoch 3693, Loss: 0.5426676757604532, learning rate: 0.00012614\n",
      "Epoch 3694, Loss: 0.5426666405348064, learning rate: 0.00012612000000000002\n",
      "Epoch 3695, Loss: 0.5426655957847855, learning rate: 0.00012610000000000003\n",
      "Epoch 3696, Loss: 0.5426645551934034, learning rate: 0.00012607999999999999\n",
      "Epoch 3697, Loss: 0.5426635146124832, learning rate: 0.00012606\n",
      "Epoch 3698, Loss: 0.5426624750884462, learning rate: 0.00012604\n",
      "Epoch 3699, Loss: 0.5426614361897543, learning rate: 0.00012602000000000002\n",
      "Epoch 3700, Loss: 0.5426604015363814, learning rate: 0.00012600000000000003\n",
      "Epoch 3701, Loss: 0.5426593569697609, learning rate: 0.00012597999999999998\n",
      "Epoch 3702, Loss: 0.5426583168821519, learning rate: 0.00012596\n",
      "Epoch 3703, Loss: 0.542657279108572, learning rate: 0.00012594\n",
      "Epoch 3704, Loss: 0.5426562409728181, learning rate: 0.00012592000000000001\n",
      "Epoch 3705, Loss: 0.5426552021674443, learning rate: 0.00012590000000000002\n",
      "Epoch 3706, Loss: 0.5426541669475641, learning rate: 0.00012588\n",
      "Epoch 3707, Loss: 0.5426531256627843, learning rate: 0.00012586\n",
      "Epoch 3708, Loss: 0.5426520869660906, learning rate: 0.00012584\n",
      "Epoch 3709, Loss: 0.5426510502490537, learning rate: 0.00012582\n",
      "Epoch 3710, Loss: 0.5426500113283235, learning rate: 0.00012580000000000002\n",
      "Epoch 3711, Loss: 0.5426489718834036, learning rate: 0.00012578\n",
      "Epoch 3712, Loss: 0.5426479377949393, learning rate: 0.00012576\n",
      "Epoch 3713, Loss: 0.5426469024649595, learning rate: 0.00012574\n",
      "Epoch 3714, Loss: 0.5426458620407321, learning rate: 0.00012572\n",
      "Epoch 3715, Loss: 0.542644823584755, learning rate: 0.00012570000000000002\n",
      "Epoch 3716, Loss: 0.5426437876957493, learning rate: 0.00012568\n",
      "Epoch 3717, Loss: 0.5426427532829434, learning rate: 0.00012565999999999999\n",
      "Epoch 3718, Loss: 0.5426417159019225, learning rate: 0.00012564\n",
      "Epoch 3719, Loss: 0.5426406812752672, learning rate: 0.00012562\n",
      "Epoch 3720, Loss: 0.5426396445083532, learning rate: 0.00012560000000000002\n",
      "Epoch 3721, Loss: 0.542638609741582, learning rate: 0.00012558\n",
      "Epoch 3722, Loss: 0.5426375715550764, learning rate: 0.00012555999999999998\n",
      "Epoch 3723, Loss: 0.5426365354498726, learning rate: 0.00012554\n",
      "Epoch 3724, Loss: 0.5426354991498884, learning rate: 0.00012552\n",
      "Epoch 3725, Loss: 0.542634464293329, learning rate: 0.00012550000000000001\n",
      "Epoch 3726, Loss: 0.542633433769233, learning rate: 0.00012548\n",
      "Epoch 3727, Loss: 0.542632395627776, learning rate: 0.00012546\n",
      "Epoch 3728, Loss: 0.5426313595197413, learning rate: 0.00012544\n",
      "Epoch 3729, Loss: 0.54263032543974, learning rate: 0.00012542\n",
      "Epoch 3730, Loss: 0.5426292906871655, learning rate: 0.0001254\n",
      "Epoch 3731, Loss: 0.5426282576461665, learning rate: 0.00012538\n",
      "Epoch 3732, Loss: 0.5426272273768786, learning rate: 0.00012536\n",
      "Epoch 3733, Loss: 0.5426261904906932, learning rate: 0.00012534\n",
      "Epoch 3734, Loss: 0.54262515481292, learning rate: 0.00012532\n",
      "Epoch 3735, Loss: 0.5426241209128242, learning rate: 0.0001253\n",
      "Epoch 3736, Loss: 0.5426230898213058, learning rate: 0.00012528000000000002\n",
      "Epoch 3737, Loss: 0.5426220534455758, learning rate: 0.00012526\n",
      "Epoch 3738, Loss: 0.5426210229468992, learning rate: 0.00012524\n",
      "Epoch 3739, Loss: 0.542619989164666, learning rate: 0.00012522\n",
      "Epoch 3740, Loss: 0.5426189552505305, learning rate: 0.0001252\n",
      "Epoch 3741, Loss: 0.5426179220587629, learning rate: 0.00012518000000000002\n",
      "Epoch 3742, Loss: 0.5426168894472863, learning rate: 0.00012516\n",
      "Epoch 3743, Loss: 0.5426158616428388, learning rate: 0.00012514\n",
      "Epoch 3744, Loss: 0.5426148245340158, learning rate: 0.00012512\n",
      "Epoch 3745, Loss: 0.5426137919196019, learning rate: 0.0001251\n",
      "Epoch 3746, Loss: 0.5426127644913086, learning rate: 0.00012508000000000002\n",
      "Epoch 3747, Loss: 0.5426117291828892, learning rate: 0.00012506\n",
      "Epoch 3748, Loss: 0.5426106987940693, learning rate: 0.00012504\n",
      "Epoch 3749, Loss: 0.5426096669027467, learning rate: 0.00012502\n",
      "Epoch 3750, Loss: 0.5426086381548152, learning rate: 0.000125\n",
      "Epoch 3751, Loss: 0.54260760504935, learning rate: 0.00012498\n",
      "Epoch 3752, Loss: 0.5426065782887478, learning rate: 0.00012496\n",
      "Epoch 3753, Loss: 0.5426055442942763, learning rate: 0.00012494\n",
      "Epoch 3754, Loss: 0.5426045113734309, learning rate: 0.00012492\n",
      "Epoch 3755, Loss: 0.5426034845608748, learning rate: 0.0001249\n",
      "Epoch 3756, Loss: 0.5426024578493864, learning rate: 0.00012488\n",
      "Epoch 3757, Loss: 0.5426014228715691, learning rate: 0.00012486000000000002\n",
      "Epoch 3758, Loss: 0.542600392475889, learning rate: 0.00012484\n",
      "Epoch 3759, Loss: 0.5425993640942333, learning rate: 0.00012482\n",
      "Epoch 3760, Loss: 0.5425983330391785, learning rate: 0.0001248\n",
      "Epoch 3761, Loss: 0.5425973080660503, learning rate: 0.00012478\n",
      "Epoch 3762, Loss: 0.5425962785677735, learning rate: 0.00012476000000000002\n",
      "Epoch 3763, Loss: 0.5425952477070042, learning rate: 0.00012474\n",
      "Epoch 3764, Loss: 0.5425942241070617, learning rate: 0.00012472\n",
      "Epoch 3765, Loss: 0.542593189655939, learning rate: 0.0001247\n",
      "Epoch 3766, Loss: 0.5425921622690393, learning rate: 0.00012468\n",
      "Epoch 3767, Loss: 0.5425911351632896, learning rate: 0.00012466000000000002\n",
      "Epoch 3768, Loss: 0.5425901051428718, learning rate: 0.00012464\n",
      "Epoch 3769, Loss: 0.5425890816445909, learning rate: 0.00012462\n",
      "Epoch 3770, Loss: 0.542588056763969, learning rate: 0.0001246\n",
      "Epoch 3771, Loss: 0.5425870245406593, learning rate: 0.00012458\n",
      "Epoch 3772, Loss: 0.5425859970200955, learning rate: 0.00012456000000000001\n",
      "Epoch 3773, Loss: 0.5425849719470413, learning rate: 0.00012454000000000002\n",
      "Epoch 3774, Loss: 0.5425839434723801, learning rate: 0.00012452\n",
      "Epoch 3775, Loss: 0.5425829174870382, learning rate: 0.0001245\n",
      "Epoch 3776, Loss: 0.5425818895289038, learning rate: 0.00012448\n",
      "Epoch 3777, Loss: 0.5425808704695289, learning rate: 0.00012446\n",
      "Epoch 3778, Loss: 0.5425798376836027, learning rate: 0.00012444000000000002\n",
      "Epoch 3779, Loss: 0.542578811614927, learning rate: 0.00012442\n",
      "Epoch 3780, Loss: 0.542577785907069, learning rate: 0.00012440000000000002\n",
      "Epoch 3781, Loss: 0.5425767646154985, learning rate: 0.00012438\n",
      "Epoch 3782, Loss: 0.5425757338391072, learning rate: 0.00012436\n",
      "Epoch 3783, Loss: 0.5425747161783596, learning rate: 0.00012434000000000002\n",
      "Epoch 3784, Loss: 0.5425736846136371, learning rate: 0.00012432\n",
      "Epoch 3785, Loss: 0.5425726587285733, learning rate: 0.0001243\n",
      "Epoch 3786, Loss: 0.5425716355134005, learning rate: 0.00012428\n",
      "Epoch 3787, Loss: 0.5425706106588588, learning rate: 0.00012426\n",
      "Epoch 3788, Loss: 0.542569589454738, learning rate: 0.00012424000000000002\n",
      "Epoch 3789, Loss: 0.5425685645884069, learning rate: 0.00012422\n",
      "Epoch 3790, Loss: 0.5425675366754995, learning rate: 0.0001242\n",
      "Epoch 3791, Loss: 0.5425665126027784, learning rate: 0.00012418\n",
      "Epoch 3792, Loss: 0.5425654914003412, learning rate: 0.00012416\n",
      "Epoch 3793, Loss: 0.5425644662724682, learning rate: 0.00012414000000000001\n",
      "Epoch 3794, Loss: 0.5425634466138787, learning rate: 0.00012412000000000002\n",
      "Epoch 3795, Loss: 0.5425624215270551, learning rate: 0.0001241\n",
      "Epoch 3796, Loss: 0.5425613971404264, learning rate: 0.00012408\n",
      "Epoch 3797, Loss: 0.5425603735229058, learning rate: 0.00012406\n",
      "Epoch 3798, Loss: 0.5425593516347463, learning rate: 0.00012404\n",
      "Epoch 3799, Loss: 0.5425583307843311, learning rate: 0.00012402000000000002\n",
      "Epoch 3800, Loss: 0.5425573121197909, learning rate: 0.000124\n",
      "Epoch 3801, Loss: 0.54255628522677, learning rate: 0.00012398000000000002\n",
      "Epoch 3802, Loss: 0.5425552632470708, learning rate: 0.00012396\n",
      "Epoch 3803, Loss: 0.5425542442300256, learning rate: 0.00012394\n",
      "Epoch 3804, Loss: 0.5425532193508269, learning rate: 0.00012392000000000002\n",
      "Epoch 3805, Loss: 0.5425521994113958, learning rate: 0.0001239\n",
      "Epoch 3806, Loss: 0.5425511794914804, learning rate: 0.00012388\n",
      "Epoch 3807, Loss: 0.5425501561827029, learning rate: 0.00012386\n",
      "Epoch 3808, Loss: 0.5425491358260451, learning rate: 0.00012384\n",
      "Epoch 3809, Loss: 0.5425481148521, learning rate: 0.00012382000000000002\n",
      "Epoch 3810, Loss: 0.5425470941935515, learning rate: 0.00012380000000000003\n",
      "Epoch 3811, Loss: 0.5425460715107651, learning rate: 0.00012378\n",
      "Epoch 3812, Loss: 0.5425450526747281, learning rate: 0.00012376\n",
      "Epoch 3813, Loss: 0.5425440373745811, learning rate: 0.00012374\n",
      "Epoch 3814, Loss: 0.5425430136056947, learning rate: 0.00012372000000000001\n",
      "Epoch 3815, Loss: 0.5425419912520668, learning rate: 0.00012370000000000003\n",
      "Epoch 3816, Loss: 0.5425409741590754, learning rate: 0.00012368\n",
      "Epoch 3817, Loss: 0.5425399516621192, learning rate: 0.00012366\n",
      "Epoch 3818, Loss: 0.5425389371003874, learning rate: 0.00012364\n",
      "Epoch 3819, Loss: 0.5425379148627714, learning rate: 0.00012362\n",
      "Epoch 3820, Loss: 0.5425368950847925, learning rate: 0.00012360000000000002\n",
      "Epoch 3821, Loss: 0.5425358804107562, learning rate: 0.00012358\n",
      "Epoch 3822, Loss: 0.5425348589132558, learning rate: 0.00012356\n",
      "Epoch 3823, Loss: 0.542533841176675, learning rate: 0.00012354\n",
      "Epoch 3824, Loss: 0.5425328232354486, learning rate: 0.00012352\n",
      "Epoch 3825, Loss: 0.5425318064557589, learning rate: 0.00012350000000000002\n",
      "Epoch 3826, Loss: 0.5425307849314704, learning rate: 0.00012348000000000003\n",
      "Epoch 3827, Loss: 0.5425297676015812, learning rate: 0.00012346\n",
      "Epoch 3828, Loss: 0.5425287501378593, learning rate: 0.00012344\n",
      "Epoch 3829, Loss: 0.5425277341592922, learning rate: 0.00012342\n",
      "Epoch 3830, Loss: 0.5425267135343852, learning rate: 0.00012340000000000002\n",
      "Epoch 3831, Loss: 0.5425257009633456, learning rate: 0.00012338000000000003\n",
      "Epoch 3832, Loss: 0.5425246821258595, learning rate: 0.00012335999999999998\n",
      "Epoch 3833, Loss: 0.542523671241957, learning rate: 0.00012334\n",
      "Epoch 3834, Loss: 0.5425226481511237, learning rate: 0.00012332\n",
      "Epoch 3835, Loss: 0.5425216302066294, learning rate: 0.00012330000000000002\n",
      "Epoch 3836, Loss: 0.5425206144396613, learning rate: 0.00012328000000000003\n",
      "Epoch 3837, Loss: 0.5425196040372267, learning rate: 0.00012325999999999998\n",
      "Epoch 3838, Loss: 0.5425185828956683, learning rate: 0.00012324\n",
      "Epoch 3839, Loss: 0.5425175670254208, learning rate: 0.00012322\n",
      "Epoch 3840, Loss: 0.5425165535020461, learning rate: 0.0001232\n",
      "Epoch 3841, Loss: 0.5425155364639431, learning rate: 0.00012318000000000002\n",
      "Epoch 3842, Loss: 0.5425145226552803, learning rate: 0.00012316\n",
      "Epoch 3843, Loss: 0.5425135110074836, learning rate: 0.00012314\n",
      "Epoch 3844, Loss: 0.5425124917275493, learning rate: 0.00012312\n",
      "Epoch 3845, Loss: 0.5425114756217214, learning rate: 0.0001231\n",
      "Epoch 3846, Loss: 0.5425104617648879, learning rate: 0.00012308000000000002\n",
      "Epoch 3847, Loss: 0.5425094482504706, learning rate: 0.00012306\n",
      "Epoch 3848, Loss: 0.5425084348978794, learning rate: 0.00012304\n",
      "Epoch 3849, Loss: 0.5425074252812561, learning rate: 0.00012302\n",
      "Epoch 3850, Loss: 0.5425064063195466, learning rate: 0.000123\n",
      "Epoch 3851, Loss: 0.5425053917093237, learning rate: 0.00012298000000000002\n",
      "Epoch 3852, Loss: 0.5425043799779186, learning rate: 0.00012296\n",
      "Epoch 3853, Loss: 0.5425033675520634, learning rate: 0.00012293999999999999\n",
      "Epoch 3854, Loss: 0.5425023567542433, learning rate: 0.00012292\n",
      "Epoch 3855, Loss: 0.5425013401815318, learning rate: 0.0001229\n",
      "Epoch 3856, Loss: 0.542500325446429, learning rate: 0.00012288000000000002\n",
      "Epoch 3857, Loss: 0.5424993127577717, learning rate: 0.00012286\n",
      "Epoch 3858, Loss: 0.5424983019709531, learning rate: 0.00012284\n",
      "Epoch 3859, Loss: 0.5424972935847026, learning rate: 0.00012282\n",
      "Epoch 3860, Loss: 0.5424962765006891, learning rate: 0.0001228\n",
      "Epoch 3861, Loss: 0.5424952649795854, learning rate: 0.00012278000000000001\n",
      "Epoch 3862, Loss: 0.5424942556303793, learning rate: 0.00012276\n",
      "Epoch 3863, Loss: 0.5424932429075686, learning rate: 0.00012274\n",
      "Epoch 3864, Loss: 0.5424922351005085, learning rate: 0.00012272\n",
      "Epoch 3865, Loss: 0.5424912193216791, learning rate: 0.0001227\n",
      "Epoch 3866, Loss: 0.5424902086832822, learning rate: 0.00012268\n",
      "Epoch 3867, Loss: 0.5424892004743782, learning rate: 0.00012266\n",
      "Epoch 3868, Loss: 0.542488186794751, learning rate: 0.00012264\n",
      "Epoch 3869, Loss: 0.5424871763825706, learning rate: 0.00012262\n",
      "Epoch 3870, Loss: 0.5424861668167732, learning rate: 0.0001226\n",
      "Epoch 3871, Loss: 0.5424851586863308, learning rate: 0.00012258\n",
      "Epoch 3872, Loss: 0.5424841456555315, learning rate: 0.00012256000000000002\n",
      "Epoch 3873, Loss: 0.5424831371451039, learning rate: 0.00012254\n",
      "Epoch 3874, Loss: 0.5424821266835078, learning rate: 0.00012251999999999999\n",
      "Epoch 3875, Loss: 0.5424811188540172, learning rate: 0.0001225\n",
      "Epoch 3876, Loss: 0.5424801074993449, learning rate: 0.00012248\n",
      "Epoch 3877, Loss: 0.5424790984277742, learning rate: 0.00012246000000000002\n",
      "Epoch 3878, Loss: 0.5424780918534364, learning rate: 0.00012244\n",
      "Epoch 3879, Loss: 0.5424770845491635, learning rate: 0.00012242\n",
      "Epoch 3880, Loss: 0.5424760732271228, learning rate: 0.0001224\n",
      "Epoch 3881, Loss: 0.5424750643186218, learning rate: 0.00012238\n",
      "Epoch 3882, Loss: 0.5424740545124733, learning rate: 0.00012236000000000001\n",
      "Epoch 3883, Loss: 0.5424730475884222, learning rate: 0.00012234\n",
      "Epoch 3884, Loss: 0.5424720400314578, learning rate: 0.00012232\n",
      "Epoch 3885, Loss: 0.5424710304486905, learning rate: 0.0001223\n",
      "Epoch 3886, Loss: 0.5424700270493475, learning rate: 0.00012228\n",
      "Epoch 3887, Loss: 0.5424690164213849, learning rate: 0.00012226\n",
      "Epoch 3888, Loss: 0.5424680107125855, learning rate: 0.00012224000000000002\n",
      "Epoch 3889, Loss: 0.5424670019786599, learning rate: 0.00012222\n",
      "Epoch 3890, Loss: 0.5424659971805542, learning rate: 0.0001222\n",
      "Epoch 3891, Loss: 0.5424649878230415, learning rate: 0.00012218\n",
      "Epoch 3892, Loss: 0.5424639848515319, learning rate: 0.00012216\n",
      "Epoch 3893, Loss: 0.5424629766071245, learning rate: 0.00012214000000000002\n",
      "Epoch 3894, Loss: 0.5424619741230302, learning rate: 0.00012212\n",
      "Epoch 3895, Loss: 0.542460963893098, learning rate: 0.0001221\n",
      "Epoch 3896, Loss: 0.5424599564760133, learning rate: 0.00012208\n",
      "Epoch 3897, Loss: 0.5424589532358939, learning rate: 0.00012206000000000001\n",
      "Epoch 3898, Loss: 0.5424579472341582, learning rate: 0.00012204\n",
      "Epoch 3899, Loss: 0.5424569458251842, learning rate: 0.00012202\n",
      "Epoch 3900, Loss: 0.5424559364432152, learning rate: 0.00012200000000000001\n",
      "Epoch 3901, Loss: 0.542454929926754, learning rate: 0.00012198000000000001\n",
      "Epoch 3902, Loss: 0.5424539252891175, learning rate: 0.00012196\n",
      "Epoch 3903, Loss: 0.5424529228713736, learning rate: 0.00012194\n",
      "Epoch 3904, Loss: 0.5424519168752057, learning rate: 0.00012192\n",
      "Epoch 3905, Loss: 0.5424509106001337, learning rate: 0.00012190000000000001\n",
      "Epoch 3906, Loss: 0.5424499093380034, learning rate: 0.00012188\n",
      "Epoch 3907, Loss: 0.5424489025755981, learning rate: 0.00012186000000000002\n",
      "Epoch 3908, Loss: 0.5424479006033931, learning rate: 0.00012184\n",
      "Epoch 3909, Loss: 0.5424468974240101, learning rate: 0.00012182000000000001\n",
      "Epoch 3910, Loss: 0.5424458934348055, learning rate: 0.0001218\n",
      "Epoch 3911, Loss: 0.542444890901505, learning rate: 0.00012178\n",
      "Epoch 3912, Loss: 0.5424438875481106, learning rate: 0.00012176000000000001\n",
      "Epoch 3913, Loss: 0.5424428877082226, learning rate: 0.00012174\n",
      "Epoch 3914, Loss: 0.5424418817122395, learning rate: 0.00012172000000000001\n",
      "Epoch 3915, Loss: 0.5424408784663599, learning rate: 0.0001217\n",
      "Epoch 3916, Loss: 0.5424398779932678, learning rate: 0.00012168000000000001\n",
      "Epoch 3917, Loss: 0.5424388752729556, learning rate: 0.00012166000000000001\n",
      "Epoch 3918, Loss: 0.5424378711123584, learning rate: 0.00012164000000000001\n",
      "Epoch 3919, Loss: 0.5424368714146882, learning rate: 0.00012162\n",
      "Epoch 3920, Loss: 0.5424358669701705, learning rate: 0.0001216\n",
      "Epoch 3921, Loss: 0.542434865325373, learning rate: 0.00012158000000000001\n",
      "Epoch 3922, Loss: 0.5424338617181065, learning rate: 0.00012156000000000001\n",
      "Epoch 3923, Loss: 0.5424328626969112, learning rate: 0.00012154\n",
      "Epoch 3924, Loss: 0.5424318622968269, learning rate: 0.00012152\n",
      "Epoch 3925, Loss: 0.5424308562565011, learning rate: 0.00012150000000000001\n",
      "Epoch 3926, Loss: 0.5424298537522456, learning rate: 0.00012148000000000001\n",
      "Epoch 3927, Loss: 0.5424288527210984, learning rate: 0.00012146\n",
      "Epoch 3928, Loss: 0.5424278528749111, learning rate: 0.00012144\n",
      "Epoch 3929, Loss: 0.5424268507945187, learning rate: 0.00012142\n",
      "Epoch 3930, Loss: 0.5424258506916632, learning rate: 0.00012140000000000001\n",
      "Epoch 3931, Loss: 0.5424248522941243, learning rate: 0.00012138\n",
      "Epoch 3932, Loss: 0.5424238510819641, learning rate: 0.00012136000000000002\n",
      "Epoch 3933, Loss: 0.5424228489975423, learning rate: 0.00012134\n",
      "Epoch 3934, Loss: 0.5424218487434923, learning rate: 0.00012132000000000001\n",
      "Epoch 3935, Loss: 0.5424208507961054, learning rate: 0.00012130000000000001\n",
      "Epoch 3936, Loss: 0.5424198520203741, learning rate: 0.00012128\n",
      "Epoch 3937, Loss: 0.5424188503314715, learning rate: 0.00012126000000000001\n",
      "Epoch 3938, Loss: 0.5424178514943412, learning rate: 0.00012124\n",
      "Epoch 3939, Loss: 0.542416849618379, learning rate: 0.00012122000000000001\n",
      "Epoch 3940, Loss: 0.5424158542425327, learning rate: 0.0001212\n",
      "Epoch 3941, Loss: 0.5424148580295972, learning rate: 0.00012118\n",
      "Epoch 3942, Loss: 0.5424138551633944, learning rate: 0.00012116000000000001\n",
      "Epoch 3943, Loss: 0.5424128544831716, learning rate: 0.00012114\n",
      "Epoch 3944, Loss: 0.5424118566293346, learning rate: 0.00012112\n",
      "Epoch 3945, Loss: 0.5424108619825214, learning rate: 0.0001211\n",
      "Epoch 3946, Loss: 0.5424098607930253, learning rate: 0.00012108000000000001\n",
      "Epoch 3947, Loss: 0.5424088622063498, learning rate: 0.00012106000000000001\n",
      "Epoch 3948, Loss: 0.5424078649741099, learning rate: 0.00012104\n",
      "Epoch 3949, Loss: 0.5424068704679508, learning rate: 0.00012102\n",
      "Epoch 3950, Loss: 0.542405871695316, learning rate: 0.000121\n",
      "Epoch 3951, Loss: 0.5424048724311211, learning rate: 0.00012098000000000001\n",
      "Epoch 3952, Loss: 0.5424038744139874, learning rate: 0.00012096000000000001\n",
      "Epoch 3953, Loss: 0.542402880584666, learning rate: 0.00012094\n",
      "Epoch 3954, Loss: 0.5424018849589538, learning rate: 0.00012092\n",
      "Epoch 3955, Loss: 0.5424008853100342, learning rate: 0.00012090000000000001\n",
      "Epoch 3956, Loss: 0.5423998890264822, learning rate: 0.00012088000000000001\n",
      "Epoch 3957, Loss: 0.5423988955129961, learning rate: 0.00012086\n",
      "Epoch 3958, Loss: 0.5423979036388084, learning rate: 0.00012084\n",
      "Epoch 3959, Loss: 0.542396901808114, learning rate: 0.00012082\n",
      "Epoch 3960, Loss: 0.5423959051523872, learning rate: 0.00012080000000000001\n",
      "Epoch 3961, Loss: 0.5423949098289355, learning rate: 0.00012078\n",
      "Epoch 3962, Loss: 0.5423939140661991, learning rate: 0.00012076000000000002\n",
      "Epoch 3963, Loss: 0.5423929222906385, learning rate: 0.00012074\n",
      "Epoch 3964, Loss: 0.5423919239017351, learning rate: 0.00012072000000000001\n",
      "Epoch 3965, Loss: 0.5423909293669695, learning rate: 0.0001207\n",
      "Epoch 3966, Loss: 0.5423899357737524, learning rate: 0.00012068\n",
      "Epoch 3967, Loss: 0.5423889406842302, learning rate: 0.00012066000000000001\n",
      "Epoch 3968, Loss: 0.5423879485511912, learning rate: 0.00012064\n",
      "Epoch 3969, Loss: 0.5423869531666142, learning rate: 0.00012062000000000001\n",
      "Epoch 3970, Loss: 0.542385959276318, learning rate: 0.0001206\n",
      "Epoch 3971, Loss: 0.542384966316885, learning rate: 0.00012058000000000001\n",
      "Epoch 3972, Loss: 0.5423839732584447, learning rate: 0.00012056000000000001\n",
      "Epoch 3973, Loss: 0.5423829783079345, learning rate: 0.00012054\n",
      "Epoch 3974, Loss: 0.5423819864522333, learning rate: 0.00012052\n",
      "Epoch 3975, Loss: 0.5423809918378852, learning rate: 0.0001205\n",
      "Epoch 3976, Loss: 0.542379998881715, learning rate: 0.00012048000000000001\n",
      "Epoch 3977, Loss: 0.5423790088739489, learning rate: 0.00012046000000000001\n",
      "Epoch 3978, Loss: 0.5423780133452795, learning rate: 0.00012044\n",
      "Epoch 3979, Loss: 0.542377024577501, learning rate: 0.00012042\n",
      "Epoch 3980, Loss: 0.5423760281921516, learning rate: 0.0001204\n",
      "Epoch 3981, Loss: 0.5423750359491071, learning rate: 0.00012038000000000001\n",
      "Epoch 3982, Loss: 0.5423740436592996, learning rate: 0.00012036\n",
      "Epoch 3983, Loss: 0.5423730527469218, learning rate: 0.00012034\n",
      "Epoch 3984, Loss: 0.5423720613441357, learning rate: 0.00012032\n",
      "Epoch 3985, Loss: 0.5423710677454563, learning rate: 0.00012030000000000001\n",
      "Epoch 3986, Loss: 0.5423700785520357, learning rate: 0.00012028000000000001\n",
      "Epoch 3987, Loss: 0.5423690905634954, learning rate: 0.00012026\n",
      "Epoch 3988, Loss: 0.542368093148758, learning rate: 0.00012024\n",
      "Epoch 3989, Loss: 0.5423671027184452, learning rate: 0.00012022\n",
      "Epoch 3990, Loss: 0.5423661152240403, learning rate: 0.00012020000000000001\n",
      "Epoch 3991, Loss: 0.5423651203818588, learning rate: 0.00012018\n",
      "Epoch 3992, Loss: 0.5423641294366122, learning rate: 0.00012016000000000002\n",
      "Epoch 3993, Loss: 0.5423631392611085, learning rate: 0.00012014\n",
      "Epoch 3994, Loss: 0.5423621566390594, learning rate: 0.00012012000000000001\n",
      "Epoch 3995, Loss: 0.5423611602425968, learning rate: 0.0001201\n",
      "Epoch 3996, Loss: 0.5423601675096829, learning rate: 0.00012008\n",
      "Epoch 3997, Loss: 0.5423591774797593, learning rate: 0.00012006000000000001\n",
      "Epoch 3998, Loss: 0.5423581953330483, learning rate: 0.00012004\n",
      "Epoch 3999, Loss: 0.5423571996032747, learning rate: 0.00012002\n",
      "Epoch 4000, Loss: 0.5423562096030504, learning rate: 0.00012\n",
      "Epoch 4001, Loss: 0.542355226032885, learning rate: 0.00011998000000000001\n",
      "Epoch 4002, Loss: 0.5423542332062768, learning rate: 0.00011996000000000001\n",
      "Epoch 4003, Loss: 0.5423532436382212, learning rate: 0.00011994000000000001\n",
      "Epoch 4004, Loss: 0.5423522542396692, learning rate: 0.00011992\n",
      "Epoch 4005, Loss: 0.5423512665204726, learning rate: 0.0001199\n",
      "Epoch 4006, Loss: 0.5423502805927555, learning rate: 0.00011988000000000001\n",
      "Epoch 4007, Loss: 0.5423492899045105, learning rate: 0.00011986000000000001\n",
      "Epoch 4008, Loss: 0.5423483028055581, learning rate: 0.00011984\n",
      "Epoch 4009, Loss: 0.5423473163381314, learning rate: 0.00011982\n",
      "Epoch 4010, Loss: 0.5423463308489526, learning rate: 0.00011980000000000001\n",
      "Epoch 4011, Loss: 0.5423453400970444, learning rate: 0.00011978000000000001\n",
      "Epoch 4012, Loss: 0.5423443529784882, learning rate: 0.00011976\n",
      "Epoch 4013, Loss: 0.5423433726600583, learning rate: 0.00011974\n",
      "Epoch 4014, Loss: 0.5423423792422852, learning rate: 0.00011972\n",
      "Epoch 4015, Loss: 0.5423413916084587, learning rate: 0.00011970000000000001\n",
      "Epoch 4016, Loss: 0.5423404090382047, learning rate: 0.00011968\n",
      "Epoch 4017, Loss: 0.5423394190316333, learning rate: 0.00011966\n",
      "Epoch 4018, Loss: 0.542338432619926, learning rate: 0.00011964\n",
      "Epoch 4019, Loss: 0.5423374482159486, learning rate: 0.00011962\n",
      "Epoch 4020, Loss: 0.5423364612130216, learning rate: 0.00011960000000000001\n",
      "Epoch 4021, Loss: 0.5423354803946805, learning rate: 0.00011958\n",
      "Epoch 4022, Loss: 0.5423344895865043, learning rate: 0.00011956000000000001\n",
      "Epoch 4023, Loss: 0.5423335051109233, learning rate: 0.00011954\n",
      "Epoch 4024, Loss: 0.5423325211563681, learning rate: 0.00011952000000000001\n",
      "Epoch 4025, Loss: 0.5423315354835265, learning rate: 0.0001195\n",
      "Epoch 4026, Loss: 0.5423305510386713, learning rate: 0.00011948\n",
      "Epoch 4027, Loss: 0.542329564788039, learning rate: 0.00011946000000000001\n",
      "Epoch 4028, Loss: 0.5423285808598902, learning rate: 0.00011944\n",
      "Epoch 4029, Loss: 0.5423275990616598, learning rate: 0.00011942\n",
      "Epoch 4030, Loss: 0.5423266127719117, learning rate: 0.0001194\n",
      "Epoch 4031, Loss: 0.542325629264402, learning rate: 0.00011938000000000001\n",
      "Epoch 4032, Loss: 0.542324643994321, learning rate: 0.00011936000000000001\n",
      "Epoch 4033, Loss: 0.5423236652693169, learning rate: 0.00011934\n",
      "Epoch 4034, Loss: 0.5423226775796594, learning rate: 0.00011932\n",
      "Epoch 4035, Loss: 0.5423216935656268, learning rate: 0.0001193\n",
      "Epoch 4036, Loss: 0.5423207124276512, learning rate: 0.00011928000000000001\n",
      "Epoch 4037, Loss: 0.5423197308377262, learning rate: 0.00011926\n",
      "Epoch 4038, Loss: 0.54231874579837, learning rate: 0.00011924000000000002\n",
      "Epoch 4039, Loss: 0.5423177637506683, learning rate: 0.00011922\n",
      "Epoch 4040, Loss: 0.5423167797111378, learning rate: 0.00011920000000000001\n",
      "Epoch 4041, Loss: 0.5423157991544029, learning rate: 0.00011918000000000001\n",
      "Epoch 4042, Loss: 0.5423148178492189, learning rate: 0.00011916\n",
      "Epoch 4043, Loss: 0.5423138360025819, learning rate: 0.00011914000000000001\n",
      "Epoch 4044, Loss: 0.542312851828384, learning rate: 0.00011912\n",
      "Epoch 4045, Loss: 0.5423118701086848, learning rate: 0.00011910000000000001\n",
      "Epoch 4046, Loss: 0.5423108907854812, learning rate: 0.00011908\n",
      "Epoch 4047, Loss: 0.5423099080864582, learning rate: 0.00011906000000000002\n",
      "Epoch 4048, Loss: 0.5423089296914473, learning rate: 0.00011904000000000001\n",
      "Epoch 4049, Loss: 0.5423079454096239, learning rate: 0.00011902\n",
      "Epoch 4050, Loss: 0.5423069715612692, learning rate: 0.000119\n",
      "Epoch 4051, Loss: 0.5423059846117231, learning rate: 0.00011898\n",
      "Epoch 4052, Loss: 0.5423050039424809, learning rate: 0.00011896000000000001\n",
      "Epoch 4053, Loss: 0.5423040252739609, learning rate: 0.00011894000000000001\n",
      "Epoch 4054, Loss: 0.542303047601095, learning rate: 0.00011892\n",
      "Epoch 4055, Loss: 0.5423020683330201, learning rate: 0.0001189\n",
      "Epoch 4056, Loss: 0.5423010837147217, learning rate: 0.00011888\n",
      "Epoch 4057, Loss: 0.5423001065609218, learning rate: 0.00011886000000000001\n",
      "Epoch 4058, Loss: 0.5422991279311457, learning rate: 0.00011884000000000001\n",
      "Epoch 4059, Loss: 0.5422981478678238, learning rate: 0.00011882\n",
      "Epoch 4060, Loss: 0.5422971685480187, learning rate: 0.0001188\n",
      "Epoch 4061, Loss: 0.5422961900628902, learning rate: 0.00011878000000000001\n",
      "Epoch 4062, Loss: 0.5422952124949774, learning rate: 0.00011876000000000001\n",
      "Epoch 4063, Loss: 0.5422942317464154, learning rate: 0.00011874\n",
      "Epoch 4064, Loss: 0.5422932549302666, learning rate: 0.00011872\n",
      "Epoch 4065, Loss: 0.5422922772025368, learning rate: 0.0001187\n",
      "Epoch 4066, Loss: 0.5422912983799657, learning rate: 0.00011868000000000001\n",
      "Epoch 4067, Loss: 0.5422903219351178, learning rate: 0.00011866\n",
      "Epoch 4068, Loss: 0.5422893414584683, learning rate: 0.00011864000000000002\n",
      "Epoch 4069, Loss: 0.5422883636118105, learning rate: 0.00011862\n",
      "Epoch 4070, Loss: 0.5422873884276381, learning rate: 0.00011860000000000001\n",
      "Epoch 4071, Loss: 0.5422864095924509, learning rate: 0.00011858\n",
      "Epoch 4072, Loss: 0.54228543242526, learning rate: 0.00011856\n",
      "Epoch 4073, Loss: 0.5422844556471247, learning rate: 0.00011854000000000001\n",
      "Epoch 4074, Loss: 0.5422834794277661, learning rate: 0.00011852\n",
      "Epoch 4075, Loss: 0.5422825092262681, learning rate: 0.00011850000000000001\n",
      "Epoch 4076, Loss: 0.542281525899322, learning rate: 0.00011848\n",
      "Epoch 4077, Loss: 0.5422805502796102, learning rate: 0.00011846000000000001\n",
      "Epoch 4078, Loss: 0.5422795757310301, learning rate: 0.00011844000000000001\n",
      "Epoch 4079, Loss: 0.5422785975953105, learning rate: 0.00011842000000000001\n",
      "Epoch 4080, Loss: 0.542277624868827, learning rate: 0.0001184\n",
      "Epoch 4081, Loss: 0.542276647793006, learning rate: 0.00011838\n",
      "Epoch 4082, Loss: 0.542275673616426, learning rate: 0.00011836000000000001\n",
      "Epoch 4083, Loss: 0.5422747009575882, learning rate: 0.00011834000000000001\n",
      "Epoch 4084, Loss: 0.5422737223931698, learning rate: 0.00011832\n",
      "Epoch 4085, Loss: 0.5422727475951623, learning rate: 0.0001183\n",
      "Epoch 4086, Loss: 0.5422717717384207, learning rate: 0.00011828\n",
      "Epoch 4087, Loss: 0.5422708012998423, learning rate: 0.00011826000000000001\n",
      "Epoch 4088, Loss: 0.5422698244711753, learning rate: 0.00011824\n",
      "Epoch 4089, Loss: 0.542268853518675, learning rate: 0.00011822\n",
      "Epoch 4090, Loss: 0.5422678755461056, learning rate: 0.0001182\n",
      "Epoch 4091, Loss: 0.542266902443686, learning rate: 0.00011818000000000001\n",
      "Epoch 4092, Loss: 0.5422659316876023, learning rate: 0.00011816000000000001\n",
      "Epoch 4093, Loss: 0.5422649568803879, learning rate: 0.00011814\n",
      "Epoch 4094, Loss: 0.5422639869319356, learning rate: 0.00011812\n",
      "Epoch 4095, Loss: 0.5422630096961774, learning rate: 0.0001181\n",
      "Epoch 4096, Loss: 0.5422620372056364, learning rate: 0.00011808000000000001\n",
      "Epoch 4097, Loss: 0.5422610666687018, learning rate: 0.00011806\n",
      "Epoch 4098, Loss: 0.542260098146939, learning rate: 0.00011804000000000002\n",
      "Epoch 4099, Loss: 0.5422591197520638, learning rate: 0.00011802\n",
      "Epoch 4100, Loss: 0.5422581478640229, learning rate: 0.00011800000000000001\n",
      "Epoch 4101, Loss: 0.5422571740270806, learning rate: 0.00011798\n",
      "Epoch 4102, Loss: 0.5422562045020687, learning rate: 0.00011796\n",
      "Epoch 4103, Loss: 0.5422552375814045, learning rate: 0.00011794000000000001\n",
      "Epoch 4104, Loss: 0.5422542611184027, learning rate: 0.00011792\n",
      "Epoch 4105, Loss: 0.5422532892034208, learning rate: 0.0001179\n",
      "Epoch 4106, Loss: 0.542252319146334, learning rate: 0.00011788\n",
      "Epoch 4107, Loss: 0.5422513505902944, learning rate: 0.00011786000000000001\n",
      "Epoch 4108, Loss: 0.5422503782109264, learning rate: 0.00011784000000000001\n",
      "Epoch 4109, Loss: 0.5422494075561333, learning rate: 0.00011782000000000001\n",
      "Epoch 4110, Loss: 0.542248436454389, learning rate: 0.0001178\n",
      "Epoch 4111, Loss: 0.5422474670750385, learning rate: 0.00011778\n",
      "Epoch 4112, Loss: 0.5422464953467323, learning rate: 0.00011776000000000001\n",
      "Epoch 4113, Loss: 0.5422455279584395, learning rate: 0.00011774000000000001\n",
      "Epoch 4114, Loss: 0.5422445577028142, learning rate: 0.00011772\n",
      "Epoch 4115, Loss: 0.5422435890700692, learning rate: 0.0001177\n",
      "Epoch 4116, Loss: 0.5422426162806427, learning rate: 0.00011768000000000001\n",
      "Epoch 4117, Loss: 0.542241649012225, learning rate: 0.00011766000000000001\n",
      "Epoch 4118, Loss: 0.5422406831886487, learning rate: 0.00011764\n",
      "Epoch 4119, Loss: 0.5422397114702108, learning rate: 0.00011762\n",
      "Epoch 4120, Loss: 0.542238740749601, learning rate: 0.0001176\n",
      "Epoch 4121, Loss: 0.5422377733914797, learning rate: 0.00011758000000000001\n",
      "Epoch 4122, Loss: 0.5422368076723478, learning rate: 0.00011756\n",
      "Epoch 4123, Loss: 0.5422358426024099, learning rate: 0.00011754\n",
      "Epoch 4124, Loss: 0.5422348687049141, learning rate: 0.00011752\n",
      "Epoch 4125, Loss: 0.5422339016172177, learning rate: 0.0001175\n",
      "Epoch 4126, Loss: 0.5422329346916245, learning rate: 0.00011748000000000001\n",
      "Epoch 4127, Loss: 0.5422319653257973, learning rate: 0.00011746\n",
      "Epoch 4128, Loss: 0.5422309979520364, learning rate: 0.00011744000000000001\n",
      "Epoch 4129, Loss: 0.5422300326308801, learning rate: 0.00011742\n",
      "Epoch 4130, Loss: 0.5422290719758508, learning rate: 0.00011740000000000001\n",
      "Epoch 4131, Loss: 0.5422280981868896, learning rate: 0.00011738\n",
      "Epoch 4132, Loss: 0.5422271313104113, learning rate: 0.00011736\n",
      "Epoch 4133, Loss: 0.5422261668096245, learning rate: 0.00011734000000000001\n",
      "Epoch 4134, Loss: 0.542225202711313, learning rate: 0.00011732\n",
      "Epoch 4135, Loss: 0.5422242330538147, learning rate: 0.0001173\n",
      "Epoch 4136, Loss: 0.5422232657620089, learning rate: 0.00011728\n",
      "Epoch 4137, Loss: 0.5422223032900779, learning rate: 0.00011726000000000001\n",
      "Epoch 4138, Loss: 0.5422213405016583, learning rate: 0.00011724000000000001\n",
      "Epoch 4139, Loss: 0.5422203704067585, learning rate: 0.00011722\n",
      "Epoch 4140, Loss: 0.5422194041267725, learning rate: 0.0001172\n",
      "Epoch 4141, Loss: 0.5422184403116105, learning rate: 0.00011718\n",
      "Epoch 4142, Loss: 0.5422174782077626, learning rate: 0.00011716000000000001\n",
      "Epoch 4143, Loss: 0.5422165104632, learning rate: 0.00011714\n",
      "Epoch 4144, Loss: 0.5422155445369036, learning rate: 0.00011712\n",
      "Epoch 4145, Loss: 0.5422145806083699, learning rate: 0.0001171\n",
      "Epoch 4146, Loss: 0.5422136206046937, learning rate: 0.00011708000000000001\n",
      "Epoch 4147, Loss: 0.5422126533373931, learning rate: 0.00011706000000000001\n",
      "Epoch 4148, Loss: 0.5422116891322277, learning rate: 0.00011704\n",
      "Epoch 4149, Loss: 0.5422107283415472, learning rate: 0.00011702\n",
      "Epoch 4150, Loss: 0.5422097613166209, learning rate: 0.000117\n",
      "Epoch 4151, Loss: 0.5422087982290374, learning rate: 0.00011698000000000001\n",
      "Epoch 4152, Loss: 0.5422078371982221, learning rate: 0.00011696\n",
      "Epoch 4153, Loss: 0.542206872649341, learning rate: 0.00011694000000000002\n",
      "Epoch 4154, Loss: 0.542205912000395, learning rate: 0.00011692\n",
      "Epoch 4155, Loss: 0.5422049514492597, learning rate: 0.00011690000000000001\n",
      "Epoch 4156, Loss: 0.542203984942313, learning rate: 0.00011688\n",
      "Epoch 4157, Loss: 0.5422030215627867, learning rate: 0.00011686\n",
      "Epoch 4158, Loss: 0.5422020632047999, learning rate: 0.00011684000000000001\n",
      "Epoch 4159, Loss: 0.5422011048803475, learning rate: 0.00011682\n",
      "Epoch 4160, Loss: 0.542200138347432, learning rate: 0.0001168\n",
      "Epoch 4161, Loss: 0.5421991751213473, learning rate: 0.00011678\n",
      "Epoch 4162, Loss: 0.5421982132479986, learning rate: 0.00011676\n",
      "Epoch 4163, Loss: 0.5421972561800034, learning rate: 0.00011674000000000001\n",
      "Epoch 4164, Loss: 0.5421962929997315, learning rate: 0.00011672000000000001\n",
      "Epoch 4165, Loss: 0.5421953307963518, learning rate: 0.0001167\n",
      "Epoch 4166, Loss: 0.5421943725805051, learning rate: 0.00011668\n",
      "Epoch 4167, Loss: 0.5421934116325939, learning rate: 0.00011666000000000001\n",
      "Epoch 4168, Loss: 0.5421924517576402, learning rate: 0.00011664000000000001\n",
      "Epoch 4169, Loss: 0.5421914895058604, learning rate: 0.00011662\n",
      "Epoch 4170, Loss: 0.5421905329921667, learning rate: 0.0001166\n",
      "Epoch 4171, Loss: 0.5421895696876126, learning rate: 0.00011658\n",
      "Epoch 4172, Loss: 0.5421886131058322, learning rate: 0.00011656000000000001\n",
      "Epoch 4173, Loss: 0.5421876501661561, learning rate: 0.00011654\n",
      "Epoch 4174, Loss: 0.5421866915483676, learning rate: 0.00011652000000000002\n",
      "Epoch 4175, Loss: 0.542185735209416, learning rate: 0.0001165\n",
      "Epoch 4176, Loss: 0.5421847792384739, learning rate: 0.00011648000000000001\n",
      "Epoch 4177, Loss: 0.5421838169361292, learning rate: 0.00011646\n",
      "Epoch 4178, Loss: 0.5421828562640079, learning rate: 0.00011644\n",
      "Epoch 4179, Loss: 0.5421818976503641, learning rate: 0.00011642000000000001\n",
      "Epoch 4180, Loss: 0.5421809419969811, learning rate: 0.0001164\n",
      "Epoch 4181, Loss: 0.5421799815031639, learning rate: 0.00011638000000000001\n",
      "Epoch 4182, Loss: 0.5421790231371796, learning rate: 0.00011636\n",
      "Epoch 4183, Loss: 0.5421780671563046, learning rate: 0.00011634000000000001\n",
      "Epoch 4184, Loss: 0.542177108347679, learning rate: 0.00011632000000000001\n",
      "Epoch 4185, Loss: 0.542176152373524, learning rate: 0.00011630000000000001\n",
      "Epoch 4186, Loss: 0.542175193653579, learning rate: 0.00011628\n",
      "Epoch 4187, Loss: 0.5421742389404927, learning rate: 0.00011626\n",
      "Epoch 4188, Loss: 0.542173279933279, learning rate: 0.00011624000000000001\n",
      "Epoch 4189, Loss: 0.5421723219239678, learning rate: 0.00011622000000000001\n",
      "Epoch 4190, Loss: 0.5421713656390086, learning rate: 0.0001162\n",
      "Epoch 4191, Loss: 0.5421704109618871, learning rate: 0.00011618\n",
      "Epoch 4192, Loss: 0.5421694527695454, learning rate: 0.00011616000000000001\n",
      "Epoch 4193, Loss: 0.5421684987111884, learning rate: 0.00011614000000000001\n",
      "Epoch 4194, Loss: 0.5421675415885876, learning rate: 0.00011612\n",
      "Epoch 4195, Loss: 0.5421665849281524, learning rate: 0.0001161\n",
      "Epoch 4196, Loss: 0.5421656337359921, learning rate: 0.00011608\n",
      "Epoch 4197, Loss: 0.5421646773740811, learning rate: 0.00011606000000000001\n",
      "Epoch 4198, Loss: 0.5421637196737505, learning rate: 0.00011604000000000001\n",
      "Epoch 4199, Loss: 0.5421627640045902, learning rate: 0.00011602\n",
      "Epoch 4200, Loss: 0.5421618100049519, learning rate: 0.000116\n",
      "Epoch 4201, Loss: 0.5421608533893085, learning rate: 0.00011598\n",
      "Epoch 4202, Loss: 0.5421599025562092, learning rate: 0.00011596000000000001\n",
      "Epoch 4203, Loss: 0.5421589484903291, learning rate: 0.00011594\n",
      "Epoch 4204, Loss: 0.5421579923593498, learning rate: 0.00011592000000000001\n",
      "Epoch 4205, Loss: 0.5421570372160357, learning rate: 0.0001159\n",
      "Epoch 4206, Loss: 0.5421560842493833, learning rate: 0.00011588000000000001\n",
      "Epoch 4207, Loss: 0.5421551299831632, learning rate: 0.00011586\n",
      "Epoch 4208, Loss: 0.5421541768254656, learning rate: 0.00011584\n",
      "Epoch 4209, Loss: 0.5421532220181978, learning rate: 0.00011582000000000001\n",
      "Epoch 4210, Loss: 0.5421522701105105, learning rate: 0.0001158\n",
      "Epoch 4211, Loss: 0.5421513190545393, learning rate: 0.00011578\n",
      "Epoch 4212, Loss: 0.5421503702716035, learning rate: 0.00011576\n",
      "Epoch 4213, Loss: 0.5421494120732596, learning rate: 0.00011574000000000001\n",
      "Epoch 4214, Loss: 0.5421484589373486, learning rate: 0.00011572000000000001\n",
      "Epoch 4215, Loss: 0.5421475081083987, learning rate: 0.00011570000000000001\n",
      "Epoch 4216, Loss: 0.542146556790553, learning rate: 0.00011568\n",
      "Epoch 4217, Loss: 0.5421456032580957, learning rate: 0.00011566\n",
      "Epoch 4218, Loss: 0.5421446511272858, learning rate: 0.00011564000000000001\n",
      "Epoch 4219, Loss: 0.5421436989826294, learning rate: 0.00011562000000000001\n",
      "Epoch 4220, Loss: 0.5421427484470851, learning rate: 0.0001156\n",
      "Epoch 4221, Loss: 0.5421417982308622, learning rate: 0.00011558\n",
      "Epoch 4222, Loss: 0.5421408488222114, learning rate: 0.00011556000000000001\n",
      "Epoch 4223, Loss: 0.5421398959317212, learning rate: 0.00011554000000000001\n",
      "Epoch 4224, Loss: 0.5421389451952275, learning rate: 0.00011552\n",
      "Epoch 4225, Loss: 0.5421379970127924, learning rate: 0.0001155\n",
      "Epoch 4226, Loss: 0.5421370443085234, learning rate: 0.00011548\n",
      "Epoch 4227, Loss: 0.54213609416441, learning rate: 0.00011546000000000001\n",
      "Epoch 4228, Loss: 0.5421351454183886, learning rate: 0.00011544\n",
      "Epoch 4229, Loss: 0.5421341973044922, learning rate: 0.00011542000000000002\n",
      "Epoch 4230, Loss: 0.5421332441188529, learning rate: 0.0001154\n",
      "Epoch 4231, Loss: 0.5421322955443685, learning rate: 0.00011538000000000001\n",
      "Epoch 4232, Loss: 0.5421313462095534, learning rate: 0.00011536000000000001\n",
      "Epoch 4233, Loss: 0.5421304000761027, learning rate: 0.00011534\n",
      "Epoch 4234, Loss: 0.5421294481294765, learning rate: 0.00011532000000000001\n",
      "Epoch 4235, Loss: 0.5421284993999742, learning rate: 0.0001153\n",
      "Epoch 4236, Loss: 0.5421275514721534, learning rate: 0.00011528000000000001\n",
      "Epoch 4237, Loss: 0.5421266031964085, learning rate: 0.00011526\n",
      "Epoch 4238, Loss: 0.5421256537324843, learning rate: 0.00011524\n",
      "Epoch 4239, Loss: 0.5421247051691183, learning rate: 0.00011522000000000001\n",
      "Epoch 4240, Loss: 0.5421237586588626, learning rate: 0.0001152\n",
      "Epoch 4241, Loss: 0.5421228096578075, learning rate: 0.00011518\n",
      "Epoch 4242, Loss: 0.5421218626277133, learning rate: 0.00011516\n",
      "Epoch 4243, Loss: 0.542120916740215, learning rate: 0.00011514000000000001\n",
      "Epoch 4244, Loss: 0.5421199685125341, learning rate: 0.00011512000000000001\n",
      "Epoch 4245, Loss: 0.5421190207633626, learning rate: 0.0001151\n",
      "Epoch 4246, Loss: 0.5421180739278862, learning rate: 0.00011508\n",
      "Epoch 4247, Loss: 0.542117128167357, learning rate: 0.00011506\n",
      "Epoch 4248, Loss: 0.5421161852154585, learning rate: 0.00011504000000000001\n",
      "Epoch 4249, Loss: 0.5421152351785186, learning rate: 0.00011502\n",
      "Epoch 4250, Loss: 0.5421142880248765, learning rate: 0.000115\n",
      "Epoch 4251, Loss: 0.5421133429099365, learning rate: 0.00011498\n",
      "Epoch 4252, Loss: 0.5421123968706287, learning rate: 0.00011496000000000001\n",
      "Epoch 4253, Loss: 0.5421114505627692, learning rate: 0.00011494000000000001\n",
      "Epoch 4254, Loss: 0.5421105045405175, learning rate: 0.00011492\n",
      "Epoch 4255, Loss: 0.5421095612287514, learning rate: 0.0001149\n",
      "Epoch 4256, Loss: 0.5421086142847459, learning rate: 0.00011488\n",
      "Epoch 4257, Loss: 0.5421076708920406, learning rate: 0.00011486000000000001\n",
      "Epoch 4258, Loss: 0.5421067251110754, learning rate: 0.00011484\n",
      "Epoch 4259, Loss: 0.5421057795138251, learning rate: 0.00011482000000000002\n",
      "Epoch 4260, Loss: 0.54210483571614, learning rate: 0.0001148\n",
      "Epoch 4261, Loss: 0.5421038931295069, learning rate: 0.00011478000000000001\n",
      "Epoch 4262, Loss: 0.5421029486957443, learning rate: 0.00011476\n",
      "Epoch 4263, Loss: 0.5421020027978131, learning rate: 0.00011474\n",
      "Epoch 4264, Loss: 0.5421010579454623, learning rate: 0.00011472000000000001\n",
      "Epoch 4265, Loss: 0.5421001161245069, learning rate: 0.0001147\n",
      "Epoch 4266, Loss: 0.5420991731778251, learning rate: 0.00011468\n",
      "Epoch 4267, Loss: 0.5420982297521986, learning rate: 0.00011466\n",
      "Epoch 4268, Loss: 0.5420972859135448, learning rate: 0.00011464000000000001\n",
      "Epoch 4269, Loss: 0.5420963417306135, learning rate: 0.00011462000000000001\n",
      "Epoch 4270, Loss: 0.5420953993061264, learning rate: 0.0001146\n",
      "Epoch 4271, Loss: 0.542094458483522, learning rate: 0.00011458\n",
      "Epoch 4272, Loss: 0.5420935156357891, learning rate: 0.00011456\n",
      "Epoch 4273, Loss: 0.5420925717687278, learning rate: 0.00011454000000000001\n",
      "Epoch 4274, Loss: 0.5420916299209875, learning rate: 0.00011452000000000001\n",
      "Epoch 4275, Loss: 0.5420906884902558, learning rate: 0.0001145\n",
      "Epoch 4276, Loss: 0.5420897471187245, learning rate: 0.00011448\n",
      "Epoch 4277, Loss: 0.5420888081260674, learning rate: 0.00011446\n",
      "Epoch 4278, Loss: 0.5420878637118365, learning rate: 0.00011444000000000001\n",
      "Epoch 4279, Loss: 0.5420869223139727, learning rate: 0.00011442\n",
      "Epoch 4280, Loss: 0.5420859825019674, learning rate: 0.0001144\n",
      "Epoch 4281, Loss: 0.5420850405973782, learning rate: 0.00011438\n",
      "Epoch 4282, Loss: 0.5420841011140158, learning rate: 0.00011436000000000001\n",
      "Epoch 4283, Loss: 0.54208315923231, learning rate: 0.00011434\n",
      "Epoch 4284, Loss: 0.5420822211735496, learning rate: 0.00011432\n",
      "Epoch 4285, Loss: 0.5420812824681985, learning rate: 0.0001143\n",
      "Epoch 4286, Loss: 0.5420803404500389, learning rate: 0.00011428\n",
      "Epoch 4287, Loss: 0.5420793996621985, learning rate: 0.00011426000000000001\n",
      "Epoch 4288, Loss: 0.5420784604195017, learning rate: 0.00011424\n",
      "Epoch 4289, Loss: 0.5420775205898631, learning rate: 0.00011422000000000001\n",
      "Epoch 4290, Loss: 0.5420765803205619, learning rate: 0.0001142\n",
      "Epoch 4291, Loss: 0.5420756428522581, learning rate: 0.00011418000000000001\n",
      "Epoch 4292, Loss: 0.54207470262942, learning rate: 0.00011416\n",
      "Epoch 4293, Loss: 0.5420737632832042, learning rate: 0.00011414\n",
      "Epoch 4294, Loss: 0.5420728270048693, learning rate: 0.00011412000000000001\n",
      "Epoch 4295, Loss: 0.5420718865651555, learning rate: 0.00011410000000000001\n",
      "Epoch 4296, Loss: 0.5420709478320374, learning rate: 0.00011408\n",
      "Epoch 4297, Loss: 0.5420700098194406, learning rate: 0.00011406\n",
      "Epoch 4298, Loss: 0.5420690707314928, learning rate: 0.00011404000000000001\n",
      "Epoch 4299, Loss: 0.542068132625531, learning rate: 0.00011402000000000001\n",
      "Epoch 4300, Loss: 0.5420671965441229, learning rate: 0.000114\n",
      "Epoch 4301, Loss: 0.5420662610549056, learning rate: 0.00011398\n",
      "Epoch 4302, Loss: 0.5420653202640191, learning rate: 0.00011396\n",
      "Epoch 4303, Loss: 0.5420643834729307, learning rate: 0.00011394000000000001\n",
      "Epoch 4304, Loss: 0.5420634453298039, learning rate: 0.00011392000000000001\n",
      "Epoch 4305, Loss: 0.5420625103356145, learning rate: 0.00011390000000000002\n",
      "Epoch 4306, Loss: 0.5420615726048321, learning rate: 0.00011388\n",
      "Epoch 4307, Loss: 0.5420606350475299, learning rate: 0.00011386\n",
      "Epoch 4308, Loss: 0.5420597013874104, learning rate: 0.00011384000000000001\n",
      "Epoch 4309, Loss: 0.5420587663588641, learning rate: 0.00011382\n",
      "Epoch 4310, Loss: 0.5420578277327894, learning rate: 0.00011380000000000001\n",
      "Epoch 4311, Loss: 0.542056893274633, learning rate: 0.00011378\n",
      "Epoch 4312, Loss: 0.5420559566501187, learning rate: 0.00011376000000000001\n",
      "Epoch 4313, Loss: 0.5420550203447398, learning rate: 0.00011374\n",
      "Epoch 4314, Loss: 0.5420540846255921, learning rate: 0.00011372\n",
      "Epoch 4315, Loss: 0.5420531489299352, learning rate: 0.00011370000000000001\n",
      "Epoch 4316, Loss: 0.5420522148892905, learning rate: 0.00011368\n",
      "Epoch 4317, Loss: 0.5420512796324478, learning rate: 0.00011366\n",
      "Epoch 4318, Loss: 0.5420503443800888, learning rate: 0.00011364\n",
      "Epoch 4319, Loss: 0.5420494106062883, learning rate: 0.00011362000000000001\n",
      "Epoch 4320, Loss: 0.5420484757267608, learning rate: 0.00011360000000000001\n",
      "Epoch 4321, Loss: 0.5420475403886541, learning rate: 0.00011358000000000001\n",
      "Epoch 4322, Loss: 0.5420466070984464, learning rate: 0.00011356\n",
      "Epoch 4323, Loss: 0.5420456747369841, learning rate: 0.00011354\n",
      "Epoch 4324, Loss: 0.542044739533021, learning rate: 0.00011352000000000001\n",
      "Epoch 4325, Loss: 0.5420438056878055, learning rate: 0.00011350000000000001\n",
      "Epoch 4326, Loss: 0.542042871741997, learning rate: 0.00011348\n",
      "Epoch 4327, Loss: 0.5420419394784642, learning rate: 0.00011346\n",
      "Epoch 4328, Loss: 0.5420410079292965, learning rate: 0.00011344000000000001\n",
      "Epoch 4329, Loss: 0.5420400738230523, learning rate: 0.00011342000000000001\n",
      "Epoch 4330, Loss: 0.5420391406547794, learning rate: 0.0001134\n",
      "Epoch 4331, Loss: 0.5420382091182461, learning rate: 0.00011338\n",
      "Epoch 4332, Loss: 0.542037277537115, learning rate: 0.00011336\n",
      "Epoch 4333, Loss: 0.5420363433513735, learning rate: 0.00011334000000000001\n",
      "Epoch 4334, Loss: 0.5420354107743567, learning rate: 0.00011332\n",
      "Epoch 4335, Loss: 0.5420344785393775, learning rate: 0.00011330000000000002\n",
      "Epoch 4336, Loss: 0.5420335463811162, learning rate: 0.00011328\n",
      "Epoch 4337, Loss: 0.5420326173468983, learning rate: 0.00011326000000000001\n",
      "Epoch 4338, Loss: 0.542031685354022, learning rate: 0.00011324000000000001\n",
      "Epoch 4339, Loss: 0.5420307540592751, learning rate: 0.00011322\n",
      "Epoch 4340, Loss: 0.5420298216731243, learning rate: 0.00011320000000000001\n",
      "Epoch 4341, Loss: 0.5420288920199945, learning rate: 0.00011318\n",
      "Epoch 4342, Loss: 0.5420279613744037, learning rate: 0.00011316000000000001\n",
      "Epoch 4343, Loss: 0.5420270299338301, learning rate: 0.00011314\n",
      "Epoch 4344, Loss: 0.5420261000619323, learning rate: 0.00011312\n",
      "Epoch 4345, Loss: 0.5420251720525107, learning rate: 0.00011310000000000001\n",
      "Epoch 4346, Loss: 0.542024240316481, learning rate: 0.00011308\n",
      "Epoch 4347, Loss: 0.5420233112192108, learning rate: 0.00011306\n",
      "Epoch 4348, Loss: 0.5420223803577491, learning rate: 0.00011304\n",
      "Epoch 4349, Loss: 0.5420214514230138, learning rate: 0.00011302000000000001\n",
      "Epoch 4350, Loss: 0.542020520860431, learning rate: 0.00011300000000000001\n",
      "Epoch 4351, Loss: 0.5420195922023054, learning rate: 0.00011298\n",
      "Epoch 4352, Loss: 0.5420186646765208, learning rate: 0.00011296\n",
      "Epoch 4353, Loss: 0.5420177379605237, learning rate: 0.00011294\n",
      "Epoch 4354, Loss: 0.5420168073082113, learning rate: 0.00011292000000000001\n",
      "Epoch 4355, Loss: 0.542015877473604, learning rate: 0.0001129\n",
      "Epoch 4356, Loss: 0.5420149491230742, learning rate: 0.00011288\n",
      "Epoch 4357, Loss: 0.5420140229516298, learning rate: 0.00011286\n",
      "Epoch 4358, Loss: 0.5420130948078774, learning rate: 0.00011284000000000001\n",
      "Epoch 4359, Loss: 0.5420121662623832, learning rate: 0.00011282000000000001\n",
      "Epoch 4360, Loss: 0.5420112379874046, learning rate: 0.0001128\n",
      "Epoch 4361, Loss: 0.5420103106557677, learning rate: 0.00011278\n",
      "Epoch 4362, Loss: 0.5420093838603107, learning rate: 0.00011276\n",
      "Epoch 4363, Loss: 0.5420084572705633, learning rate: 0.00011274000000000001\n",
      "Epoch 4364, Loss: 0.542007530611247, learning rate: 0.00011272\n",
      "Epoch 4365, Loss: 0.5420066022692701, learning rate: 0.00011270000000000002\n",
      "Epoch 4366, Loss: 0.5420056774006384, learning rate: 0.00011268\n",
      "Epoch 4367, Loss: 0.5420047499625008, learning rate: 0.00011266000000000001\n",
      "Epoch 4368, Loss: 0.5420038228355389, learning rate: 0.00011264\n",
      "Epoch 4369, Loss: 0.5420028975493036, learning rate: 0.00011262\n",
      "Epoch 4370, Loss: 0.5420019741026029, learning rate: 0.00011260000000000001\n",
      "Epoch 4371, Loss: 0.542001045621961, learning rate: 0.00011258\n",
      "Epoch 4372, Loss: 0.542000119756079, learning rate: 0.00011256\n",
      "Epoch 4373, Loss: 0.5419991956668465, learning rate: 0.00011254\n",
      "Epoch 4374, Loss: 0.5419982687992342, learning rate: 0.00011252000000000001\n",
      "Epoch 4375, Loss: 0.5419973439018079, learning rate: 0.00011250000000000001\n",
      "Epoch 4376, Loss: 0.541996417811757, learning rate: 0.00011248000000000001\n",
      "Epoch 4377, Loss: 0.5419954903128074, learning rate: 0.00011246\n",
      "Epoch 4378, Loss: 0.5419945565783423, learning rate: 0.00011244\n",
      "Epoch 4379, Loss: 0.5419936243495347, learning rate: 0.00011242000000000001\n",
      "Epoch 4380, Loss: 0.5419926954594634, learning rate: 0.00011240000000000001\n",
      "Epoch 4381, Loss: 0.5419917614800135, learning rate: 0.00011238\n",
      "Epoch 4382, Loss: 0.5419908365427175, learning rate: 0.00011236\n",
      "Epoch 4383, Loss: 0.541989902407299, learning rate: 0.00011234\n",
      "Epoch 4384, Loss: 0.5419889758585887, learning rate: 0.00011232000000000001\n",
      "Epoch 4385, Loss: 0.5419880421311506, learning rate: 0.0001123\n",
      "Epoch 4386, Loss: 0.5419871148071593, learning rate: 0.00011228\n",
      "Epoch 4387, Loss: 0.5419861850771365, learning rate: 0.00011226\n",
      "Epoch 4388, Loss: 0.5419852552093737, learning rate: 0.00011224000000000001\n",
      "Epoch 4389, Loss: 0.5419843304132828, learning rate: 0.00011222\n",
      "Epoch 4390, Loss: 0.5419833986431153, learning rate: 0.0001122\n",
      "Epoch 4391, Loss: 0.5419824703828129, learning rate: 0.00011218\n",
      "Epoch 4392, Loss: 0.5419815408407938, learning rate: 0.00011216\n",
      "Epoch 4393, Loss: 0.541980611918811, learning rate: 0.00011214000000000001\n",
      "Epoch 4394, Loss: 0.5419796913852245, learning rate: 0.00011212\n",
      "Epoch 4395, Loss: 0.5419787576433487, learning rate: 0.00011210000000000001\n",
      "Epoch 4396, Loss: 0.5419778279862244, learning rate: 0.00011208\n",
      "Epoch 4397, Loss: 0.5419769070090668, learning rate: 0.00011206000000000001\n",
      "Epoch 4398, Loss: 0.5419759761505027, learning rate: 0.00011204\n",
      "Epoch 4399, Loss: 0.5419750483774975, learning rate: 0.00011202\n",
      "Epoch 4400, Loss: 0.5419741196210326, learning rate: 0.00011200000000000001\n",
      "Epoch 4401, Loss: 0.5419731931306755, learning rate: 0.00011198\n",
      "Epoch 4402, Loss: 0.5419722663011775, learning rate: 0.00011196\n",
      "Epoch 4403, Loss: 0.5419713406958491, learning rate: 0.00011194\n",
      "Epoch 4404, Loss: 0.5419704141472232, learning rate: 0.00011192000000000001\n",
      "Epoch 4405, Loss: 0.5419694903498662, learning rate: 0.00011190000000000001\n",
      "Epoch 4406, Loss: 0.5419685674131598, learning rate: 0.00011188\n",
      "Epoch 4407, Loss: 0.5419676417701075, learning rate: 0.00011186\n",
      "Epoch 4408, Loss: 0.5419667173191617, learning rate: 0.00011184\n",
      "Epoch 4409, Loss: 0.5419657928623759, learning rate: 0.00011182000000000001\n",
      "Epoch 4410, Loss: 0.5419648667143311, learning rate: 0.00011180000000000001\n",
      "Epoch 4411, Loss: 0.5419639423558129, learning rate: 0.00011178\n",
      "Epoch 4412, Loss: 0.5419630196960157, learning rate: 0.00011176\n",
      "Epoch 4413, Loss: 0.5419620974077971, learning rate: 0.00011174000000000001\n",
      "Epoch 4414, Loss: 0.5419611767550885, learning rate: 0.00011172000000000001\n",
      "Epoch 4415, Loss: 0.5419602497125432, learning rate: 0.0001117\n",
      "Epoch 4416, Loss: 0.541959326407978, learning rate: 0.00011168\n",
      "Epoch 4417, Loss: 0.5419584061709543, learning rate: 0.00011166\n",
      "Epoch 4418, Loss: 0.5419574850152339, learning rate: 0.00011164000000000001\n",
      "Epoch 4419, Loss: 0.5419565603169358, learning rate: 0.00011162\n",
      "Epoch 4420, Loss: 0.5419556381393184, learning rate: 0.0001116\n",
      "Epoch 4421, Loss: 0.5419547189707832, learning rate: 0.00011158\n",
      "Epoch 4422, Loss: 0.5419537944767341, learning rate: 0.00011156\n",
      "Epoch 4423, Loss: 0.5419528726242585, learning rate: 0.00011154\n",
      "Epoch 4424, Loss: 0.5419519544735782, learning rate: 0.00011152\n",
      "Epoch 4425, Loss: 0.5419510306009854, learning rate: 0.00011150000000000001\n",
      "Epoch 4426, Loss: 0.5419501101178663, learning rate: 0.00011148\n",
      "Epoch 4427, Loss: 0.5419491883500739, learning rate: 0.00011146000000000001\n",
      "Epoch 4428, Loss: 0.541948267660875, learning rate: 0.00011144\n",
      "Epoch 4429, Loss: 0.5419473513383618, learning rate: 0.00011142\n",
      "Epoch 4430, Loss: 0.5419464278226355, learning rate: 0.00011140000000000001\n",
      "Epoch 4431, Loss: 0.5419455066699832, learning rate: 0.00011138000000000001\n",
      "Epoch 4432, Loss: 0.5419445919148597, learning rate: 0.00011136\n",
      "Epoch 4433, Loss: 0.5419436702524191, learning rate: 0.00011134\n",
      "Epoch 4434, Loss: 0.541942750990158, learning rate: 0.00011132000000000001\n",
      "Epoch 4435, Loss: 0.5419418298724162, learning rate: 0.00011130000000000001\n",
      "Epoch 4436, Loss: 0.5419409096494404, learning rate: 0.00011128\n",
      "Epoch 4437, Loss: 0.5419399950313167, learning rate: 0.00011126\n",
      "Epoch 4438, Loss: 0.5419390754846583, learning rate: 0.00011124\n",
      "Epoch 4439, Loss: 0.5419381551177717, learning rate: 0.00011122000000000001\n",
      "Epoch 4440, Loss: 0.5419372425696335, learning rate: 0.0001112\n",
      "Epoch 4441, Loss: 0.5419363196133983, learning rate: 0.00011118000000000002\n",
      "Epoch 4442, Loss: 0.5419354015547554, learning rate: 0.00011116\n",
      "Epoch 4443, Loss: 0.5419344886848169, learning rate: 0.00011114000000000001\n",
      "Epoch 4444, Loss: 0.5419335684775473, learning rate: 0.00011112\n",
      "Epoch 4445, Loss: 0.5419326487083553, learning rate: 0.0001111\n",
      "Epoch 4446, Loss: 0.541931733219261, learning rate: 0.00011108000000000001\n",
      "Epoch 4447, Loss: 0.5419308157911401, learning rate: 0.00011106\n",
      "Epoch 4448, Loss: 0.5419298982254588, learning rate: 0.00011104000000000001\n",
      "Epoch 4449, Loss: 0.5419289809821839, learning rate: 0.00011102\n",
      "Epoch 4450, Loss: 0.5419280664465894, learning rate: 0.00011100000000000001\n",
      "Epoch 4451, Loss: 0.5419271509482934, learning rate: 0.00011098000000000001\n",
      "Epoch 4452, Loss: 0.541926235306626, learning rate: 0.00011096000000000001\n",
      "Epoch 4453, Loss: 0.5419253160296691, learning rate: 0.00011094\n",
      "Epoch 4454, Loss: 0.5419244008386817, learning rate: 0.00011092\n",
      "Epoch 4455, Loss: 0.5419234846319851, learning rate: 0.00011090000000000001\n",
      "Epoch 4456, Loss: 0.5419225730480767, learning rate: 0.00011088000000000001\n",
      "Epoch 4457, Loss: 0.541921657483964, learning rate: 0.00011086\n",
      "Epoch 4458, Loss: 0.5419207408420704, learning rate: 0.00011084\n",
      "Epoch 4459, Loss: 0.5419198247504257, learning rate: 0.00011082\n",
      "Epoch 4460, Loss: 0.5419189104442795, learning rate: 0.00011080000000000001\n",
      "Epoch 4461, Loss: 0.5419179963046381, learning rate: 0.00011078\n",
      "Epoch 4462, Loss: 0.5419170834361361, learning rate: 0.00011076\n",
      "Epoch 4463, Loss: 0.5419161677420278, learning rate: 0.00011074\n",
      "Epoch 4464, Loss: 0.5419152534085369, learning rate: 0.00011072000000000001\n",
      "Epoch 4465, Loss: 0.5419143427036162, learning rate: 0.00011070000000000001\n",
      "Epoch 4466, Loss: 0.5419134281168878, learning rate: 0.00011068\n",
      "Epoch 4467, Loss: 0.5419125129367881, learning rate: 0.00011066\n",
      "Epoch 4468, Loss: 0.541911605142681, learning rate: 0.00011064\n",
      "Epoch 4469, Loss: 0.5419106881828349, learning rate: 0.00011062000000000001\n",
      "Epoch 4470, Loss: 0.5419097743221671, learning rate: 0.0001106\n",
      "Epoch 4471, Loss: 0.5419088623467077, learning rate: 0.00011058000000000002\n",
      "Epoch 4472, Loss: 0.5419079484273676, learning rate: 0.00011056\n",
      "Epoch 4473, Loss: 0.5419070400475369, learning rate: 0.00011054000000000001\n",
      "Epoch 4474, Loss: 0.5419061269878953, learning rate: 0.00011052\n",
      "Epoch 4475, Loss: 0.541905215346156, learning rate: 0.0001105\n",
      "Epoch 4476, Loss: 0.5419043004254513, learning rate: 0.00011048000000000001\n",
      "Epoch 4477, Loss: 0.5419033927189528, learning rate: 0.00011046\n",
      "Epoch 4478, Loss: 0.5419024798173848, learning rate: 0.00011044\n",
      "Epoch 4479, Loss: 0.5419015666309034, learning rate: 0.00011042\n",
      "Epoch 4480, Loss: 0.5419006550143747, learning rate: 0.00011040000000000001\n",
      "Epoch 4481, Loss: 0.5418997458089653, learning rate: 0.00011038000000000001\n",
      "Epoch 4482, Loss: 0.5418988346533771, learning rate: 0.00011036000000000001\n",
      "Epoch 4483, Loss: 0.5418979248511507, learning rate: 0.00011034\n",
      "Epoch 4484, Loss: 0.5418970114847479, learning rate: 0.00011032\n",
      "Epoch 4485, Loss: 0.5418961013240117, learning rate: 0.00011030000000000001\n",
      "Epoch 4486, Loss: 0.541895190431057, learning rate: 0.00011028000000000001\n",
      "Epoch 4487, Loss: 0.5418942850828325, learning rate: 0.00011026\n",
      "Epoch 4488, Loss: 0.5418933747915822, learning rate: 0.00011024\n",
      "Epoch 4489, Loss: 0.5418924621830363, learning rate: 0.00011022000000000001\n",
      "Epoch 4490, Loss: 0.5418915522030152, learning rate: 0.00011020000000000001\n",
      "Epoch 4491, Loss: 0.541890646521508, learning rate: 0.00011018\n",
      "Epoch 4492, Loss: 0.5418897371072248, learning rate: 0.00011016\n",
      "Epoch 4493, Loss: 0.5418888258320477, learning rate: 0.00011014\n",
      "Epoch 4494, Loss: 0.5418879163775514, learning rate: 0.00011012000000000001\n",
      "Epoch 4495, Loss: 0.5418870085306944, learning rate: 0.0001101\n",
      "Epoch 4496, Loss: 0.5418861024423345, learning rate: 0.00011008\n",
      "Epoch 4497, Loss: 0.5418851940090313, learning rate: 0.00011006\n",
      "Epoch 4498, Loss: 0.5418842831952759, learning rate: 0.00011004\n",
      "Epoch 4499, Loss: 0.5418833758868228, learning rate: 0.00011002000000000001\n",
      "Epoch 4500, Loss: 0.5418824691411904, learning rate: 0.00011\n",
      "Epoch 4501, Loss: 0.5418815598314727, learning rate: 0.00010998000000000001\n",
      "Epoch 4502, Loss: 0.5418806523578984, learning rate: 0.00010996\n",
      "Epoch 4503, Loss: 0.5418797464589291, learning rate: 0.00010994000000000001\n",
      "Epoch 4504, Loss: 0.5418788409646728, learning rate: 0.00010992\n",
      "Epoch 4505, Loss: 0.5418779364508031, learning rate: 0.0001099\n",
      "Epoch 4506, Loss: 0.5418770236172454, learning rate: 0.00010988000000000001\n",
      "Epoch 4507, Loss: 0.5418761175693215, learning rate: 0.00010986\n",
      "Epoch 4508, Loss: 0.5418752118341164, learning rate: 0.00010984\n",
      "Epoch 4509, Loss: 0.5418743072342395, learning rate: 0.00010982\n",
      "Epoch 4510, Loss: 0.5418734018427798, learning rate: 0.00010980000000000001\n",
      "Epoch 4511, Loss: 0.5418724933363372, learning rate: 0.00010978000000000001\n",
      "Epoch 4512, Loss: 0.5418715914332408, learning rate: 0.00010976\n",
      "Epoch 4513, Loss: 0.5418706849015895, learning rate: 0.00010974\n",
      "Epoch 4514, Loss: 0.5418697791287339, learning rate: 0.00010972\n",
      "Epoch 4515, Loss: 0.5418688717889208, learning rate: 0.00010970000000000001\n",
      "Epoch 4516, Loss: 0.5418679670926868, learning rate: 0.00010968000000000001\n",
      "Epoch 4517, Loss: 0.5418670637637503, learning rate: 0.00010966\n",
      "Epoch 4518, Loss: 0.541866160752647, learning rate: 0.00010964\n",
      "Epoch 4519, Loss: 0.5418652516663904, learning rate: 0.00010962000000000001\n",
      "Epoch 4520, Loss: 0.5418643490832704, learning rate: 0.00010960000000000001\n",
      "Epoch 4521, Loss: 0.5418634466481507, learning rate: 0.00010958\n",
      "Epoch 4522, Loss: 0.5418625430735036, learning rate: 0.00010956\n",
      "Epoch 4523, Loss: 0.5418616401340622, learning rate: 0.00010954\n",
      "Epoch 4524, Loss: 0.5418607324930379, learning rate: 0.00010952000000000001\n",
      "Epoch 4525, Loss: 0.5418598310464402, learning rate: 0.0001095\n",
      "Epoch 4526, Loss: 0.5418589292560035, learning rate: 0.00010948000000000002\n",
      "Epoch 4527, Loss: 0.5418580225993618, learning rate: 0.00010946\n",
      "Epoch 4528, Loss: 0.541857121382252, learning rate: 0.00010944\n",
      "Epoch 4529, Loss: 0.5418562160542323, learning rate: 0.00010942\n",
      "Epoch 4530, Loss: 0.5418553183684174, learning rate: 0.0001094\n",
      "Epoch 4531, Loss: 0.5418544123462011, learning rate: 0.00010938000000000001\n",
      "Epoch 4532, Loss: 0.5418535101914685, learning rate: 0.00010936\n",
      "Epoch 4533, Loss: 0.541852608181935, learning rate: 0.00010934000000000001\n",
      "Epoch 4534, Loss: 0.5418517089223758, learning rate: 0.00010932\n",
      "Epoch 4535, Loss: 0.5418508028966, learning rate: 0.0001093\n",
      "Epoch 4536, Loss: 0.5418499001669295, learning rate: 0.00010928000000000001\n",
      "Epoch 4537, Loss: 0.5418489988302635, learning rate: 0.00010926\n",
      "Epoch 4538, Loss: 0.5418481000981127, learning rate: 0.00010924\n",
      "Epoch 4539, Loss: 0.5418471985717835, learning rate: 0.00010922\n",
      "Epoch 4540, Loss: 0.5418462994684958, learning rate: 0.00010920000000000001\n",
      "Epoch 4541, Loss: 0.5418453985326409, learning rate: 0.00010918000000000001\n",
      "Epoch 4542, Loss: 0.5418444952498396, learning rate: 0.00010916\n",
      "Epoch 4543, Loss: 0.5418435978643477, learning rate: 0.00010914\n",
      "Epoch 4544, Loss: 0.5418426935243549, learning rate: 0.00010912\n",
      "Epoch 4545, Loss: 0.5418417936182209, learning rate: 0.00010910000000000001\n",
      "Epoch 4546, Loss: 0.5418408939340696, learning rate: 0.00010908\n",
      "Epoch 4547, Loss: 0.5418399963404631, learning rate: 0.00010906\n",
      "Epoch 4548, Loss: 0.541839100286467, learning rate: 0.00010904\n",
      "Epoch 4549, Loss: 0.5418381954679042, learning rate: 0.00010902000000000001\n",
      "Epoch 4550, Loss: 0.5418372983827439, learning rate: 0.000109\n",
      "Epoch 4551, Loss: 0.5418364006514629, learning rate: 0.00010898\n",
      "Epoch 4552, Loss: 0.5418355010075662, learning rate: 0.00010896\n",
      "Epoch 4553, Loss: 0.5418346009034164, learning rate: 0.00010894\n",
      "Epoch 4554, Loss: 0.5418337034429231, learning rate: 0.00010892000000000001\n",
      "Epoch 4555, Loss: 0.5418328025851886, learning rate: 0.0001089\n",
      "Epoch 4556, Loss: 0.5418319074235008, learning rate: 0.00010888000000000001\n",
      "Epoch 4557, Loss: 0.5418310066864235, learning rate: 0.00010886\n",
      "Epoch 4558, Loss: 0.5418301092317009, learning rate: 0.00010884000000000001\n",
      "Epoch 4559, Loss: 0.5418292132408602, learning rate: 0.00010882\n",
      "Epoch 4560, Loss: 0.5418283153438616, learning rate: 0.0001088\n",
      "Epoch 4561, Loss: 0.5418274205823699, learning rate: 0.00010878000000000001\n",
      "Epoch 4562, Loss: 0.5418265218763715, learning rate: 0.00010876000000000001\n",
      "Epoch 4563, Loss: 0.5418256270167718, learning rate: 0.00010874\n",
      "Epoch 4564, Loss: 0.5418247257384383, learning rate: 0.00010872\n",
      "Epoch 4565, Loss: 0.5418238308251754, learning rate: 0.00010870000000000001\n",
      "Epoch 4566, Loss: 0.5418229330094977, learning rate: 0.00010868000000000001\n",
      "Epoch 4567, Loss: 0.5418220362550554, learning rate: 0.00010866\n",
      "Epoch 4568, Loss: 0.5418211415584095, learning rate: 0.00010864\n",
      "Epoch 4569, Loss: 0.5418202445397063, learning rate: 0.00010862\n",
      "Epoch 4570, Loss: 0.541819349267624, learning rate: 0.00010860000000000001\n",
      "Epoch 4571, Loss: 0.5418184533975307, learning rate: 0.00010858000000000001\n",
      "Epoch 4572, Loss: 0.541817560382703, learning rate: 0.00010856\n",
      "Epoch 4573, Loss: 0.5418166630183445, learning rate: 0.00010854\n",
      "Epoch 4574, Loss: 0.5418157677804383, learning rate: 0.00010852\n",
      "Epoch 4575, Loss: 0.5418148762603627, learning rate: 0.00010850000000000001\n",
      "Epoch 4576, Loss: 0.5418139821732493, learning rate: 0.00010848\n",
      "Epoch 4577, Loss: 0.5418130840470375, learning rate: 0.00010846000000000002\n",
      "Epoch 4578, Loss: 0.541812190219566, learning rate: 0.00010844\n",
      "Epoch 4579, Loss: 0.541811295371064, learning rate: 0.00010842000000000001\n",
      "Epoch 4580, Loss: 0.5418104038630064, learning rate: 0.0001084\n",
      "Epoch 4581, Loss: 0.5418095085105008, learning rate: 0.00010838\n",
      "Epoch 4582, Loss: 0.5418086156568923, learning rate: 0.00010836000000000001\n",
      "Epoch 4583, Loss: 0.5418077211655734, learning rate: 0.00010834\n",
      "Epoch 4584, Loss: 0.5418068299301604, learning rate: 0.00010832\n",
      "Epoch 4585, Loss: 0.5418059346050518, learning rate: 0.0001083\n",
      "Epoch 4586, Loss: 0.5418050421471239, learning rate: 0.00010828000000000001\n",
      "Epoch 4587, Loss: 0.5418041491715477, learning rate: 0.00010826000000000001\n",
      "Epoch 4588, Loss: 0.5418032614811236, learning rate: 0.00010824000000000001\n",
      "Epoch 4589, Loss: 0.541802364061477, learning rate: 0.00010822\n",
      "Epoch 4590, Loss: 0.5418014743349974, learning rate: 0.0001082\n",
      "Epoch 4591, Loss: 0.5418005810047242, learning rate: 0.00010818000000000001\n",
      "Epoch 4592, Loss: 0.5417996869533458, learning rate: 0.00010816000000000001\n",
      "Epoch 4593, Loss: 0.5417987953739177, learning rate: 0.00010814\n",
      "Epoch 4594, Loss: 0.5417979056582302, learning rate: 0.00010812\n",
      "Epoch 4595, Loss: 0.541797016606256, learning rate: 0.00010810000000000001\n",
      "Epoch 4596, Loss: 0.5417961230265138, learning rate: 0.00010808000000000001\n",
      "Epoch 4597, Loss: 0.5417952299558153, learning rate: 0.00010806\n",
      "Epoch 4598, Loss: 0.5417943397665119, learning rate: 0.00010804\n",
      "Epoch 4599, Loss: 0.5417934521508243, learning rate: 0.00010802\n",
      "Epoch 4600, Loss: 0.5417925594474603, learning rate: 0.00010800000000000001\n",
      "Epoch 4601, Loss: 0.5417916680949186, learning rate: 0.00010798\n",
      "Epoch 4602, Loss: 0.5417907772687492, learning rate: 0.00010796000000000002\n",
      "Epoch 4603, Loss: 0.5417898923606015, learning rate: 0.00010794\n",
      "Epoch 4604, Loss: 0.5417889979303223, learning rate: 0.00010792\n",
      "Epoch 4605, Loss: 0.5417881097149972, learning rate: 0.00010790000000000001\n",
      "Epoch 4606, Loss: 0.5417872226770216, learning rate: 0.00010788\n",
      "Epoch 4607, Loss: 0.5417863298499457, learning rate: 0.00010786000000000001\n",
      "Epoch 4608, Loss: 0.5417854409440367, learning rate: 0.00010784\n",
      "Epoch 4609, Loss: 0.5417845502682752, learning rate: 0.00010782000000000001\n",
      "Epoch 4610, Loss: 0.5417836618803237, learning rate: 0.0001078\n",
      "Epoch 4611, Loss: 0.5417827738165789, learning rate: 0.00010778\n",
      "Epoch 4612, Loss: 0.5417818866253975, learning rate: 0.00010776000000000001\n",
      "Epoch 4613, Loss: 0.5417810003273673, learning rate: 0.00010774\n",
      "Epoch 4614, Loss: 0.5417801113603583, learning rate: 0.00010772\n",
      "Epoch 4615, Loss: 0.5417792219040467, learning rate: 0.0001077\n",
      "Epoch 4616, Loss: 0.5417783314392747, learning rate: 0.00010768000000000001\n",
      "Epoch 4617, Loss: 0.5417774432074183, learning rate: 0.00010766000000000001\n",
      "Epoch 4618, Loss: 0.5417765592098573, learning rate: 0.00010764\n",
      "Epoch 4619, Loss: 0.5417756692387028, learning rate: 0.00010762\n",
      "Epoch 4620, Loss: 0.5417747806679553, learning rate: 0.0001076\n",
      "Epoch 4621, Loss: 0.5417738937609387, learning rate: 0.00010758000000000001\n",
      "Epoch 4622, Loss: 0.5417730115290282, learning rate: 0.00010756000000000001\n",
      "Epoch 4623, Loss: 0.541772126236423, learning rate: 0.00010754\n",
      "Epoch 4624, Loss: 0.5417712404027556, learning rate: 0.00010752\n",
      "Epoch 4625, Loss: 0.5417703576381937, learning rate: 0.00010750000000000001\n",
      "Epoch 4626, Loss: 0.541769468967236, learning rate: 0.00010748000000000001\n",
      "Epoch 4627, Loss: 0.5417685871580105, learning rate: 0.00010746\n",
      "Epoch 4628, Loss: 0.5417677015910651, learning rate: 0.00010744\n",
      "Epoch 4629, Loss: 0.5417668165061048, learning rate: 0.00010742\n",
      "Epoch 4630, Loss: 0.5417659328740088, learning rate: 0.00010740000000000001\n",
      "Epoch 4631, Loss: 0.5417650508449873, learning rate: 0.00010738\n",
      "Epoch 4632, Loss: 0.5417641636817903, learning rate: 0.00010736000000000002\n",
      "Epoch 4633, Loss: 0.5417632834127132, learning rate: 0.00010734\n",
      "Epoch 4634, Loss: 0.5417623978776193, learning rate: 0.00010732000000000001\n",
      "Epoch 4635, Loss: 0.5417615162554048, learning rate: 0.0001073\n",
      "Epoch 4636, Loss: 0.5417606340249139, learning rate: 0.00010728\n",
      "Epoch 4637, Loss: 0.5417597489265132, learning rate: 0.00010726000000000001\n",
      "Epoch 4638, Loss: 0.5417588654010851, learning rate: 0.00010724\n",
      "Epoch 4639, Loss: 0.5417579840560206, learning rate: 0.00010722000000000001\n",
      "Epoch 4640, Loss: 0.5417571051771801, learning rate: 0.0001072\n",
      "Epoch 4641, Loss: 0.5417562208570156, learning rate: 0.00010718\n",
      "Epoch 4642, Loss: 0.5417553396261, learning rate: 0.00010716000000000001\n",
      "Epoch 4643, Loss: 0.5417544551192455, learning rate: 0.00010714\n",
      "Epoch 4644, Loss: 0.5417535736465702, learning rate: 0.00010712\n",
      "Epoch 4645, Loss: 0.5417526956716164, learning rate: 0.0001071\n",
      "Epoch 4646, Loss: 0.5417518104574873, learning rate: 0.00010708000000000001\n",
      "Epoch 4647, Loss: 0.5417509292945278, learning rate: 0.00010706000000000001\n",
      "Epoch 4648, Loss: 0.5417500490776296, learning rate: 0.00010704\n",
      "Epoch 4649, Loss: 0.5417491713257424, learning rate: 0.00010702\n",
      "Epoch 4650, Loss: 0.5417482867379085, learning rate: 0.000107\n",
      "Epoch 4651, Loss: 0.541747408127686, learning rate: 0.00010698000000000001\n",
      "Epoch 4652, Loss: 0.5417465271085625, learning rate: 0.00010696\n",
      "Epoch 4653, Loss: 0.5417456471579439, learning rate: 0.00010694\n",
      "Epoch 4654, Loss: 0.5417447710206704, learning rate: 0.00010692\n",
      "Epoch 4655, Loss: 0.5417438873233311, learning rate: 0.00010690000000000001\n",
      "Epoch 4656, Loss: 0.5417430074342385, learning rate: 0.00010688\n",
      "Epoch 4657, Loss: 0.541742127524632, learning rate: 0.00010686\n",
      "Epoch 4658, Loss: 0.5417412474620258, learning rate: 0.00010684\n",
      "Epoch 4659, Loss: 0.5417403682748844, learning rate: 0.00010682\n",
      "Epoch 4660, Loss: 0.5417394914427776, learning rate: 0.00010680000000000001\n",
      "Epoch 4661, Loss: 0.5417386091105835, learning rate: 0.00010678\n",
      "Epoch 4662, Loss: 0.541737730120809, learning rate: 0.00010676000000000001\n",
      "Epoch 4663, Loss: 0.5417368512117752, learning rate: 0.00010674\n",
      "Epoch 4664, Loss: 0.54173597726097, learning rate: 0.00010672000000000001\n",
      "Epoch 4665, Loss: 0.5417350955083855, learning rate: 0.0001067\n",
      "Epoch 4666, Loss: 0.5417342163514401, learning rate: 0.00010668\n",
      "Epoch 4667, Loss: 0.541733339580797, learning rate: 0.00010666000000000001\n",
      "Epoch 4668, Loss: 0.5417324590347603, learning rate: 0.00010664\n",
      "Epoch 4669, Loss: 0.5417315822227536, learning rate: 0.00010662\n",
      "Epoch 4670, Loss: 0.5417307065435615, learning rate: 0.0001066\n",
      "Epoch 4671, Loss: 0.5417298283395742, learning rate: 0.00010658000000000001\n",
      "Epoch 4672, Loss: 0.5417289497962112, learning rate: 0.00010656000000000001\n",
      "Epoch 4673, Loss: 0.5417280768669799, learning rate: 0.00010654\n",
      "Epoch 4674, Loss: 0.5417271957288649, learning rate: 0.00010652\n",
      "Epoch 4675, Loss: 0.5417263197438444, learning rate: 0.0001065\n",
      "Epoch 4676, Loss: 0.5417254423175808, learning rate: 0.00010648000000000001\n",
      "Epoch 4677, Loss: 0.5417245655429167, learning rate: 0.00010646000000000001\n",
      "Epoch 4678, Loss: 0.54172369046218, learning rate: 0.00010644\n",
      "Epoch 4679, Loss: 0.5417228154230113, learning rate: 0.00010642\n",
      "Epoch 4680, Loss: 0.5417219387641923, learning rate: 0.0001064\n",
      "Epoch 4681, Loss: 0.5417210627943846, learning rate: 0.00010638000000000001\n",
      "Epoch 4682, Loss: 0.5417201856448445, learning rate: 0.00010636\n",
      "Epoch 4683, Loss: 0.5417193098173222, learning rate: 0.00010634\n",
      "Epoch 4684, Loss: 0.5417184337891832, learning rate: 0.00010632\n",
      "Epoch 4685, Loss: 0.5417175619890567, learning rate: 0.00010630000000000001\n",
      "Epoch 4686, Loss: 0.5417166832824276, learning rate: 0.00010628\n",
      "Epoch 4687, Loss: 0.5417158087095056, learning rate: 0.00010626\n",
      "Epoch 4688, Loss: 0.5417149350329558, learning rate: 0.00010624\n",
      "Epoch 4689, Loss: 0.5417140583898427, learning rate: 0.00010622\n",
      "Epoch 4690, Loss: 0.5417131844078447, learning rate: 0.0001062\n",
      "Epoch 4691, Loss: 0.5417123108666133, learning rate: 0.00010618\n",
      "Epoch 4692, Loss: 0.5417114392943894, learning rate: 0.00010616000000000001\n",
      "Epoch 4693, Loss: 0.5417105645424843, learning rate: 0.00010614000000000001\n",
      "Epoch 4694, Loss: 0.5417096920993035, learning rate: 0.00010612000000000001\n",
      "Epoch 4695, Loss: 0.5417088148101618, learning rate: 0.0001061\n",
      "Epoch 4696, Loss: 0.5417079410944943, learning rate: 0.00010608\n",
      "Epoch 4697, Loss: 0.5417070699040248, learning rate: 0.00010606000000000001\n",
      "Epoch 4698, Loss: 0.5417061962485996, learning rate: 0.00010604000000000001\n",
      "Epoch 4699, Loss: 0.5417053235728518, learning rate: 0.00010602\n",
      "Epoch 4700, Loss: 0.541704452496349, learning rate: 0.000106\n",
      "Epoch 4701, Loss: 0.5417035795986657, learning rate: 0.00010598000000000001\n",
      "Epoch 4702, Loss: 0.5417027039305936, learning rate: 0.00010596000000000001\n",
      "Epoch 4703, Loss: 0.5417018322569328, learning rate: 0.00010594\n",
      "Epoch 4704, Loss: 0.5417009610665203, learning rate: 0.00010592\n",
      "Epoch 4705, Loss: 0.5417000891917004, learning rate: 0.0001059\n",
      "Epoch 4706, Loss: 0.5416992205474096, learning rate: 0.00010588000000000001\n",
      "Epoch 4707, Loss: 0.5416983463005614, learning rate: 0.00010586\n",
      "Epoch 4708, Loss: 0.5416974736023373, learning rate: 0.00010584000000000002\n",
      "Epoch 4709, Loss: 0.541696604306303, learning rate: 0.00010582\n",
      "Epoch 4710, Loss: 0.5416957314452076, learning rate: 0.00010580000000000001\n",
      "Epoch 4711, Loss: 0.5416948633380119, learning rate: 0.00010578000000000001\n",
      "Epoch 4712, Loss: 0.5416939906680462, learning rate: 0.00010576\n",
      "Epoch 4713, Loss: 0.5416931199466591, learning rate: 0.00010574000000000001\n",
      "Epoch 4714, Loss: 0.541692249807694, learning rate: 0.00010572\n",
      "Epoch 4715, Loss: 0.5416913820476053, learning rate: 0.00010570000000000001\n",
      "Epoch 4716, Loss: 0.5416905102408923, learning rate: 0.00010568\n",
      "Epoch 4717, Loss: 0.541689639959263, learning rate: 0.00010566\n",
      "Epoch 4718, Loss: 0.5416887709174762, learning rate: 0.00010564000000000001\n",
      "Epoch 4719, Loss: 0.5416878997515753, learning rate: 0.00010562\n",
      "Epoch 4720, Loss: 0.5416870319962576, learning rate: 0.0001056\n",
      "Epoch 4721, Loss: 0.5416861646594985, learning rate: 0.00010558\n",
      "Epoch 4722, Loss: 0.5416852937895354, learning rate: 0.00010556000000000001\n",
      "Epoch 4723, Loss: 0.5416844243603056, learning rate: 0.00010554000000000001\n",
      "Epoch 4724, Loss: 0.5416835561859852, learning rate: 0.00010552\n",
      "Epoch 4725, Loss: 0.5416826859201426, learning rate: 0.0001055\n",
      "Epoch 4726, Loss: 0.5416818184182702, learning rate: 0.00010548\n",
      "Epoch 4727, Loss: 0.5416809506186029, learning rate: 0.00010546000000000001\n",
      "Epoch 4728, Loss: 0.5416800851749419, learning rate: 0.00010544000000000001\n",
      "Epoch 4729, Loss: 0.5416792144999609, learning rate: 0.00010542\n",
      "Epoch 4730, Loss: 0.5416783486459286, learning rate: 0.0001054\n",
      "Epoch 4731, Loss: 0.5416774814044828, learning rate: 0.00010538000000000001\n",
      "Epoch 4732, Loss: 0.5416766114397286, learning rate: 0.00010536000000000001\n",
      "Epoch 4733, Loss: 0.5416757444279537, learning rate: 0.00010534\n",
      "Epoch 4734, Loss: 0.5416748776571527, learning rate: 0.00010532\n",
      "Epoch 4735, Loss: 0.5416740124788341, learning rate: 0.0001053\n",
      "Epoch 4736, Loss: 0.5416731467730695, learning rate: 0.00010528000000000001\n",
      "Epoch 4737, Loss: 0.5416722775761589, learning rate: 0.00010526\n",
      "Epoch 4738, Loss: 0.5416714144334253, learning rate: 0.00010524000000000002\n",
      "Epoch 4739, Loss: 0.5416705460960575, learning rate: 0.00010522\n",
      "Epoch 4740, Loss: 0.5416696804329506, learning rate: 0.00010520000000000001\n",
      "Epoch 4741, Loss: 0.5416688175509698, learning rate: 0.00010518\n",
      "Epoch 4742, Loss: 0.5416679482092664, learning rate: 0.00010516\n",
      "Epoch 4743, Loss: 0.5416670820318917, learning rate: 0.00010514000000000001\n",
      "Epoch 4744, Loss: 0.5416662173418291, learning rate: 0.00010512\n",
      "Epoch 4745, Loss: 0.5416653563558592, learning rate: 0.00010510000000000001\n",
      "Epoch 4746, Loss: 0.5416644872075776, learning rate: 0.00010508\n",
      "Epoch 4747, Loss: 0.5416636256643582, learning rate: 0.00010506000000000001\n",
      "Epoch 4748, Loss: 0.541662760568618, learning rate: 0.00010504000000000001\n",
      "Epoch 4749, Loss: 0.5416618939490295, learning rate: 0.00010502\n",
      "Epoch 4750, Loss: 0.541661030389248, learning rate: 0.000105\n",
      "Epoch 4751, Loss: 0.541660165462027, learning rate: 0.00010498\n",
      "Epoch 4752, Loss: 0.5416593037286034, learning rate: 0.00010496000000000001\n",
      "Epoch 4753, Loss: 0.5416584388258641, learning rate: 0.00010494000000000001\n",
      "Epoch 4754, Loss: 0.541657575298266, learning rate: 0.00010492\n",
      "Epoch 4755, Loss: 0.5416567112037511, learning rate: 0.0001049\n",
      "Epoch 4756, Loss: 0.5416558475867721, learning rate: 0.00010488\n",
      "Epoch 4757, Loss: 0.5416549838992409, learning rate: 0.00010486000000000001\n",
      "Epoch 4758, Loss: 0.541654120313969, learning rate: 0.00010484\n",
      "Epoch 4759, Loss: 0.5416532571931921, learning rate: 0.00010482\n",
      "Epoch 4760, Loss: 0.5416523975944817, learning rate: 0.0001048\n",
      "Epoch 4761, Loss: 0.541651533593032, learning rate: 0.00010478000000000001\n",
      "Epoch 4762, Loss: 0.5416506723207178, learning rate: 0.00010476\n",
      "Epoch 4763, Loss: 0.5416498082834778, learning rate: 0.00010474\n",
      "Epoch 4764, Loss: 0.5416489459804384, learning rate: 0.00010472\n",
      "Epoch 4765, Loss: 0.5416480860125825, learning rate: 0.0001047\n",
      "Epoch 4766, Loss: 0.5416472254311844, learning rate: 0.00010468000000000001\n",
      "Epoch 4767, Loss: 0.541646361625841, learning rate: 0.00010466\n",
      "Epoch 4768, Loss: 0.54164550047319, learning rate: 0.00010464000000000001\n",
      "Epoch 4769, Loss: 0.5416446383733297, learning rate: 0.00010462\n",
      "Epoch 4770, Loss: 0.541643781324449, learning rate: 0.00010460000000000001\n",
      "Epoch 4771, Loss: 0.541642916716331, learning rate: 0.00010458\n",
      "Epoch 4772, Loss: 0.5416420559454473, learning rate: 0.00010456\n",
      "Epoch 4773, Loss: 0.541641195930389, learning rate: 0.00010454000000000001\n",
      "Epoch 4774, Loss: 0.5416403347731518, learning rate: 0.00010452\n",
      "Epoch 4775, Loss: 0.5416394753224524, learning rate: 0.0001045\n",
      "Epoch 4776, Loss: 0.5416386178446396, learning rate: 0.00010448\n",
      "Epoch 4777, Loss: 0.541637757940276, learning rate: 0.00010446000000000001\n",
      "Epoch 4778, Loss: 0.5416368986961887, learning rate: 0.00010444000000000001\n",
      "Epoch 4779, Loss: 0.5416360368341723, learning rate: 0.00010442\n",
      "Epoch 4780, Loss: 0.5416351774380507, learning rate: 0.0001044\n",
      "Epoch 4781, Loss: 0.5416343180116931, learning rate: 0.00010438\n",
      "Epoch 4782, Loss: 0.5416334622852305, learning rate: 0.00010436000000000001\n",
      "Epoch 4783, Loss: 0.541632601445001, learning rate: 0.00010434000000000001\n",
      "Epoch 4784, Loss: 0.5416317410580563, learning rate: 0.00010432\n",
      "Epoch 4785, Loss: 0.5416308840142305, learning rate: 0.0001043\n",
      "Epoch 4786, Loss: 0.5416300268203126, learning rate: 0.00010428000000000001\n",
      "Epoch 4787, Loss: 0.5416291700767588, learning rate: 0.00010426000000000001\n",
      "Epoch 4788, Loss: 0.5416283092429945, learning rate: 0.00010424\n",
      "Epoch 4789, Loss: 0.5416274534944435, learning rate: 0.00010422\n",
      "Epoch 4790, Loss: 0.5416265953756171, learning rate: 0.0001042\n",
      "Epoch 4791, Loss: 0.5416257366762018, learning rate: 0.00010418000000000001\n",
      "Epoch 4792, Loss: 0.541624879767752, learning rate: 0.00010416\n",
      "Epoch 4793, Loss: 0.541624022276143, learning rate: 0.00010414\n",
      "Epoch 4794, Loss: 0.5416231655775237, learning rate: 0.00010412\n",
      "Epoch 4795, Loss: 0.5416223092176744, learning rate: 0.0001041\n",
      "Epoch 4796, Loss: 0.5416214557121899, learning rate: 0.00010408\n",
      "Epoch 4797, Loss: 0.5416205958351684, learning rate: 0.00010406\n",
      "Epoch 4798, Loss: 0.5416197428296813, learning rate: 0.00010404000000000001\n",
      "Epoch 4799, Loss: 0.5416188854937697, learning rate: 0.00010402\n",
      "Epoch 4800, Loss: 0.5416180283101067, learning rate: 0.00010400000000000001\n",
      "Epoch 4801, Loss: 0.5416171714784671, learning rate: 0.00010398\n",
      "Epoch 4802, Loss: 0.5416163157065694, learning rate: 0.00010396\n",
      "Epoch 4803, Loss: 0.5416154616654311, learning rate: 0.00010394000000000001\n",
      "Epoch 4804, Loss: 0.5416146110559704, learning rate: 0.00010392\n",
      "Epoch 4805, Loss: 0.5416137530917715, learning rate: 0.0001039\n",
      "Epoch 4806, Loss: 0.5416128956682581, learning rate: 0.00010388\n",
      "Epoch 4807, Loss: 0.5416120416963693, learning rate: 0.00010386000000000001\n",
      "Epoch 4808, Loss: 0.5416111897849429, learning rate: 0.00010384000000000001\n",
      "Epoch 4809, Loss: 0.5416103324745856, learning rate: 0.00010382\n",
      "Epoch 4810, Loss: 0.5416094804402314, learning rate: 0.0001038\n",
      "Epoch 4811, Loss: 0.5416086266944669, learning rate: 0.00010378\n",
      "Epoch 4812, Loss: 0.5416077703357056, learning rate: 0.00010376000000000001\n",
      "Epoch 4813, Loss: 0.5416069166875085, learning rate: 0.00010374\n",
      "Epoch 4814, Loss: 0.5416060635773775, learning rate: 0.00010372\n",
      "Epoch 4815, Loss: 0.541605210437204, learning rate: 0.0001037\n",
      "Epoch 4816, Loss: 0.5416043576092027, learning rate: 0.00010368000000000001\n",
      "Epoch 4817, Loss: 0.5416035092472616, learning rate: 0.00010366000000000001\n",
      "Epoch 4818, Loss: 0.5416026523860292, learning rate: 0.00010364\n",
      "Epoch 4819, Loss: 0.5416017994142379, learning rate: 0.00010362\n",
      "Epoch 4820, Loss: 0.5416009503020686, learning rate: 0.0001036\n",
      "Epoch 4821, Loss: 0.5416000968649588, learning rate: 0.00010358000000000001\n",
      "Epoch 4822, Loss: 0.5415992427980606, learning rate: 0.00010356\n",
      "Epoch 4823, Loss: 0.5415983917693827, learning rate: 0.00010354000000000001\n",
      "Epoch 4824, Loss: 0.5415975407425124, learning rate: 0.00010352000000000001\n",
      "Epoch 4825, Loss: 0.5415966880627532, learning rate: 0.0001035\n",
      "Epoch 4826, Loss: 0.5415958388831592, learning rate: 0.00010348\n",
      "Epoch 4827, Loss: 0.5415949868078807, learning rate: 0.00010346\n",
      "Epoch 4828, Loss: 0.5415941353874771, learning rate: 0.00010344000000000001\n",
      "Epoch 4829, Loss: 0.5415932836948473, learning rate: 0.00010342000000000001\n",
      "Epoch 4830, Loss: 0.5415924339614464, learning rate: 0.0001034\n",
      "Epoch 4831, Loss: 0.5415915822305971, learning rate: 0.00010338\n",
      "Epoch 4832, Loss: 0.5415907345855031, learning rate: 0.00010336\n",
      "Epoch 4833, Loss: 0.5415898856676303, learning rate: 0.00010334000000000001\n",
      "Epoch 4834, Loss: 0.5415890344805799, learning rate: 0.00010332000000000001\n",
      "Epoch 4835, Loss: 0.5415881831720619, learning rate: 0.0001033\n",
      "Epoch 4836, Loss: 0.5415873335427378, learning rate: 0.00010328\n",
      "Epoch 4837, Loss: 0.5415864848193792, learning rate: 0.00010326000000000001\n",
      "Epoch 4838, Loss: 0.5415856368784087, learning rate: 0.00010324000000000001\n",
      "Epoch 4839, Loss: 0.5415847853360491, learning rate: 0.00010322\n",
      "Epoch 4840, Loss: 0.5415839370441777, learning rate: 0.0001032\n",
      "Epoch 4841, Loss: 0.5415830889648294, learning rate: 0.00010318\n",
      "Epoch 4842, Loss: 0.5415822388959227, learning rate: 0.00010316000000000001\n",
      "Epoch 4843, Loss: 0.5415813958328277, learning rate: 0.00010314\n",
      "Epoch 4844, Loss: 0.5415805441998324, learning rate: 0.00010312000000000002\n",
      "Epoch 4845, Loss: 0.5415796973379844, learning rate: 0.0001031\n",
      "Epoch 4846, Loss: 0.5415788504844237, learning rate: 0.00010308000000000001\n",
      "Epoch 4847, Loss: 0.5415779996445366, learning rate: 0.00010306\n",
      "Epoch 4848, Loss: 0.5415771531940006, learning rate: 0.00010304\n",
      "Epoch 4849, Loss: 0.5415763066229714, learning rate: 0.00010302000000000001\n",
      "Epoch 4850, Loss: 0.5415754584277301, learning rate: 0.000103\n",
      "Epoch 4851, Loss: 0.5415746094939962, learning rate: 0.00010298\n",
      "Epoch 4852, Loss: 0.5415737633451855, learning rate: 0.00010296\n",
      "Epoch 4853, Loss: 0.5415729168208219, learning rate: 0.00010294000000000001\n",
      "Epoch 4854, Loss: 0.5415720696394744, learning rate: 0.00010292000000000001\n",
      "Epoch 4855, Loss: 0.5415712258349428, learning rate: 0.00010290000000000001\n",
      "Epoch 4856, Loss: 0.5415703769891363, learning rate: 0.00010288\n",
      "Epoch 4857, Loss: 0.5415695304491047, learning rate: 0.00010286\n",
      "Epoch 4858, Loss: 0.5415686852464111, learning rate: 0.00010284000000000001\n",
      "Epoch 4859, Loss: 0.5415678396891226, learning rate: 0.00010282000000000001\n",
      "Epoch 4860, Loss: 0.5415669946723244, learning rate: 0.0001028\n",
      "Epoch 4861, Loss: 0.5415661478212489, learning rate: 0.00010278\n",
      "Epoch 4862, Loss: 0.5415653041084904, learning rate: 0.00010276\n",
      "Epoch 4863, Loss: 0.5415644607497189, learning rate: 0.00010274000000000001\n",
      "Epoch 4864, Loss: 0.5415636130779711, learning rate: 0.00010272\n",
      "Epoch 4865, Loss: 0.541562768095462, learning rate: 0.0001027\n",
      "Epoch 4866, Loss: 0.5415619242393449, learning rate: 0.00010268\n",
      "Epoch 4867, Loss: 0.5415610826406294, learning rate: 0.00010266000000000001\n",
      "Epoch 4868, Loss: 0.5415602364490811, learning rate: 0.00010264\n",
      "Epoch 4869, Loss: 0.5415593915420166, learning rate: 0.00010262\n",
      "Epoch 4870, Loss: 0.5415585478925258, learning rate: 0.0001026\n",
      "Epoch 4871, Loss: 0.5415577051094472, learning rate: 0.00010258\n",
      "Epoch 4872, Loss: 0.5415568607224599, learning rate: 0.00010256000000000001\n",
      "Epoch 4873, Loss: 0.5415560204964646, learning rate: 0.00010254\n",
      "Epoch 4874, Loss: 0.5415551753142218, learning rate: 0.00010252000000000001\n",
      "Epoch 4875, Loss: 0.541554334211842, learning rate: 0.0001025\n",
      "Epoch 4876, Loss: 0.5415534882612596, learning rate: 0.00010248000000000001\n",
      "Epoch 4877, Loss: 0.5415526454228028, learning rate: 0.00010246\n",
      "Epoch 4878, Loss: 0.5415518035924936, learning rate: 0.00010244\n",
      "Epoch 4879, Loss: 0.5415509622531753, learning rate: 0.00010242000000000001\n",
      "Epoch 4880, Loss: 0.5415501198923639, learning rate: 0.0001024\n",
      "Epoch 4881, Loss: 0.5415492809463694, learning rate: 0.00010238\n",
      "Epoch 4882, Loss: 0.5415484359499018, learning rate: 0.00010236\n",
      "Epoch 4883, Loss: 0.5415475972490719, learning rate: 0.00010234000000000001\n",
      "Epoch 4884, Loss: 0.5415467518736908, learning rate: 0.00010232000000000001\n",
      "Epoch 4885, Loss: 0.5415459131035325, learning rate: 0.0001023\n",
      "Epoch 4886, Loss: 0.5415450721255015, learning rate: 0.00010228\n",
      "Epoch 4887, Loss: 0.5415442298079635, learning rate: 0.00010226\n",
      "Epoch 4888, Loss: 0.5415433902827821, learning rate: 0.00010224000000000001\n",
      "Epoch 4889, Loss: 0.5415425487286571, learning rate: 0.00010222000000000001\n",
      "Epoch 4890, Loss: 0.5415417099515002, learning rate: 0.0001022\n",
      "Epoch 4891, Loss: 0.5415408682004976, learning rate: 0.00010218\n",
      "Epoch 4892, Loss: 0.5415400288175325, learning rate: 0.00010216000000000001\n",
      "Epoch 4893, Loss: 0.541539189655949, learning rate: 0.00010214000000000001\n",
      "Epoch 4894, Loss: 0.5415383474034772, learning rate: 0.00010212\n",
      "Epoch 4895, Loss: 0.541537507319303, learning rate: 0.0001021\n",
      "Epoch 4896, Loss: 0.5415366682940727, learning rate: 0.00010208\n",
      "Epoch 4897, Loss: 0.5415358286973194, learning rate: 0.00010206000000000001\n",
      "Epoch 4898, Loss: 0.5415349902280331, learning rate: 0.00010204\n",
      "Epoch 4899, Loss: 0.5415341498478007, learning rate: 0.00010202\n",
      "Epoch 4900, Loss: 0.5415333119076222, learning rate: 0.000102\n",
      "Epoch 4901, Loss: 0.5415324749150777, learning rate: 0.00010198\n",
      "Epoch 4902, Loss: 0.5415316342247087, learning rate: 0.00010196\n",
      "Epoch 4903, Loss: 0.5415307987483098, learning rate: 0.00010194\n",
      "Epoch 4904, Loss: 0.5415299593301053, learning rate: 0.00010192000000000001\n",
      "Epoch 4905, Loss: 0.5415291202516619, learning rate: 0.0001019\n",
      "Epoch 4906, Loss: 0.541528283270533, learning rate: 0.00010188000000000001\n",
      "Epoch 4907, Loss: 0.5415274478796901, learning rate: 0.00010186\n",
      "Epoch 4908, Loss: 0.5415266070593808, learning rate: 0.00010184\n",
      "Epoch 4909, Loss: 0.5415257690858222, learning rate: 0.00010182000000000001\n",
      "Epoch 4910, Loss: 0.5415249326288045, learning rate: 0.0001018\n",
      "Epoch 4911, Loss: 0.5415240954926521, learning rate: 0.00010178\n",
      "Epoch 4912, Loss: 0.5415232589605359, learning rate: 0.00010176\n",
      "Epoch 4913, Loss: 0.5415224241002435, learning rate: 0.00010174000000000001\n",
      "Epoch 4914, Loss: 0.5415215916343356, learning rate: 0.00010172000000000001\n",
      "Epoch 4915, Loss: 0.5415207507276647, learning rate: 0.0001017\n",
      "Epoch 4916, Loss: 0.5415199148987941, learning rate: 0.00010168\n",
      "Epoch 4917, Loss: 0.5415190805014494, learning rate: 0.00010166\n",
      "Epoch 4918, Loss: 0.5415182422270866, learning rate: 0.00010164000000000001\n",
      "Epoch 4919, Loss: 0.5415174055205365, learning rate: 0.00010162\n",
      "Epoch 4920, Loss: 0.5415165705213424, learning rate: 0.0001016\n",
      "Epoch 4921, Loss: 0.5415157348028101, learning rate: 0.00010158\n",
      "Epoch 4922, Loss: 0.5415149011709313, learning rate: 0.00010156000000000001\n",
      "Epoch 4923, Loss: 0.5415140655497074, learning rate: 0.00010154000000000001\n",
      "Epoch 4924, Loss: 0.5415132319939646, learning rate: 0.00010152\n",
      "Epoch 4925, Loss: 0.5415123949785702, learning rate: 0.0001015\n",
      "Epoch 4926, Loss: 0.541511564573842, learning rate: 0.00010148\n",
      "Epoch 4927, Loss: 0.541510727917391, learning rate: 0.00010146000000000001\n",
      "Epoch 4928, Loss: 0.5415098921100568, learning rate: 0.00010144\n",
      "Epoch 4929, Loss: 0.5415090593689166, learning rate: 0.00010142000000000001\n",
      "Epoch 4930, Loss: 0.5415082279966856, learning rate: 0.0001014\n",
      "Epoch 4931, Loss: 0.5415073905652781, learning rate: 0.00010138000000000001\n",
      "Epoch 4932, Loss: 0.5415065574691459, learning rate: 0.00010136\n",
      "Epoch 4933, Loss: 0.5415057236205729, learning rate: 0.00010134\n",
      "Epoch 4934, Loss: 0.541504892103285, learning rate: 0.00010132000000000001\n",
      "Epoch 4935, Loss: 0.5415040582092151, learning rate: 0.0001013\n",
      "Epoch 4936, Loss: 0.5415032259810789, learning rate: 0.00010128\n",
      "Epoch 4937, Loss: 0.5415023921566589, learning rate: 0.00010126\n",
      "Epoch 4938, Loss: 0.5415015604470249, learning rate: 0.00010124\n",
      "Epoch 4939, Loss: 0.5415007281611269, learning rate: 0.00010122000000000001\n",
      "Epoch 4940, Loss: 0.5414998965491419, learning rate: 0.0001012\n",
      "Epoch 4941, Loss: 0.5414990628417522, learning rate: 0.00010118\n",
      "Epoch 4942, Loss: 0.5414982317097714, learning rate: 0.00010116\n",
      "Epoch 4943, Loss: 0.5414973992674257, learning rate: 0.00010114000000000001\n",
      "Epoch 4944, Loss: 0.5414965718257013, learning rate: 0.00010112000000000001\n",
      "Epoch 4945, Loss: 0.5414957376999984, learning rate: 0.0001011\n",
      "Epoch 4946, Loss: 0.5414949060394868, learning rate: 0.00010108\n",
      "Epoch 4947, Loss: 0.5414940751710571, learning rate: 0.00010106\n",
      "Epoch 4948, Loss: 0.5414932448144345, learning rate: 0.00010104000000000001\n",
      "Epoch 4949, Loss: 0.5414924124966348, learning rate: 0.00010102\n",
      "Epoch 4950, Loss: 0.5414915831950702, learning rate: 0.000101\n",
      "Epoch 4951, Loss: 0.5414907529517249, learning rate: 0.00010098\n",
      "Epoch 4952, Loss: 0.5414899277743509, learning rate: 0.00010096000000000001\n",
      "Epoch 4953, Loss: 0.5414890938200896, learning rate: 0.00010094\n",
      "Epoch 4954, Loss: 0.5414882634083035, learning rate: 0.00010092\n",
      "Epoch 4955, Loss: 0.5414874359116566, learning rate: 0.00010090000000000001\n",
      "Epoch 4956, Loss: 0.5414866044098945, learning rate: 0.00010088\n",
      "Epoch 4957, Loss: 0.5414857750884959, learning rate: 0.00010086\n",
      "Epoch 4958, Loss: 0.5414849454706049, learning rate: 0.00010084\n",
      "Epoch 4959, Loss: 0.5414841171586755, learning rate: 0.00010082000000000001\n",
      "Epoch 4960, Loss: 0.5414832904421202, learning rate: 0.00010080000000000001\n",
      "Epoch 4961, Loss: 0.5414824590422012, learning rate: 0.00010078000000000001\n",
      "Epoch 4962, Loss: 0.5414816303395535, learning rate: 0.00010076\n",
      "Epoch 4963, Loss: 0.5414808015496464, learning rate: 0.00010074\n",
      "Epoch 4964, Loss: 0.541479973782066, learning rate: 0.00010072000000000001\n",
      "Epoch 4965, Loss: 0.5414791460592391, learning rate: 0.00010070000000000001\n",
      "Epoch 4966, Loss: 0.5414783182420692, learning rate: 0.00010068\n",
      "Epoch 4967, Loss: 0.5414774906465007, learning rate: 0.00010066\n",
      "Epoch 4968, Loss: 0.5414766636207955, learning rate: 0.00010064000000000001\n",
      "Epoch 4969, Loss: 0.5414758367112056, learning rate: 0.00010062000000000001\n",
      "Epoch 4970, Loss: 0.541475008719333, learning rate: 0.0001006\n",
      "Epoch 4971, Loss: 0.5414741855163389, learning rate: 0.00010058\n",
      "Epoch 4972, Loss: 0.5414733550706653, learning rate: 0.00010056\n",
      "Epoch 4973, Loss: 0.5414725304382454, learning rate: 0.00010054000000000001\n",
      "Epoch 4974, Loss: 0.541471705110433, learning rate: 0.00010052\n",
      "Epoch 4975, Loss: 0.541470876149698, learning rate: 0.0001005\n",
      "Epoch 4976, Loss: 0.5414700496131131, learning rate: 0.00010048\n",
      "Epoch 4977, Loss: 0.5414692249901397, learning rate: 0.00010046\n",
      "Epoch 4978, Loss: 0.5414683964302236, learning rate: 0.00010044000000000001\n",
      "Epoch 4979, Loss: 0.5414675730637366, learning rate: 0.00010042\n",
      "Epoch 4980, Loss: 0.5414667450248659, learning rate: 0.00010040000000000001\n",
      "Epoch 4981, Loss: 0.5414659211373684, learning rate: 0.00010038\n",
      "Epoch 4982, Loss: 0.5414650972560543, learning rate: 0.00010036000000000001\n",
      "Epoch 4983, Loss: 0.5414642698190794, learning rate: 0.00010034\n",
      "Epoch 4984, Loss: 0.541463445807143, learning rate: 0.00010032\n",
      "Epoch 4985, Loss: 0.5414626207412958, learning rate: 0.00010030000000000001\n",
      "Epoch 4986, Loss: 0.5414617947604002, learning rate: 0.00010028\n",
      "Epoch 4987, Loss: 0.5414609706591327, learning rate: 0.00010026\n",
      "Epoch 4988, Loss: 0.5414601445962569, learning rate: 0.00010024\n",
      "Epoch 4989, Loss: 0.5414593204791325, learning rate: 0.00010022000000000001\n",
      "Epoch 4990, Loss: 0.5414584969348206, learning rate: 0.00010020000000000001\n",
      "Epoch 4991, Loss: 0.5414576714808409, learning rate: 0.00010018\n",
      "Epoch 4992, Loss: 0.5414568478231045, learning rate: 0.00010016\n",
      "Epoch 4993, Loss: 0.5414560258463345, learning rate: 0.00010014\n",
      "Epoch 4994, Loss: 0.5414552040670885, learning rate: 0.00010012000000000001\n",
      "Epoch 4995, Loss: 0.5414543775556045, learning rate: 0.00010010000000000001\n",
      "Epoch 4996, Loss: 0.5414535536552625, learning rate: 0.00010008\n",
      "Epoch 4997, Loss: 0.541452731172949, learning rate: 0.00010006\n",
      "Epoch 4998, Loss: 0.5414519067513858, learning rate: 0.00010004000000000001\n",
      "Epoch 4999, Loss: 0.5414510858080508, learning rate: 0.00010002000000000001\n",
      "Epoch 5000, Loss: 0.5414502613400771, learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "NN.train(x_train, y_train, epochs=5000, min_learning_rate=1e-4, max_learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 - Predicted: 3, Actual: 3\n",
      "Image 1 - Predicted: 1, Actual: 1\n",
      "Image 2 - Predicted: 8, Actual: 8\n",
      "Image 3 - Predicted: 9, Actual: 9\n",
      "Image 4 - Predicted: 9, Actual: 5\n",
      "Image 5 - Predicted: 5, Actual: 5\n",
      "Image 6 - Predicted: 9, Actual: 9\n",
      "Image 7 - Predicted: 1, Actual: 1\n",
      "Image 8 - Predicted: 9, Actual: 9\n",
      "Image 9 - Predicted: 9, Actual: 7\n",
      "Image 10 - Predicted: 6, Actual: 2\n",
      "Image 11 - Predicted: 8, Actual: 4\n",
      "Image 12 - Predicted: 8, Actual: 8\n",
      "Image 13 - Predicted: 6, Actual: 6\n",
      "Image 14 - Predicted: 4, Actual: 4\n",
      "Image 15 - Predicted: 5, Actual: 5\n",
      "Image 16 - Predicted: 8, Actual: 8\n",
      "Image 17 - Predicted: 6, Actual: 6\n",
      "Image 18 - Predicted: 9, Actual: 5\n",
      "Image 19 - Predicted: 7, Actual: 7\n",
      "Image 20 - Predicted: 8, Actual: 8\n",
      "Image 21 - Predicted: 5, Actual: 5\n",
      "Image 22 - Predicted: 8, Actual: 8\n",
      "Image 23 - Predicted: 3, Actual: 3\n",
      "Image 24 - Predicted: 2, Actual: 2\n",
      "Image 25 - Predicted: 9, Actual: 7\n",
      "Image 26 - Predicted: 4, Actual: 4\n",
      "Image 27 - Predicted: 1, Actual: 1\n",
      "Image 28 - Predicted: 0, Actual: 0\n",
      "Image 29 - Predicted: 8, Actual: 4\n",
      "Image 30 - Predicted: 8, Actual: 9\n",
      "Image 31 - Predicted: 9, Actual: 9\n",
      "Image 32 - Predicted: 5, Actual: 5\n",
      "Image 33 - Predicted: 0, Actual: 0\n",
      "Image 34 - Predicted: 8, Actual: 8\n",
      "Image 35 - Predicted: 7, Actual: 7\n",
      "Image 36 - Predicted: 3, Actual: 3\n",
      "Image 37 - Predicted: 4, Actual: 4\n",
      "Image 38 - Predicted: 9, Actual: 9\n",
      "Image 39 - Predicted: 6, Actual: 6\n",
      "Image 40 - Predicted: 6, Actual: 6\n",
      "Image 41 - Predicted: 4, Actual: 4\n",
      "Image 42 - Predicted: 2, Actual: 1\n",
      "Image 43 - Predicted: 7, Actual: 7\n",
      "Image 44 - Predicted: 0, Actual: 0\n",
      "Image 45 - Predicted: 6, Actual: 6\n",
      "Image 46 - Predicted: 8, Actual: 8\n",
      "Image 47 - Predicted: 4, Actual: 8\n",
      "Image 48 - Predicted: 6, Actual: 6\n",
      "Image 49 - Predicted: 0, Actual: 0\n",
      "Image 50 - Predicted: 1, Actual: 1\n",
      "Image 51 - Predicted: 8, Actual: 8\n",
      "Image 52 - Predicted: 8, Actual: 8\n",
      "Image 53 - Predicted: 8, Actual: 8\n",
      "Image 54 - Predicted: 2, Actual: 2\n",
      "Image 55 - Predicted: 6, Actual: 6\n",
      "Image 56 - Predicted: 3, Actual: 3\n",
      "Image 57 - Predicted: 3, Actual: 3\n",
      "Image 58 - Predicted: 1, Actual: 1\n",
      "Image 59 - Predicted: 9, Actual: 9\n",
      "Image 60 - Predicted: 8, Actual: 8\n",
      "Image 61 - Predicted: 1, Actual: 1\n",
      "Image 62 - Predicted: 0, Actual: 0\n",
      "Image 63 - Predicted: 7, Actual: 7\n",
      "Image 64 - Predicted: 5, Actual: 5\n",
      "Image 65 - Predicted: 3, Actual: 3\n",
      "Image 66 - Predicted: 8, Actual: 4\n",
      "Image 67 - Predicted: 3, Actual: 3\n",
      "Image 68 - Predicted: 9, Actual: 8\n",
      "Image 69 - Predicted: 2, Actual: 6\n",
      "Image 70 - Predicted: 1, Actual: 1\n",
      "Image 71 - Predicted: 4, Actual: 4\n",
      "Image 72 - Predicted: 8, Actual: 7\n",
      "Image 73 - Predicted: 8, Actual: 8\n",
      "Image 74 - Predicted: 9, Actual: 9\n",
      "Image 75 - Predicted: 1, Actual: 1\n",
      "Image 76 - Predicted: 0, Actual: 0\n",
      "Image 77 - Predicted: 8, Actual: 5\n",
      "Image 78 - Predicted: 1, Actual: 1\n",
      "Image 79 - Predicted: 7, Actual: 7\n",
      "Image 80 - Predicted: 7, Actual: 9\n",
      "Image 81 - Predicted: 1, Actual: 1\n",
      "Image 82 - Predicted: 4, Actual: 4\n",
      "Image 83 - Predicted: 1, Actual: 1\n",
      "Image 84 - Predicted: 7, Actual: 7\n",
      "Image 85 - Predicted: 8, Actual: 8\n",
      "Image 86 - Predicted: 6, Actual: 0\n",
      "Image 87 - Predicted: 1, Actual: 1\n",
      "Image 88 - Predicted: 6, Actual: 6\n",
      "Image 89 - Predicted: 9, Actual: 9\n",
      "Image 90 - Predicted: 3, Actual: 3\n",
      "Image 91 - Predicted: 6, Actual: 6\n",
      "Image 92 - Predicted: 6, Actual: 6\n",
      "Image 93 - Predicted: 1, Actual: 1\n",
      "Image 94 - Predicted: 3, Actual: 3\n",
      "Image 95 - Predicted: 3, Actual: 3\n",
      "Image 96 - Predicted: 6, Actual: 6\n",
      "Image 97 - Predicted: 0, Actual: 0\n",
      "Image 98 - Predicted: 0, Actual: 0\n",
      "Image 99 - Predicted: 5, Actual: 5\n",
      "Image 100 - Predicted: 6, Actual: 6\n",
      "Image 101 - Predicted: 1, Actual: 1\n",
      "Image 102 - Predicted: 5, Actual: 4\n",
      "Image 103 - Predicted: 8, Actual: 8\n",
      "Image 104 - Predicted: 0, Actual: 0\n",
      "Image 105 - Predicted: 1, Actual: 1\n",
      "Image 106 - Predicted: 4, Actual: 4\n",
      "Image 107 - Predicted: 2, Actual: 2\n",
      "Image 108 - Predicted: 0, Actual: 0\n",
      "Image 109 - Predicted: 5, Actual: 4\n",
      "Image 110 - Predicted: 6, Actual: 6\n",
      "Image 111 - Predicted: 1, Actual: 1\n",
      "Image 112 - Predicted: 8, Actual: 8\n",
      "Image 113 - Predicted: 6, Actual: 6\n",
      "Image 114 - Predicted: 7, Actual: 7\n",
      "Image 115 - Predicted: 9, Actual: 9\n",
      "Image 116 - Predicted: 6, Actual: 6\n",
      "Image 117 - Predicted: 6, Actual: 6\n",
      "Image 118 - Predicted: 9, Actual: 9\n",
      "Image 119 - Predicted: 0, Actual: 0\n",
      "Image 120 - Predicted: 9, Actual: 9\n",
      "Image 121 - Predicted: 1, Actual: 1\n",
      "Image 122 - Predicted: 5, Actual: 8\n",
      "Image 123 - Predicted: 5, Actual: 5\n",
      "Image 124 - Predicted: 1, Actual: 1\n",
      "Image 125 - Predicted: 5, Actual: 5\n",
      "Image 126 - Predicted: 0, Actual: 0\n",
      "Image 127 - Predicted: 9, Actual: 9\n",
      "Image 128 - Predicted: 8, Actual: 4\n",
      "Image 129 - Predicted: 9, Actual: 9\n",
      "Image 130 - Predicted: 3, Actual: 3\n",
      "Image 131 - Predicted: 1, Actual: 1\n",
      "Image 132 - Predicted: 7, Actual: 7\n",
      "Image 133 - Predicted: 2, Actual: 2\n",
      "Image 134 - Predicted: 0, Actual: 0\n",
      "Image 135 - Predicted: 0, Actual: 1\n",
      "Image 136 - Predicted: 9, Actual: 9\n",
      "Image 137 - Predicted: 3, Actual: 3\n",
      "Image 138 - Predicted: 0, Actual: 0\n",
      "Image 139 - Predicted: 1, Actual: 1\n",
      "Image 140 - Predicted: 1, Actual: 1\n",
      "Image 141 - Predicted: 6, Actual: 6\n",
      "Image 142 - Predicted: 1, Actual: 1\n",
      "Image 143 - Predicted: 4, Actual: 4\n",
      "Image 144 - Predicted: 4, Actual: 4\n",
      "Image 145 - Predicted: 1, Actual: 1\n",
      "Image 146 - Predicted: 3, Actual: 3\n",
      "Image 147 - Predicted: 3, Actual: 3\n",
      "Image 148 - Predicted: 5, Actual: 9\n",
      "Image 149 - Predicted: 5, Actual: 5\n",
      "Image 150 - Predicted: 8, Actual: 4\n",
      "Image 151 - Predicted: 1, Actual: 1\n",
      "Image 152 - Predicted: 6, Actual: 6\n",
      "Image 153 - Predicted: 4, Actual: 4\n",
      "Image 154 - Predicted: 7, Actual: 7\n",
      "Image 155 - Predicted: 1, Actual: 1\n",
      "Image 156 - Predicted: 9, Actual: 9\n",
      "Image 157 - Predicted: 5, Actual: 5\n",
      "Image 158 - Predicted: 3, Actual: 3\n",
      "Image 159 - Predicted: 7, Actual: 7\n",
      "Image 160 - Predicted: 8, Actual: 8\n",
      "Image 161 - Predicted: 8, Actual: 8\n",
      "Image 162 - Predicted: 0, Actual: 0\n",
      "Image 163 - Predicted: 2, Actual: 2\n",
      "Image 164 - Predicted: 9, Actual: 8\n",
      "Image 165 - Predicted: 3, Actual: 3\n",
      "Image 166 - Predicted: 3, Actual: 8\n",
      "Image 167 - Predicted: 5, Actual: 5\n",
      "Image 168 - Predicted: 1, Actual: 1\n",
      "Image 169 - Predicted: 2, Actual: 2\n",
      "Image 170 - Predicted: 2, Actual: 2\n",
      "Image 171 - Predicted: 2, Actual: 2\n",
      "Image 172 - Predicted: 2, Actual: 2\n",
      "Image 173 - Predicted: 7, Actual: 7\n",
      "Image 174 - Predicted: 8, Actual: 8\n",
      "Image 175 - Predicted: 9, Actual: 9\n",
      "Image 176 - Predicted: 0, Actual: 1\n",
      "Image 177 - Predicted: 7, Actual: 7\n",
      "Image 178 - Predicted: 4, Actual: 4\n",
      "Image 179 - Predicted: 7, Actual: 7\n",
      "Image 180 - Predicted: 0, Actual: 0\n",
      "Image 181 - Predicted: 3, Actual: 3\n",
      "Image 182 - Predicted: 5, Actual: 4\n",
      "Image 183 - Predicted: 5, Actual: 5\n",
      "Image 184 - Predicted: 4, Actual: 4\n",
      "Image 185 - Predicted: 5, Actual: 5\n",
      "Image 186 - Predicted: 5, Actual: 5\n",
      "Image 187 - Predicted: 3, Actual: 3\n",
      "Image 188 - Predicted: 1, Actual: 1\n",
      "Image 189 - Predicted: 8, Actual: 8\n",
      "Image 190 - Predicted: 6, Actual: 2\n",
      "Image 191 - Predicted: 0, Actual: 0\n",
      "Image 192 - Predicted: 9, Actual: 9\n",
      "Image 193 - Predicted: 8, Actual: 7\n",
      "Image 194 - Predicted: 3, Actual: 3\n",
      "Image 195 - Predicted: 5, Actual: 5\n",
      "Image 196 - Predicted: 3, Actual: 3\n",
      "Image 197 - Predicted: 4, Actual: 4\n",
      "Image 198 - Predicted: 4, Actual: 4\n",
      "Image 199 - Predicted: 4, Actual: 4\n",
      "Image 200 - Predicted: 4, Actual: 4\n",
      "Image 201 - Predicted: 0, Actual: 0\n",
      "Image 202 - Predicted: 9, Actual: 7\n",
      "Image 203 - Predicted: 8, Actual: 8\n",
      "Image 204 - Predicted: 1, Actual: 0\n",
      "Image 205 - Predicted: 4, Actual: 8\n",
      "Image 206 - Predicted: 4, Actual: 4\n",
      "Image 207 - Predicted: 9, Actual: 7\n",
      "Image 208 - Predicted: 2, Actual: 2\n",
      "Image 209 - Predicted: 3, Actual: 3\n",
      "Image 210 - Predicted: 9, Actual: 9\n",
      "Image 211 - Predicted: 0, Actual: 0\n",
      "Image 212 - Predicted: 1, Actual: 1\n",
      "Image 213 - Predicted: 0, Actual: 0\n",
      "Image 214 - Predicted: 2, Actual: 2\n",
      "Image 215 - Predicted: 9, Actual: 9\n",
      "Image 216 - Predicted: 2, Actual: 2\n",
      "Image 217 - Predicted: 6, Actual: 6\n",
      "Image 218 - Predicted: 6, Actual: 6\n",
      "Image 219 - Predicted: 8, Actual: 8\n",
      "Image 220 - Predicted: 3, Actual: 3\n",
      "Image 221 - Predicted: 4, Actual: 4\n",
      "Image 222 - Predicted: 1, Actual: 1\n",
      "Image 223 - Predicted: 1, Actual: 1\n",
      "Image 224 - Predicted: 6, Actual: 6\n",
      "Image 225 - Predicted: 3, Actual: 3\n",
      "Image 226 - Predicted: 1, Actual: 1\n",
      "Image 227 - Predicted: 2, Actual: 3\n",
      "Image 228 - Predicted: 1, Actual: 1\n",
      "Image 229 - Predicted: 7, Actual: 7\n",
      "Image 230 - Predicted: 0, Actual: 0\n",
      "Image 231 - Predicted: 6, Actual: 6\n",
      "Image 232 - Predicted: 4, Actual: 4\n",
      "Image 233 - Predicted: 8, Actual: 4\n",
      "Image 234 - Predicted: 9, Actual: 9\n",
      "Image 235 - Predicted: 5, Actual: 5\n",
      "Image 236 - Predicted: 0, Actual: 0\n",
      "Image 237 - Predicted: 7, Actual: 8\n",
      "Image 238 - Predicted: 7, Actual: 7\n",
      "Image 239 - Predicted: 2, Actual: 2\n",
      "Image 240 - Predicted: 2, Actual: 2\n",
      "Image 241 - Predicted: 8, Actual: 8\n",
      "Image 242 - Predicted: 9, Actual: 9\n",
      "Image 243 - Predicted: 1, Actual: 1\n",
      "Image 244 - Predicted: 6, Actual: 6\n",
      "Image 245 - Predicted: 1, Actual: 1\n",
      "Image 246 - Predicted: 7, Actual: 7\n",
      "Image 247 - Predicted: 0, Actual: 0\n",
      "Image 248 - Predicted: 3, Actual: 3\n",
      "Image 249 - Predicted: 9, Actual: 9\n",
      "Image 250 - Predicted: 1, Actual: 1\n",
      "Image 251 - Predicted: 9, Actual: 8\n",
      "Image 252 - Predicted: 0, Actual: 0\n",
      "Image 253 - Predicted: 2, Actual: 2\n",
      "Image 254 - Predicted: 7, Actual: 7\n",
      "Image 255 - Predicted: 0, Actual: 0\n",
      "Image 256 - Predicted: 9, Actual: 9\n",
      "Image 257 - Predicted: 2, Actual: 2\n",
      "Image 258 - Predicted: 0, Actual: 0\n",
      "Image 259 - Predicted: 1, Actual: 1\n",
      "Image 260 - Predicted: 2, Actual: 6\n",
      "Image 261 - Predicted: 4, Actual: 4\n",
      "Image 262 - Predicted: 2, Actual: 2\n",
      "Image 263 - Predicted: 5, Actual: 5\n",
      "Image 264 - Predicted: 1, Actual: 1\n",
      "Image 265 - Predicted: 0, Actual: 0\n",
      "Image 266 - Predicted: 8, Actual: 8\n",
      "Image 267 - Predicted: 7, Actual: 7\n",
      "Image 268 - Predicted: 8, Actual: 8\n",
      "Image 269 - Predicted: 4, Actual: 4\n",
      "Image 270 - Predicted: 5, Actual: 5\n",
      "Image 271 - Predicted: 6, Actual: 6\n",
      "Image 272 - Predicted: 8, Actual: 8\n",
      "Image 273 - Predicted: 2, Actual: 2\n",
      "Image 274 - Predicted: 2, Actual: 2\n",
      "Image 275 - Predicted: 1, Actual: 1\n",
      "Image 276 - Predicted: 4, Actual: 4\n",
      "Image 277 - Predicted: 7, Actual: 7\n",
      "Image 278 - Predicted: 8, Actual: 8\n",
      "Image 279 - Predicted: 1, Actual: 1\n",
      "Image 280 - Predicted: 5, Actual: 5\n",
      "Image 281 - Predicted: 5, Actual: 5\n",
      "Image 282 - Predicted: 7, Actual: 9\n",
      "Image 283 - Predicted: 5, Actual: 5\n",
      "Image 284 - Predicted: 9, Actual: 9\n",
      "Image 285 - Predicted: 2, Actual: 2\n",
      "Image 286 - Predicted: 0, Actual: 0\n",
      "Image 287 - Predicted: 7, Actual: 7\n",
      "Image 288 - Predicted: 6, Actual: 6\n",
      "Image 289 - Predicted: 1, Actual: 1\n",
      "Image 290 - Predicted: 8, Actual: 8\n",
      "Image 291 - Predicted: 8, Actual: 8\n",
      "Image 292 - Predicted: 4, Actual: 4\n",
      "Image 293 - Predicted: 9, Actual: 4\n",
      "Image 294 - Predicted: 0, Actual: 0\n",
      "Image 295 - Predicted: 2, Actual: 2\n",
      "Image 296 - Predicted: 4, Actual: 4\n",
      "Image 297 - Predicted: 9, Actual: 8\n",
      "Image 298 - Predicted: 4, Actual: 4\n",
      "Image 299 - Predicted: 8, Actual: 4\n",
      "Image 300 - Predicted: 2, Actual: 2\n",
      "Image 301 - Predicted: 0, Actual: 0\n",
      "Image 302 - Predicted: 0, Actual: 1\n",
      "Image 303 - Predicted: 8, Actual: 8\n",
      "Image 304 - Predicted: 7, Actual: 3\n",
      "Image 305 - Predicted: 5, Actual: 5\n",
      "Image 306 - Predicted: 6, Actual: 6\n",
      "Image 307 - Predicted: 0, Actual: 0\n",
      "Image 308 - Predicted: 3, Actual: 3\n",
      "Image 309 - Predicted: 9, Actual: 9\n",
      "Image 310 - Predicted: 1, Actual: 1\n",
      "Image 311 - Predicted: 2, Actual: 2\n",
      "Image 312 - Predicted: 7, Actual: 7\n",
      "Image 313 - Predicted: 7, Actual: 7\n",
      "Image 314 - Predicted: 7, Actual: 8\n",
      "Image 315 - Predicted: 9, Actual: 9\n",
      "Image 316 - Predicted: 9, Actual: 9\n",
      "Image 317 - Predicted: 4, Actual: 8\n",
      "Image 318 - Predicted: 2, Actual: 6\n",
      "Image 319 - Predicted: 1, Actual: 1\n",
      "Image 320 - Predicted: 2, Actual: 2\n",
      "Image 321 - Predicted: 4, Actual: 4\n",
      "Image 322 - Predicted: 4, Actual: 4\n",
      "Image 323 - Predicted: 2, Actual: 6\n",
      "Image 324 - Predicted: 6, Actual: 6\n",
      "Image 325 - Predicted: 9, Actual: 4\n",
      "Image 326 - Predicted: 9, Actual: 8\n",
      "Image 327 - Predicted: 6, Actual: 6\n",
      "Image 328 - Predicted: 1, Actual: 1\n",
      "Image 329 - Predicted: 5, Actual: 5\n",
      "Image 330 - Predicted: 6, Actual: 6\n",
      "Image 331 - Predicted: 6, Actual: 6\n",
      "Image 332 - Predicted: 5, Actual: 5\n",
      "Image 333 - Predicted: 3, Actual: 3\n",
      "Image 334 - Predicted: 5, Actual: 5\n",
      "Image 335 - Predicted: 0, Actual: 0\n",
      "Image 336 - Predicted: 8, Actual: 8\n",
      "Image 337 - Predicted: 1, Actual: 1\n",
      "Image 338 - Predicted: 7, Actual: 9\n",
      "Image 339 - Predicted: 1, Actual: 1\n",
      "Image 340 - Predicted: 3, Actual: 3\n",
      "Image 341 - Predicted: 9, Actual: 9\n",
      "Image 342 - Predicted: 4, Actual: 4\n",
      "Image 343 - Predicted: 1, Actual: 1\n",
      "Image 344 - Predicted: 3, Actual: 3\n",
      "Image 345 - Predicted: 6, Actual: 6\n",
      "Image 346 - Predicted: 8, Actual: 9\n",
      "Image 347 - Predicted: 5, Actual: 5\n",
      "Image 348 - Predicted: 6, Actual: 6\n",
      "Image 349 - Predicted: 5, Actual: 4\n",
      "Image 350 - Predicted: 4, Actual: 4\n",
      "Image 351 - Predicted: 8, Actual: 8\n",
      "Image 352 - Predicted: 9, Actual: 9\n",
      "Image 353 - Predicted: 8, Actual: 8\n",
      "Image 354 - Predicted: 4, Actual: 4\n",
      "Image 355 - Predicted: 4, Actual: 4\n",
      "Image 356 - Predicted: 7, Actual: 8\n",
      "Image 357 - Predicted: 1, Actual: 1\n",
      "Image 358 - Predicted: 0, Actual: 0\n",
      "Image 359 - Predicted: 0, Actual: 7\n",
      "Image 360 - Predicted: 3, Actual: 3\n",
      "Image 361 - Predicted: 7, Actual: 9\n",
      "Image 362 - Predicted: 2, Actual: 2\n",
      "Image 363 - Predicted: 8, Actual: 8\n",
      "Image 364 - Predicted: 6, Actual: 6\n",
      "Image 365 - Predicted: 3, Actual: 3\n",
      "Image 366 - Predicted: 1, Actual: 1\n",
      "Image 367 - Predicted: 7, Actual: 7\n",
      "Image 368 - Predicted: 2, Actual: 2\n",
      "Image 369 - Predicted: 7, Actual: 3\n",
      "Image 370 - Predicted: 7, Actual: 9\n",
      "Image 371 - Predicted: 0, Actual: 0\n",
      "Image 372 - Predicted: 1, Actual: 1\n",
      "Image 373 - Predicted: 4, Actual: 4\n",
      "Image 374 - Predicted: 7, Actual: 9\n",
      "Image 375 - Predicted: 9, Actual: 8\n",
      "Image 376 - Predicted: 6, Actual: 6\n",
      "Image 377 - Predicted: 0, Actual: 0\n",
      "Image 378 - Predicted: 5, Actual: 5\n",
      "Image 379 - Predicted: 9, Actual: 9\n",
      "Image 380 - Predicted: 6, Actual: 6\n",
      "Image 381 - Predicted: 3, Actual: 3\n",
      "Image 382 - Predicted: 7, Actual: 7\n",
      "Image 383 - Predicted: 5, Actual: 5\n",
      "Image 384 - Predicted: 4, Actual: 5\n",
      "Image 385 - Predicted: 4, Actual: 4\n",
      "Image 386 - Predicted: 3, Actual: 3\n",
      "Image 387 - Predicted: 9, Actual: 9\n",
      "Image 388 - Predicted: 3, Actual: 3\n",
      "Image 389 - Predicted: 5, Actual: 8\n",
      "Image 390 - Predicted: 8, Actual: 8\n",
      "Image 391 - Predicted: 5, Actual: 5\n",
      "Image 392 - Predicted: 5, Actual: 4\n",
      "Image 393 - Predicted: 1, Actual: 0\n",
      "Image 394 - Predicted: 1, Actual: 1\n",
      "Image 395 - Predicted: 4, Actual: 4\n",
      "Image 396 - Predicted: 2, Actual: 2\n",
      "Image 397 - Predicted: 0, Actual: 0\n",
      "Image 398 - Predicted: 1, Actual: 1\n",
      "Image 399 - Predicted: 7, Actual: 7\n",
      "Image 400 - Predicted: 2, Actual: 2\n",
      "Image 401 - Predicted: 5, Actual: 5\n",
      "Image 402 - Predicted: 5, Actual: 5\n",
      "Image 403 - Predicted: 0, Actual: 0\n",
      "Image 404 - Predicted: 8, Actual: 8\n",
      "Image 405 - Predicted: 3, Actual: 1\n",
      "Image 406 - Predicted: 7, Actual: 8\n",
      "Image 407 - Predicted: 4, Actual: 4\n",
      "Image 408 - Predicted: 5, Actual: 4\n",
      "Image 409 - Predicted: 8, Actual: 8\n",
      "Image 410 - Predicted: 3, Actual: 3\n",
      "Image 411 - Predicted: 8, Actual: 4\n",
      "Image 412 - Predicted: 7, Actual: 7\n",
      "Image 413 - Predicted: 4, Actual: 8\n",
      "Image 414 - Predicted: 4, Actual: 5\n",
      "Image 415 - Predicted: 9, Actual: 9\n",
      "Image 416 - Predicted: 9, Actual: 9\n",
      "Image 417 - Predicted: 3, Actual: 3\n",
      "Image 418 - Predicted: 1, Actual: 6\n",
      "Image 419 - Predicted: 4, Actual: 4\n",
      "Image 420 - Predicted: 5, Actual: 5\n",
      "Image 421 - Predicted: 8, Actual: 8\n",
      "Image 422 - Predicted: 6, Actual: 6\n",
      "Image 423 - Predicted: 3, Actual: 3\n",
      "Image 424 - Predicted: 9, Actual: 9\n",
      "Image 425 - Predicted: 1, Actual: 1\n",
      "Image 426 - Predicted: 2, Actual: 2\n",
      "Image 427 - Predicted: 3, Actual: 3\n",
      "Image 428 - Predicted: 3, Actual: 3\n",
      "Image 429 - Predicted: 0, Actual: 1\n",
      "Image 430 - Predicted: 5, Actual: 5\n",
      "Image 431 - Predicted: 4, Actual: 4\n",
      "Image 432 - Predicted: 7, Actual: 7\n",
      "Image 433 - Predicted: 4, Actual: 4\n",
      "Image 434 - Predicted: 3, Actual: 3\n",
      "Image 435 - Predicted: 6, Actual: 6\n",
      "Image 436 - Predicted: 0, Actual: 0\n",
      "Image 437 - Predicted: 1, Actual: 1\n",
      "Image 438 - Predicted: 4, Actual: 4\n",
      "Image 439 - Predicted: 6, Actual: 6\n",
      "Image 440 - Predicted: 0, Actual: 0\n",
      "Image 441 - Predicted: 5, Actual: 5\n",
      "Image 442 - Predicted: 7, Actual: 7\n",
      "Image 443 - Predicted: 7, Actual: 7\n",
      "Image 444 - Predicted: 9, Actual: 9\n",
      "Image 445 - Predicted: 9, Actual: 9\n",
      "Image 446 - Predicted: 4, Actual: 4\n",
      "Image 447 - Predicted: 7, Actual: 7\n",
      "Image 448 - Predicted: 7, Actual: 7\n",
      "Image 449 - Predicted: 0, Actual: 0\n",
      "Image 450 - Predicted: 5, Actual: 5\n",
      "Image 451 - Predicted: 2, Actual: 2\n",
      "Image 452 - Predicted: 0, Actual: 1\n",
      "Image 453 - Predicted: 5, Actual: 5\n",
      "Image 454 - Predicted: 7, Actual: 7\n",
      "Image 455 - Predicted: 4, Actual: 8\n",
      "Image 456 - Predicted: 2, Actual: 2\n",
      "Image 457 - Predicted: 5, Actual: 5\n",
      "Image 458 - Predicted: 7, Actual: 7\n",
      "Image 459 - Predicted: 9, Actual: 9\n",
      "Image 460 - Predicted: 8, Actual: 8\n",
      "Image 461 - Predicted: 8, Actual: 8\n",
      "Image 462 - Predicted: 5, Actual: 5\n",
      "Image 463 - Predicted: 7, Actual: 7\n",
      "Image 464 - Predicted: 4, Actual: 4\n",
      "Image 465 - Predicted: 2, Actual: 6\n",
      "Image 466 - Predicted: 1, Actual: 1\n",
      "Image 467 - Predicted: 7, Actual: 3\n",
      "Image 468 - Predicted: 1, Actual: 1\n",
      "Image 469 - Predicted: 4, Actual: 4\n",
      "Image 470 - Predicted: 4, Actual: 4\n",
      "Image 471 - Predicted: 5, Actual: 4\n",
      "Image 472 - Predicted: 9, Actual: 9\n",
      "Image 473 - Predicted: 8, Actual: 8\n",
      "Image 474 - Predicted: 9, Actual: 4\n",
      "Image 475 - Predicted: 3, Actual: 3\n",
      "Image 476 - Predicted: 9, Actual: 9\n",
      "Image 477 - Predicted: 8, Actual: 9\n",
      "Image 478 - Predicted: 9, Actual: 9\n",
      "Image 479 - Predicted: 0, Actual: 0\n",
      "Image 480 - Predicted: 6, Actual: 1\n",
      "Image 481 - Predicted: 9, Actual: 9\n",
      "Image 482 - Predicted: 9, Actual: 9\n",
      "Image 483 - Predicted: 0, Actual: 0\n",
      "Image 484 - Predicted: 8, Actual: 8\n",
      "Image 485 - Predicted: 1, Actual: 1\n",
      "Image 486 - Predicted: 0, Actual: 0\n",
      "Image 487 - Predicted: 4, Actual: 4\n",
      "Image 488 - Predicted: 4, Actual: 4\n",
      "Image 489 - Predicted: 6, Actual: 6\n",
      "Image 490 - Predicted: 4, Actual: 4\n",
      "Image 491 - Predicted: 3, Actual: 3\n",
      "Image 492 - Predicted: 6, Actual: 6\n",
      "Image 493 - Predicted: 0, Actual: 0\n",
      "Image 494 - Predicted: 7, Actual: 7\n",
      "Image 495 - Predicted: 7, Actual: 7\n",
      "Image 496 - Predicted: 1, Actual: 1\n",
      "Image 497 - Predicted: 7, Actual: 7\n",
      "Image 498 - Predicted: 9, Actual: 9\n",
      "Image 499 - Predicted: 4, Actual: 4\n",
      "Image 500 - Predicted: 3, Actual: 3\n",
      "Image 501 - Predicted: 7, Actual: 7\n",
      "Image 502 - Predicted: 2, Actual: 2\n",
      "Image 503 - Predicted: 6, Actual: 6\n",
      "Image 504 - Predicted: 9, Actual: 7\n",
      "Image 505 - Predicted: 2, Actual: 2\n",
      "Image 506 - Predicted: 7, Actual: 7\n",
      "Image 507 - Predicted: 1, Actual: 1\n",
      "Image 508 - Predicted: 9, Actual: 4\n",
      "Image 509 - Predicted: 4, Actual: 4\n",
      "Image 510 - Predicted: 2, Actual: 2\n",
      "Image 511 - Predicted: 8, Actual: 8\n",
      "Image 512 - Predicted: 3, Actual: 3\n",
      "Image 513 - Predicted: 5, Actual: 5\n",
      "Image 514 - Predicted: 2, Actual: 2\n",
      "Image 515 - Predicted: 0, Actual: 0\n",
      "Image 516 - Predicted: 3, Actual: 3\n",
      "Image 517 - Predicted: 1, Actual: 1\n",
      "Image 518 - Predicted: 1, Actual: 1\n",
      "Image 519 - Predicted: 3, Actual: 3\n",
      "Image 520 - Predicted: 8, Actual: 8\n",
      "Image 521 - Predicted: 7, Actual: 7\n",
      "Image 522 - Predicted: 9, Actual: 9\n",
      "Image 523 - Predicted: 4, Actual: 8\n",
      "Image 524 - Predicted: 2, Actual: 2\n",
      "Image 525 - Predicted: 1, Actual: 1\n",
      "Image 526 - Predicted: 1, Actual: 1\n",
      "Image 527 - Predicted: 4, Actual: 8\n",
      "Image 528 - Predicted: 2, Actual: 2\n",
      "Image 529 - Predicted: 4, Actual: 4\n",
      "Image 530 - Predicted: 7, Actual: 7\n",
      "Image 531 - Predicted: 0, Actual: 1\n",
      "Image 532 - Predicted: 3, Actual: 5\n",
      "Image 533 - Predicted: 3, Actual: 3\n",
      "Image 534 - Predicted: 4, Actual: 8\n",
      "Image 535 - Predicted: 8, Actual: 8\n",
      "Image 536 - Predicted: 2, Actual: 2\n",
      "Image 537 - Predicted: 8, Actual: 4\n",
      "Image 538 - Predicted: 9, Actual: 7\n",
      "Image 539 - Predicted: 8, Actual: 4\n",
      "Image 540 - Predicted: 3, Actual: 3\n",
      "Image 541 - Predicted: 0, Actual: 0\n",
      "Image 542 - Predicted: 9, Actual: 9\n",
      "Image 543 - Predicted: 6, Actual: 2\n",
      "Image 544 - Predicted: 7, Actual: 7\n",
      "Image 545 - Predicted: 7, Actual: 7\n",
      "Image 546 - Predicted: 0, Actual: 0\n",
      "Image 547 - Predicted: 4, Actual: 4\n",
      "Image 548 - Predicted: 4, Actual: 9\n",
      "Image 549 - Predicted: 5, Actual: 5\n",
      "Image 550 - Predicted: 2, Actual: 2\n",
      "Image 551 - Predicted: 8, Actual: 7\n",
      "Image 552 - Predicted: 3, Actual: 3\n",
      "Image 553 - Predicted: 5, Actual: 5\n",
      "Image 554 - Predicted: 0, Actual: 0\n",
      "Image 555 - Predicted: 0, Actual: 0\n",
      "Image 556 - Predicted: 9, Actual: 9\n",
      "Image 557 - Predicted: 5, Actual: 5\n",
      "Image 558 - Predicted: 7, Actual: 7\n",
      "Image 559 - Predicted: 0, Actual: 0\n",
      "Image 560 - Predicted: 9, Actual: 9\n",
      "Image 561 - Predicted: 8, Actual: 8\n",
      "Image 562 - Predicted: 0, Actual: 0\n",
      "Image 563 - Predicted: 1, Actual: 1\n",
      "Image 564 - Predicted: 6, Actual: 6\n",
      "Image 565 - Predicted: 1, Actual: 1\n",
      "Image 566 - Predicted: 5, Actual: 5\n",
      "Image 567 - Predicted: 3, Actual: 3\n",
      "Image 568 - Predicted: 5, Actual: 8\n",
      "Image 569 - Predicted: 2, Actual: 2\n",
      "Image 570 - Predicted: 9, Actual: 8\n",
      "Image 571 - Predicted: 7, Actual: 7\n",
      "Image 572 - Predicted: 0, Actual: 0\n",
      "Image 573 - Predicted: 6, Actual: 6\n",
      "Image 574 - Predicted: 7, Actual: 7\n",
      "Image 575 - Predicted: 6, Actual: 6\n",
      "Image 576 - Predicted: 6, Actual: 6\n",
      "Image 577 - Predicted: 9, Actual: 9\n",
      "Image 578 - Predicted: 5, Actual: 5\n",
      "Image 579 - Predicted: 5, Actual: 5\n",
      "Image 580 - Predicted: 7, Actual: 8\n",
      "Image 581 - Predicted: 0, Actual: 0\n",
      "Image 582 - Predicted: 9, Actual: 9\n",
      "Image 583 - Predicted: 7, Actual: 7\n",
      "Image 584 - Predicted: 9, Actual: 9\n",
      "Image 585 - Predicted: 4, Actual: 4\n",
      "Image 586 - Predicted: 8, Actual: 8\n",
      "Image 587 - Predicted: 2, Actual: 2\n",
      "Image 588 - Predicted: 9, Actual: 9\n",
      "Image 589 - Predicted: 9, Actual: 9\n",
      "Image 590 - Predicted: 3, Actual: 1\n",
      "Image 591 - Predicted: 3, Actual: 3\n",
      "Image 592 - Predicted: 1, Actual: 1\n",
      "Image 593 - Predicted: 3, Actual: 3\n",
      "Image 594 - Predicted: 2, Actual: 2\n",
      "Image 595 - Predicted: 3, Actual: 3\n",
      "Image 596 - Predicted: 3, Actual: 3\n",
      "Image 597 - Predicted: 4, Actual: 4\n",
      "Image 598 - Predicted: 9, Actual: 8\n",
      "Image 599 - Predicted: 9, Actual: 4\n",
      "Image 600 - Predicted: 8, Actual: 4\n",
      "Image 601 - Predicted: 8, Actual: 8\n",
      "Image 602 - Predicted: 6, Actual: 6\n",
      "Image 603 - Predicted: 2, Actual: 2\n",
      "Image 604 - Predicted: 2, Actual: 2\n",
      "Image 605 - Predicted: 0, Actual: 0\n",
      "Image 606 - Predicted: 1, Actual: 1\n",
      "Image 607 - Predicted: 6, Actual: 2\n",
      "Image 608 - Predicted: 7, Actual: 7\n",
      "Image 609 - Predicted: 4, Actual: 4\n",
      "Image 610 - Predicted: 9, Actual: 9\n",
      "Image 611 - Predicted: 7, Actual: 7\n",
      "Image 612 - Predicted: 6, Actual: 6\n",
      "Image 613 - Predicted: 6, Actual: 6\n",
      "Image 614 - Predicted: 7, Actual: 7\n",
      "Image 615 - Predicted: 1, Actual: 1\n",
      "Image 616 - Predicted: 5, Actual: 5\n",
      "Image 617 - Predicted: 0, Actual: 0\n",
      "Image 618 - Predicted: 3, Actual: 3\n",
      "Image 619 - Predicted: 8, Actual: 8\n",
      "Image 620 - Predicted: 6, Actual: 6\n",
      "Image 621 - Predicted: 3, Actual: 3\n",
      "Image 622 - Predicted: 8, Actual: 8\n",
      "Image 623 - Predicted: 5, Actual: 5\n",
      "Image 624 - Predicted: 7, Actual: 7\n",
      "Image 625 - Predicted: 5, Actual: 5\n",
      "Image 626 - Predicted: 6, Actual: 6\n",
      "Image 627 - Predicted: 0, Actual: 1\n",
      "Image 628 - Predicted: 9, Actual: 7\n",
      "Image 629 - Predicted: 4, Actual: 5\n",
      "Image 630 - Predicted: 5, Actual: 5\n",
      "Image 631 - Predicted: 5, Actual: 5\n",
      "Image 632 - Predicted: 4, Actual: 9\n",
      "Image 633 - Predicted: 3, Actual: 3\n",
      "Image 634 - Predicted: 7, Actual: 7\n",
      "Image 635 - Predicted: 4, Actual: 4\n",
      "Image 636 - Predicted: 8, Actual: 8\n",
      "Image 637 - Predicted: 0, Actual: 0\n",
      "Image 638 - Predicted: 6, Actual: 6\n",
      "Image 639 - Predicted: 3, Actual: 3\n",
      "Image 640 - Predicted: 6, Actual: 2\n",
      "Image 641 - Predicted: 1, Actual: 1\n",
      "Image 642 - Predicted: 2, Actual: 2\n",
      "Image 643 - Predicted: 0, Actual: 0\n",
      "Image 644 - Predicted: 2, Actual: 6\n",
      "Image 645 - Predicted: 2, Actual: 2\n",
      "Image 646 - Predicted: 9, Actual: 9\n",
      "Image 647 - Predicted: 3, Actual: 3\n",
      "Image 648 - Predicted: 2, Actual: 2\n",
      "Image 649 - Predicted: 5, Actual: 5\n",
      "Image 650 - Predicted: 2, Actual: 2\n",
      "Image 651 - Predicted: 8, Actual: 8\n",
      "Image 652 - Predicted: 7, Actual: 7\n",
      "Image 653 - Predicted: 3, Actual: 3\n",
      "Image 654 - Predicted: 7, Actual: 7\n",
      "Image 655 - Predicted: 0, Actual: 0\n",
      "Image 656 - Predicted: 9, Actual: 9\n",
      "Image 657 - Predicted: 0, Actual: 0\n",
      "Image 658 - Predicted: 5, Actual: 5\n",
      "Image 659 - Predicted: 5, Actual: 5\n",
      "Image 660 - Predicted: 5, Actual: 5\n",
      "Image 661 - Predicted: 8, Actual: 4\n",
      "Image 662 - Predicted: 7, Actual: 4\n",
      "Image 663 - Predicted: 7, Actual: 7\n",
      "Image 664 - Predicted: 7, Actual: 7\n",
      "Image 665 - Predicted: 9, Actual: 9\n",
      "Image 666 - Predicted: 9, Actual: 9\n",
      "Image 667 - Predicted: 6, Actual: 6\n",
      "Image 668 - Predicted: 9, Actual: 4\n",
      "Image 669 - Predicted: 4, Actual: 8\n",
      "Image 670 - Predicted: 5, Actual: 5\n",
      "Image 671 - Predicted: 3, Actual: 3\n",
      "Image 672 - Predicted: 8, Actual: 8\n",
      "Image 673 - Predicted: 5, Actual: 5\n",
      "Image 674 - Predicted: 0, Actual: 0\n",
      "Image 675 - Predicted: 9, Actual: 9\n",
      "Image 676 - Predicted: 7, Actual: 7\n",
      "Image 677 - Predicted: 2, Actual: 2\n",
      "Image 678 - Predicted: 6, Actual: 6\n",
      "Image 679 - Predicted: 0, Actual: 0\n",
      "Image 680 - Predicted: 7, Actual: 7\n",
      "Image 681 - Predicted: 8, Actual: 7\n",
      "Image 682 - Predicted: 2, Actual: 2\n",
      "Image 683 - Predicted: 8, Actual: 7\n",
      "Image 684 - Predicted: 3, Actual: 3\n",
      "Image 685 - Predicted: 0, Actual: 0\n",
      "Image 686 - Predicted: 9, Actual: 9\n",
      "Image 687 - Predicted: 0, Actual: 0\n",
      "Image 688 - Predicted: 4, Actual: 4\n",
      "Image 689 - Predicted: 1, Actual: 1\n",
      "Image 690 - Predicted: 4, Actual: 4\n",
      "Image 691 - Predicted: 5, Actual: 4\n",
      "Image 692 - Predicted: 6, Actual: 6\n",
      "Image 693 - Predicted: 2, Actual: 2\n",
      "Image 694 - Predicted: 9, Actual: 9\n",
      "Image 695 - Predicted: 1, Actual: 1\n",
      "Image 696 - Predicted: 7, Actual: 7\n",
      "Image 697 - Predicted: 3, Actual: 3\n",
      "Image 698 - Predicted: 7, Actual: 7\n",
      "Image 699 - Predicted: 3, Actual: 3\n",
      "Image 700 - Predicted: 6, Actual: 6\n",
      "Image 701 - Predicted: 6, Actual: 6\n",
      "Image 702 - Predicted: 1, Actual: 1\n",
      "Image 703 - Predicted: 5, Actual: 5\n",
      "Image 704 - Predicted: 0, Actual: 0\n",
      "Image 705 - Predicted: 5, Actual: 5\n",
      "Image 706 - Predicted: 4, Actual: 8\n",
      "Image 707 - Predicted: 6, Actual: 6\n",
      "Image 708 - Predicted: 2, Actual: 2\n",
      "Image 709 - Predicted: 2, Actual: 2\n",
      "Image 710 - Predicted: 5, Actual: 5\n",
      "Image 711 - Predicted: 8, Actual: 8\n",
      "Image 712 - Predicted: 4, Actual: 4\n",
      "Image 713 - Predicted: 0, Actual: 0\n",
      "Image 714 - Predicted: 4, Actual: 4\n",
      "Image 715 - Predicted: 3, Actual: 7\n",
      "Image 716 - Predicted: 6, Actual: 6\n",
      "Image 717 - Predicted: 3, Actual: 3\n",
      "Image 718 - Predicted: 7, Actual: 7\n",
      "Image 719 - Predicted: 0, Actual: 0\n",
      "Image 720 - Predicted: 8, Actual: 8\n",
      "Image 721 - Predicted: 5, Actual: 5\n",
      "Image 722 - Predicted: 0, Actual: 0\n",
      "Image 723 - Predicted: 2, Actual: 2\n",
      "Image 724 - Predicted: 9, Actual: 9\n",
      "Image 725 - Predicted: 3, Actual: 3\n",
      "Image 726 - Predicted: 8, Actual: 8\n",
      "Image 727 - Predicted: 5, Actual: 5\n",
      "Image 728 - Predicted: 7, Actual: 7\n",
      "Image 729 - Predicted: 2, Actual: 2\n",
      "Image 730 - Predicted: 7, Actual: 2\n",
      "Image 731 - Predicted: 7, Actual: 7\n",
      "Image 732 - Predicted: 3, Actual: 3\n",
      "Image 733 - Predicted: 6, Actual: 6\n",
      "Image 734 - Predicted: 5, Actual: 5\n",
      "Image 735 - Predicted: 5, Actual: 5\n",
      "Image 736 - Predicted: 7, Actual: 7\n",
      "Image 737 - Predicted: 1, Actual: 1\n",
      "Image 738 - Predicted: 8, Actual: 8\n",
      "Image 739 - Predicted: 4, Actual: 5\n",
      "Image 740 - Predicted: 2, Actual: 2\n",
      "Image 741 - Predicted: 5, Actual: 5\n",
      "Image 742 - Predicted: 6, Actual: 6\n",
      "Image 743 - Predicted: 8, Actual: 4\n",
      "Image 744 - Predicted: 5, Actual: 5\n",
      "Image 745 - Predicted: 7, Actual: 7\n",
      "Image 746 - Predicted: 4, Actual: 4\n",
      "Image 747 - Predicted: 0, Actual: 0\n",
      "Image 748 - Predicted: 0, Actual: 0\n",
      "Image 749 - Predicted: 2, Actual: 2\n",
      "Image 750 - Predicted: 1, Actual: 1\n",
      "Image 751 - Predicted: 2, Actual: 2\n",
      "Image 752 - Predicted: 9, Actual: 9\n",
      "Image 753 - Predicted: 0, Actual: 1\n",
      "Image 754 - Predicted: 1, Actual: 1\n",
      "Image 755 - Predicted: 5, Actual: 4\n",
      "Image 756 - Predicted: 4, Actual: 4\n",
      "Image 757 - Predicted: 7, Actual: 7\n",
      "Image 758 - Predicted: 6, Actual: 2\n",
      "Image 759 - Predicted: 0, Actual: 0\n",
      "Image 760 - Predicted: 0, Actual: 0\n",
      "Image 761 - Predicted: 3, Actual: 3\n",
      "Image 762 - Predicted: 2, Actual: 2\n",
      "Image 763 - Predicted: 0, Actual: 0\n",
      "Image 764 - Predicted: 1, Actual: 1\n",
      "Image 765 - Predicted: 1, Actual: 1\n",
      "Image 766 - Predicted: 7, Actual: 7\n",
      "Image 767 - Predicted: 1, Actual: 0\n",
      "Image 768 - Predicted: 5, Actual: 5\n",
      "Image 769 - Predicted: 9, Actual: 9\n",
      "Image 770 - Predicted: 8, Actual: 8\n",
      "Image 771 - Predicted: 2, Actual: 2\n",
      "Image 772 - Predicted: 7, Actual: 9\n",
      "Image 773 - Predicted: 1, Actual: 1\n",
      "Image 774 - Predicted: 2, Actual: 2\n",
      "Image 775 - Predicted: 3, Actual: 3\n",
      "Image 776 - Predicted: 3, Actual: 3\n",
      "Image 777 - Predicted: 2, Actual: 2\n",
      "Image 778 - Predicted: 4, Actual: 4\n",
      "Image 779 - Predicted: 9, Actual: 9\n",
      "Image 780 - Predicted: 4, Actual: 4\n",
      "Image 781 - Predicted: 8, Actual: 8\n",
      "Image 782 - Predicted: 6, Actual: 6\n",
      "Image 783 - Predicted: 0, Actual: 0\n",
      "Image 784 - Predicted: 3, Actual: 3\n",
      "Image 785 - Predicted: 0, Actual: 0\n",
      "Image 786 - Predicted: 8, Actual: 4\n",
      "Image 787 - Predicted: 3, Actual: 3\n",
      "Image 788 - Predicted: 2, Actual: 2\n",
      "Image 789 - Predicted: 2, Actual: 2\n",
      "Image 790 - Predicted: 7, Actual: 8\n",
      "Image 791 - Predicted: 1, Actual: 1\n",
      "Image 792 - Predicted: 4, Actual: 4\n",
      "Image 793 - Predicted: 8, Actual: 8\n",
      "Image 794 - Predicted: 0, Actual: 0\n",
      "Image 795 - Predicted: 8, Actual: 8\n",
      "Image 796 - Predicted: 0, Actual: 0\n",
      "Image 797 - Predicted: 0, Actual: 0\n",
      "Image 798 - Predicted: 9, Actual: 9\n",
      "Image 799 - Predicted: 2, Actual: 2\n",
      "Image 800 - Predicted: 0, Actual: 0\n",
      "Image 801 - Predicted: 9, Actual: 9\n",
      "Image 802 - Predicted: 3, Actual: 3\n",
      "Image 803 - Predicted: 8, Actual: 8\n",
      "Image 804 - Predicted: 9, Actual: 9\n",
      "Image 805 - Predicted: 9, Actual: 9\n",
      "Image 806 - Predicted: 6, Actual: 6\n",
      "Image 807 - Predicted: 1, Actual: 1\n",
      "Image 808 - Predicted: 0, Actual: 0\n",
      "Image 809 - Predicted: 3, Actual: 1\n",
      "Image 810 - Predicted: 8, Actual: 8\n",
      "Image 811 - Predicted: 2, Actual: 2\n",
      "Image 812 - Predicted: 2, Actual: 2\n",
      "Image 813 - Predicted: 4, Actual: 4\n",
      "Image 814 - Predicted: 4, Actual: 4\n",
      "Image 815 - Predicted: 3, Actual: 3\n",
      "Image 816 - Predicted: 3, Actual: 3\n",
      "Image 817 - Predicted: 9, Actual: 9\n",
      "Image 818 - Predicted: 8, Actual: 8\n",
      "Image 819 - Predicted: 5, Actual: 5\n",
      "Image 820 - Predicted: 3, Actual: 3\n",
      "Image 821 - Predicted: 6, Actual: 6\n",
      "Image 822 - Predicted: 5, Actual: 5\n",
      "Image 823 - Predicted: 9, Actual: 9\n",
      "Image 824 - Predicted: 8, Actual: 8\n",
      "Image 825 - Predicted: 5, Actual: 5\n",
      "Image 826 - Predicted: 9, Actual: 5\n",
      "Image 827 - Predicted: 2, Actual: 2\n",
      "Image 828 - Predicted: 4, Actual: 4\n",
      "Image 829 - Predicted: 7, Actual: 7\n",
      "Image 830 - Predicted: 4, Actual: 8\n",
      "Image 831 - Predicted: 9, Actual: 7\n",
      "Image 832 - Predicted: 4, Actual: 4\n",
      "Image 833 - Predicted: 9, Actual: 9\n",
      "Image 834 - Predicted: 9, Actual: 9\n",
      "Image 835 - Predicted: 5, Actual: 4\n",
      "Image 836 - Predicted: 9, Actual: 8\n",
      "Image 837 - Predicted: 3, Actual: 3\n",
      "Image 838 - Predicted: 3, Actual: 3\n",
      "Image 839 - Predicted: 9, Actual: 9\n",
      "Image 840 - Predicted: 6, Actual: 6\n",
      "Image 841 - Predicted: 5, Actual: 5\n",
      "Image 842 - Predicted: 9, Actual: 9\n",
      "Image 843 - Predicted: 4, Actual: 4\n",
      "Image 844 - Predicted: 5, Actual: 5\n",
      "Image 845 - Predicted: 1, Actual: 1\n",
      "Image 846 - Predicted: 2, Actual: 2\n",
      "Image 847 - Predicted: 5, Actual: 5\n",
      "Image 848 - Predicted: 7, Actual: 7\n",
      "Image 849 - Predicted: 9, Actual: 9\n",
      "Image 850 - Predicted: 6, Actual: 6\n",
      "Image 851 - Predicted: 6, Actual: 6\n",
      "Image 852 - Predicted: 2, Actual: 6\n",
      "Image 853 - Predicted: 3, Actual: 3\n",
      "Image 854 - Predicted: 6, Actual: 6\n",
      "Image 855 - Predicted: 5, Actual: 5\n",
      "Image 856 - Predicted: 9, Actual: 8\n",
      "Image 857 - Predicted: 1, Actual: 1\n",
      "Image 858 - Predicted: 5, Actual: 5\n",
      "Image 859 - Predicted: 3, Actual: 3\n",
      "Image 860 - Predicted: 9, Actual: 7\n",
      "Image 861 - Predicted: 8, Actual: 8\n",
      "Image 862 - Predicted: 0, Actual: 0\n",
      "Image 863 - Predicted: 4, Actual: 4\n",
      "Image 864 - Predicted: 7, Actual: 7\n",
      "Image 865 - Predicted: 7, Actual: 7\n",
      "Image 866 - Predicted: 6, Actual: 6\n",
      "Image 867 - Predicted: 9, Actual: 9\n",
      "Image 868 - Predicted: 0, Actual: 0\n",
      "Image 869 - Predicted: 9, Actual: 9\n",
      "Image 870 - Predicted: 6, Actual: 6\n",
      "Image 871 - Predicted: 7, Actual: 7\n",
      "Image 872 - Predicted: 3, Actual: 3\n",
      "Image 873 - Predicted: 4, Actual: 4\n",
      "Image 874 - Predicted: 5, Actual: 4\n",
      "Image 875 - Predicted: 6, Actual: 6\n",
      "Image 876 - Predicted: 3, Actual: 3\n",
      "Image 877 - Predicted: 4, Actual: 4\n",
      "Image 878 - Predicted: 5, Actual: 5\n",
      "Image 879 - Predicted: 3, Actual: 3\n",
      "Image 880 - Predicted: 5, Actual: 5\n",
      "Image 881 - Predicted: 0, Actual: 0\n",
      "Image 882 - Predicted: 3, Actual: 3\n",
      "Image 883 - Predicted: 7, Actual: 7\n",
      "Image 884 - Predicted: 5, Actual: 5\n",
      "Image 885 - Predicted: 9, Actual: 8\n",
      "Image 886 - Predicted: 9, Actual: 9\n",
      "Image 887 - Predicted: 8, Actual: 5\n",
      "Image 888 - Predicted: 1, Actual: 0\n",
      "Image 889 - Predicted: 1, Actual: 3\n",
      "Image 890 - Predicted: 6, Actual: 6\n",
      "Image 891 - Predicted: 8, Actual: 8\n",
      "Image 892 - Predicted: 8, Actual: 8\n",
      "Image 893 - Predicted: 9, Actual: 9\n",
      "Image 894 - Predicted: 4, Actual: 4\n",
      "Image 895 - Predicted: 5, Actual: 5\n",
      "Image 896 - Predicted: 6, Actual: 6\n",
      "Image 897 - Predicted: 9, Actual: 9\n",
      "Image 898 - Predicted: 6, Actual: 6\n",
      "Image 899 - Predicted: 0, Actual: 8\n",
      "Image 900 - Predicted: 0, Actual: 1\n",
      "Image 901 - Predicted: 5, Actual: 5\n",
      "Image 902 - Predicted: 3, Actual: 3\n",
      "Image 903 - Predicted: 2, Actual: 2\n",
      "Image 904 - Predicted: 6, Actual: 6\n",
      "Image 905 - Predicted: 2, Actual: 2\n",
      "Image 906 - Predicted: 7, Actual: 7\n",
      "Image 907 - Predicted: 7, Actual: 7\n",
      "Image 908 - Predicted: 2, Actual: 2\n",
      "Image 909 - Predicted: 4, Actual: 8\n",
      "Image 910 - Predicted: 0, Actual: 0\n",
      "Image 911 - Predicted: 6, Actual: 6\n",
      "Image 912 - Predicted: 5, Actual: 5\n",
      "Image 913 - Predicted: 0, Actual: 0\n",
      "Image 914 - Predicted: 8, Actual: 7\n",
      "Image 915 - Predicted: 0, Actual: 0\n",
      "Image 916 - Predicted: 7, Actual: 7\n",
      "Image 917 - Predicted: 9, Actual: 9\n",
      "Image 918 - Predicted: 5, Actual: 4\n",
      "Image 919 - Predicted: 0, Actual: 0\n",
      "Image 920 - Predicted: 7, Actual: 7\n",
      "Image 921 - Predicted: 4, Actual: 4\n",
      "Image 922 - Predicted: 7, Actual: 7\n",
      "Image 923 - Predicted: 4, Actual: 4\n",
      "Image 924 - Predicted: 6, Actual: 6\n",
      "Image 925 - Predicted: 8, Actual: 8\n",
      "Image 926 - Predicted: 2, Actual: 2\n",
      "Image 927 - Predicted: 1, Actual: 1\n",
      "Image 928 - Predicted: 5, Actual: 5\n",
      "Image 929 - Predicted: 0, Actual: 0\n",
      "Image 930 - Predicted: 9, Actual: 4\n",
      "Image 931 - Predicted: 5, Actual: 5\n",
      "Image 932 - Predicted: 5, Actual: 8\n",
      "Image 933 - Predicted: 9, Actual: 9\n",
      "Image 934 - Predicted: 3, Actual: 3\n",
      "Image 935 - Predicted: 2, Actual: 2\n",
      "Image 936 - Predicted: 1, Actual: 1\n",
      "Image 937 - Predicted: 9, Actual: 9\n",
      "Image 938 - Predicted: 0, Actual: 0\n",
      "Image 939 - Predicted: 3, Actual: 3\n",
      "Image 940 - Predicted: 5, Actual: 5\n",
      "Image 941 - Predicted: 8, Actual: 8\n",
      "Image 942 - Predicted: 1, Actual: 1\n",
      "Image 943 - Predicted: 7, Actual: 7\n",
      "Image 944 - Predicted: 8, Actual: 8\n",
      "Image 945 - Predicted: 8, Actual: 8\n",
      "Image 946 - Predicted: 6, Actual: 6\n",
      "Image 947 - Predicted: 4, Actual: 8\n",
      "Image 948 - Predicted: 0, Actual: 0\n",
      "Image 949 - Predicted: 3, Actual: 3\n",
      "Image 950 - Predicted: 3, Actual: 3\n",
      "Image 951 - Predicted: 7, Actual: 9\n",
      "Image 952 - Predicted: 0, Actual: 0\n",
      "Image 953 - Predicted: 9, Actual: 8\n",
      "Image 954 - Predicted: 5, Actual: 5\n",
      "Image 955 - Predicted: 3, Actual: 3\n",
      "Image 956 - Predicted: 8, Actual: 4\n",
      "Image 957 - Predicted: 4, Actual: 8\n",
      "Image 958 - Predicted: 3, Actual: 3\n",
      "Image 959 - Predicted: 4, Actual: 4\n",
      "Image 960 - Predicted: 9, Actual: 9\n",
      "Image 961 - Predicted: 7, Actual: 7\n",
      "Image 962 - Predicted: 6, Actual: 1\n",
      "Image 963 - Predicted: 5, Actual: 5\n",
      "Image 964 - Predicted: 6, Actual: 6\n",
      "Image 965 - Predicted: 7, Actual: 7\n",
      "Image 966 - Predicted: 5, Actual: 5\n",
      "Image 967 - Predicted: 5, Actual: 9\n",
      "Image 968 - Predicted: 5, Actual: 8\n",
      "Image 969 - Predicted: 2, Actual: 2\n",
      "Image 970 - Predicted: 7, Actual: 8\n",
      "Image 971 - Predicted: 7, Actual: 9\n",
      "Image 972 - Predicted: 0, Actual: 0\n",
      "Image 973 - Predicted: 7, Actual: 7\n",
      "Image 974 - Predicted: 1, Actual: 1\n",
      "Image 975 - Predicted: 5, Actual: 5\n",
      "Image 976 - Predicted: 6, Actual: 6\n",
      "Image 977 - Predicted: 7, Actual: 7\n",
      "Image 978 - Predicted: 0, Actual: 0\n",
      "Image 979 - Predicted: 1, Actual: 1\n",
      "Image 980 - Predicted: 3, Actual: 3\n",
      "Image 981 - Predicted: 2, Actual: 2\n",
      "Image 982 - Predicted: 4, Actual: 4\n",
      "Image 983 - Predicted: 3, Actual: 3\n",
      "Image 984 - Predicted: 1, Actual: 1\n",
      "Image 985 - Predicted: 1, Actual: 1\n",
      "Image 986 - Predicted: 1, Actual: 7\n",
      "Image 987 - Predicted: 7, Actual: 7\n",
      "Image 988 - Predicted: 1, Actual: 0\n",
      "Image 989 - Predicted: 5, Actual: 5\n",
      "Image 990 - Predicted: 5, Actual: 5\n",
      "Image 991 - Predicted: 9, Actual: 9\n",
      "Image 992 - Predicted: 7, Actual: 7\n",
      "Image 993 - Predicted: 5, Actual: 5\n",
      "Image 994 - Predicted: 8, Actual: 8\n",
      "Image 995 - Predicted: 0, Actual: 0\n",
      "Image 996 - Predicted: 7, Actual: 8\n",
      "Image 997 - Predicted: 9, Actual: 9\n",
      "Image 998 - Predicted: 8, Actual: 8\n",
      "Image 999 - Predicted: 2, Actual: 6\n",
      "Image 1000 - Predicted: 5, Actual: 5\n",
      "Image 1001 - Predicted: 3, Actual: 3\n",
      "Image 1002 - Predicted: 1, Actual: 1\n",
      "Image 1003 - Predicted: 7, Actual: 7\n",
      "Image 1004 - Predicted: 2, Actual: 2\n",
      "Image 1005 - Predicted: 9, Actual: 9\n",
      "Image 1006 - Predicted: 8, Actual: 8\n",
      "Image 1007 - Predicted: 8, Actual: 4\n",
      "Image 1008 - Predicted: 5, Actual: 5\n",
      "Image 1009 - Predicted: 4, Actual: 4\n",
      "Image 1010 - Predicted: 8, Actual: 8\n",
      "Image 1011 - Predicted: 8, Actual: 8\n",
      "Image 1012 - Predicted: 3, Actual: 3\n",
      "Image 1013 - Predicted: 1, Actual: 1\n",
      "Image 1014 - Predicted: 7, Actual: 7\n",
      "Image 1015 - Predicted: 0, Actual: 0\n",
      "Image 1016 - Predicted: 7, Actual: 8\n",
      "Image 1017 - Predicted: 1, Actual: 1\n",
      "Image 1018 - Predicted: 3, Actual: 5\n",
      "Image 1019 - Predicted: 5, Actual: 5\n",
      "Image 1020 - Predicted: 5, Actual: 5\n",
      "Image 1021 - Predicted: 4, Actual: 4\n",
      "Image 1022 - Predicted: 9, Actual: 9\n",
      "Image 1023 - Predicted: 0, Actual: 0\n",
      "Image 1024 - Predicted: 3, Actual: 3\n",
      "Image 1025 - Predicted: 4, Actual: 8\n",
      "Image 1026 - Predicted: 5, Actual: 5\n",
      "Image 1027 - Predicted: 4, Actual: 8\n",
      "Image 1028 - Predicted: 1, Actual: 1\n",
      "Image 1029 - Predicted: 3, Actual: 3\n",
      "Image 1030 - Predicted: 5, Actual: 5\n",
      "Image 1031 - Predicted: 4, Actual: 5\n",
      "Image 1032 - Predicted: 1, Actual: 1\n",
      "Image 1033 - Predicted: 3, Actual: 3\n",
      "Image 1034 - Predicted: 8, Actual: 8\n",
      "Image 1035 - Predicted: 5, Actual: 5\n",
      "Image 1036 - Predicted: 1, Actual: 1\n",
      "Image 1037 - Predicted: 0, Actual: 0\n",
      "Image 1038 - Predicted: 7, Actual: 7\n",
      "Image 1039 - Predicted: 5, Actual: 8\n",
      "Image 1040 - Predicted: 6, Actual: 6\n",
      "Image 1041 - Predicted: 8, Actual: 8\n",
      "Image 1042 - Predicted: 5, Actual: 5\n",
      "Image 1043 - Predicted: 2, Actual: 2\n",
      "Image 1044 - Predicted: 7, Actual: 7\n",
      "Image 1045 - Predicted: 5, Actual: 5\n",
      "Image 1046 - Predicted: 5, Actual: 4\n",
      "Image 1047 - Predicted: 5, Actual: 5\n",
      "Image 1048 - Predicted: 1, Actual: 1\n",
      "Image 1049 - Predicted: 1, Actual: 1\n",
      "Image 1050 - Predicted: 5, Actual: 5\n",
      "Image 1051 - Predicted: 5, Actual: 5\n",
      "Image 1052 - Predicted: 7, Actual: 9\n",
      "Image 1053 - Predicted: 4, Actual: 4\n",
      "Image 1054 - Predicted: 8, Actual: 8\n",
      "Image 1055 - Predicted: 6, Actual: 6\n",
      "Image 1056 - Predicted: 8, Actual: 8\n",
      "Image 1057 - Predicted: 1, Actual: 1\n",
      "Image 1058 - Predicted: 7, Actual: 7\n",
      "Image 1059 - Predicted: 2, Actual: 2\n",
      "Image 1060 - Predicted: 7, Actual: 7\n",
      "Image 1061 - Predicted: 6, Actual: 6\n",
      "Image 1062 - Predicted: 2, Actual: 2\n",
      "Image 1063 - Predicted: 0, Actual: 0\n",
      "Image 1064 - Predicted: 2, Actual: 2\n",
      "Image 1065 - Predicted: 7, Actual: 7\n",
      "Image 1066 - Predicted: 4, Actual: 4\n",
      "Image 1067 - Predicted: 5, Actual: 4\n",
      "Image 1068 - Predicted: 0, Actual: 0\n",
      "Image 1069 - Predicted: 3, Actual: 3\n",
      "Image 1070 - Predicted: 5, Actual: 5\n",
      "Image 1071 - Predicted: 5, Actual: 5\n",
      "Image 1072 - Predicted: 2, Actual: 6\n",
      "Image 1073 - Predicted: 3, Actual: 3\n",
      "Image 1074 - Predicted: 9, Actual: 9\n",
      "Image 1075 - Predicted: 7, Actual: 7\n",
      "Image 1076 - Predicted: 6, Actual: 6\n",
      "Image 1077 - Predicted: 3, Actual: 3\n",
      "Image 1078 - Predicted: 0, Actual: 0\n",
      "Image 1079 - Predicted: 4, Actual: 4\n",
      "Image 1080 - Predicted: 1, Actual: 1\n",
      "Image 1081 - Predicted: 7, Actual: 9\n",
      "Image 1082 - Predicted: 1, Actual: 0\n",
      "Image 1083 - Predicted: 4, Actual: 4\n",
      "Image 1084 - Predicted: 1, Actual: 1\n",
      "Image 1085 - Predicted: 8, Actual: 9\n",
      "Image 1086 - Predicted: 6, Actual: 6\n",
      "Image 1087 - Predicted: 4, Actual: 8\n",
      "Image 1088 - Predicted: 2, Actual: 2\n",
      "Image 1089 - Predicted: 6, Actual: 6\n",
      "Image 1090 - Predicted: 0, Actual: 0\n",
      "Image 1091 - Predicted: 6, Actual: 6\n",
      "Image 1092 - Predicted: 9, Actual: 9\n",
      "Image 1093 - Predicted: 8, Actual: 7\n",
      "Image 1094 - Predicted: 3, Actual: 3\n",
      "Image 1095 - Predicted: 5, Actual: 5\n",
      "Image 1096 - Predicted: 6, Actual: 6\n",
      "Image 1097 - Predicted: 6, Actual: 6\n",
      "Image 1098 - Predicted: 2, Actual: 1\n",
      "Image 1099 - Predicted: 2, Actual: 2\n",
      "Image 1100 - Predicted: 9, Actual: 9\n",
      "Image 1101 - Predicted: 0, Actual: 0\n",
      "Image 1102 - Predicted: 8, Actual: 4\n",
      "Image 1103 - Predicted: 7, Actual: 7\n",
      "Image 1104 - Predicted: 6, Actual: 6\n",
      "Image 1105 - Predicted: 3, Actual: 3\n",
      "Image 1106 - Predicted: 5, Actual: 4\n",
      "Image 1107 - Predicted: 6, Actual: 1\n",
      "Image 1108 - Predicted: 3, Actual: 3\n",
      "Image 1109 - Predicted: 4, Actual: 4\n",
      "Image 1110 - Predicted: 8, Actual: 7\n",
      "Image 1111 - Predicted: 2, Actual: 2\n",
      "Image 1112 - Predicted: 6, Actual: 2\n",
      "Image 1113 - Predicted: 2, Actual: 2\n",
      "Image 1114 - Predicted: 1, Actual: 1\n",
      "Image 1115 - Predicted: 6, Actual: 6\n",
      "Image 1116 - Predicted: 0, Actual: 0\n",
      "Image 1117 - Predicted: 0, Actual: 0\n",
      "Image 1118 - Predicted: 7, Actual: 8\n",
      "Image 1119 - Predicted: 7, Actual: 6\n",
      "Image 1120 - Predicted: 7, Actual: 7\n",
      "Image 1121 - Predicted: 4, Actual: 4\n",
      "Image 1122 - Predicted: 0, Actual: 0\n",
      "Image 1123 - Predicted: 8, Actual: 4\n",
      "Image 1124 - Predicted: 3, Actual: 3\n",
      "Image 1125 - Predicted: 3, Actual: 3\n",
      "Image 1126 - Predicted: 4, Actual: 4\n",
      "Image 1127 - Predicted: 5, Actual: 5\n",
      "Image 1128 - Predicted: 8, Actual: 8\n",
      "Image 1129 - Predicted: 6, Actual: 6\n",
      "Image 1130 - Predicted: 4, Actual: 4\n",
      "Image 1131 - Predicted: 7, Actual: 3\n",
      "Image 1132 - Predicted: 9, Actual: 9\n",
      "Image 1133 - Predicted: 0, Actual: 0\n",
      "Image 1134 - Predicted: 3, Actual: 3\n",
      "Image 1135 - Predicted: 2, Actual: 2\n",
      "Image 1136 - Predicted: 9, Actual: 9\n",
      "Image 1137 - Predicted: 2, Actual: 2\n",
      "Image 1138 - Predicted: 5, Actual: 5\n",
      "Image 1139 - Predicted: 7, Actual: 8\n",
      "Image 1140 - Predicted: 9, Actual: 9\n",
      "Image 1141 - Predicted: 8, Actual: 8\n",
      "Image 1142 - Predicted: 4, Actual: 4\n",
      "Image 1143 - Predicted: 6, Actual: 6\n",
      "Image 1144 - Predicted: 8, Actual: 8\n",
      "Image 1145 - Predicted: 3, Actual: 3\n",
      "Image 1146 - Predicted: 1, Actual: 1\n",
      "Image 1147 - Predicted: 9, Actual: 9\n",
      "Image 1148 - Predicted: 4, Actual: 5\n",
      "Image 1149 - Predicted: 8, Actual: 8\n",
      "Image 1150 - Predicted: 9, Actual: 7\n",
      "Image 1151 - Predicted: 4, Actual: 4\n",
      "Image 1152 - Predicted: 5, Actual: 5\n",
      "Image 1153 - Predicted: 0, Actual: 0\n",
      "Image 1154 - Predicted: 2, Actual: 2\n",
      "Image 1155 - Predicted: 4, Actual: 4\n",
      "Image 1156 - Predicted: 2, Actual: 2\n",
      "Image 1157 - Predicted: 5, Actual: 5\n",
      "Image 1158 - Predicted: 8, Actual: 7\n",
      "Image 1159 - Predicted: 2, Actual: 2\n",
      "Image 1160 - Predicted: 1, Actual: 1\n",
      "Image 1161 - Predicted: 1, Actual: 1\n",
      "Image 1162 - Predicted: 8, Actual: 7\n",
      "Image 1163 - Predicted: 3, Actual: 3\n",
      "Image 1164 - Predicted: 6, Actual: 6\n",
      "Image 1165 - Predicted: 4, Actual: 9\n",
      "Image 1166 - Predicted: 4, Actual: 4\n",
      "Image 1167 - Predicted: 1, Actual: 1\n",
      "Image 1168 - Predicted: 6, Actual: 6\n",
      "Image 1169 - Predicted: 9, Actual: 9\n",
      "Image 1170 - Predicted: 4, Actual: 5\n",
      "Image 1171 - Predicted: 0, Actual: 0\n",
      "Image 1172 - Predicted: 5, Actual: 5\n",
      "Image 1173 - Predicted: 5, Actual: 5\n",
      "Image 1174 - Predicted: 1, Actual: 0\n",
      "Image 1175 - Predicted: 7, Actual: 7\n",
      "Image 1176 - Predicted: 8, Actual: 8\n",
      "Image 1177 - Predicted: 7, Actual: 4\n",
      "Image 1178 - Predicted: 2, Actual: 2\n",
      "Image 1179 - Predicted: 6, Actual: 6\n",
      "Image 1180 - Predicted: 9, Actual: 9\n",
      "Image 1181 - Predicted: 8, Actual: 7\n",
      "Image 1182 - Predicted: 7, Actual: 7\n",
      "Image 1183 - Predicted: 2, Actual: 2\n",
      "Image 1184 - Predicted: 3, Actual: 3\n",
      "Image 1185 - Predicted: 0, Actual: 0\n",
      "Image 1186 - Predicted: 0, Actual: 0\n",
      "Image 1187 - Predicted: 4, Actual: 3\n",
      "Image 1188 - Predicted: 8, Actual: 8\n",
      "Image 1189 - Predicted: 6, Actual: 2\n",
      "Image 1190 - Predicted: 6, Actual: 6\n",
      "Image 1191 - Predicted: 8, Actual: 5\n",
      "Image 1192 - Predicted: 1, Actual: 1\n",
      "Image 1193 - Predicted: 7, Actual: 2\n",
      "Image 1194 - Predicted: 0, Actual: 0\n",
      "Image 1195 - Predicted: 2, Actual: 2\n",
      "Image 1196 - Predicted: 6, Actual: 6\n",
      "Image 1197 - Predicted: 4, Actual: 5\n",
      "Image 1198 - Predicted: 3, Actual: 3\n",
      "Image 1199 - Predicted: 8, Actual: 8\n",
      "Image 1200 - Predicted: 6, Actual: 6\n",
      "Image 1201 - Predicted: 6, Actual: 6\n",
      "Image 1202 - Predicted: 2, Actual: 2\n",
      "Image 1203 - Predicted: 1, Actual: 1\n",
      "Image 1204 - Predicted: 9, Actual: 9\n",
      "Image 1205 - Predicted: 2, Actual: 2\n",
      "Image 1206 - Predicted: 2, Actual: 2\n",
      "Image 1207 - Predicted: 9, Actual: 4\n",
      "Image 1208 - Predicted: 5, Actual: 5\n",
      "Image 1209 - Predicted: 8, Actual: 5\n",
      "Image 1210 - Predicted: 0, Actual: 1\n",
      "Image 1211 - Predicted: 1, Actual: 1\n",
      "Image 1212 - Predicted: 4, Actual: 5\n",
      "Image 1213 - Predicted: 9, Actual: 8\n",
      "Image 1214 - Predicted: 6, Actual: 6\n",
      "Image 1215 - Predicted: 0, Actual: 0\n",
      "Image 1216 - Predicted: 1, Actual: 1\n",
      "Image 1217 - Predicted: 9, Actual: 9\n",
      "Image 1218 - Predicted: 9, Actual: 9\n",
      "Image 1219 - Predicted: 9, Actual: 9\n",
      "Image 1220 - Predicted: 4, Actual: 4\n",
      "Image 1221 - Predicted: 1, Actual: 1\n",
      "Image 1222 - Predicted: 6, Actual: 6\n",
      "Image 1223 - Predicted: 9, Actual: 9\n",
      "Image 1224 - Predicted: 8, Actual: 8\n",
      "Image 1225 - Predicted: 1, Actual: 1\n",
      "Image 1226 - Predicted: 4, Actual: 4\n",
      "Image 1227 - Predicted: 9, Actual: 9\n",
      "Image 1228 - Predicted: 8, Actual: 8\n",
      "Image 1229 - Predicted: 4, Actual: 8\n",
      "Image 1230 - Predicted: 1, Actual: 1\n",
      "Image 1231 - Predicted: 9, Actual: 9\n",
      "Image 1232 - Predicted: 7, Actual: 9\n",
      "Image 1233 - Predicted: 1, Actual: 6\n",
      "Image 1234 - Predicted: 9, Actual: 4\n",
      "Image 1235 - Predicted: 2, Actual: 6\n",
      "Image 1236 - Predicted: 5, Actual: 5\n",
      "Image 1237 - Predicted: 2, Actual: 2\n",
      "Image 1238 - Predicted: 0, Actual: 0\n",
      "Image 1239 - Predicted: 4, Actual: 4\n",
      "Image 1240 - Predicted: 9, Actual: 9\n",
      "Image 1241 - Predicted: 3, Actual: 3\n",
      "Image 1242 - Predicted: 6, Actual: 6\n",
      "Image 1243 - Predicted: 6, Actual: 6\n",
      "Image 1244 - Predicted: 3, Actual: 3\n",
      "Image 1245 - Predicted: 1, Actual: 1\n",
      "Image 1246 - Predicted: 3, Actual: 3\n",
      "Image 1247 - Predicted: 5, Actual: 5\n",
      "Image 1248 - Predicted: 6, Actual: 6\n",
      "Image 1249 - Predicted: 0, Actual: 0\n",
      "Image 1250 - Predicted: 3, Actual: 3\n",
      "Image 1251 - Predicted: 3, Actual: 3\n",
      "Image 1252 - Predicted: 6, Actual: 6\n",
      "Image 1253 - Predicted: 7, Actual: 7\n",
      "Image 1254 - Predicted: 6, Actual: 6\n",
      "Image 1255 - Predicted: 9, Actual: 9\n",
      "Image 1256 - Predicted: 1, Actual: 0\n",
      "Image 1257 - Predicted: 0, Actual: 0\n",
      "Image 1258 - Predicted: 9, Actual: 8\n",
      "Image 1259 - Predicted: 0, Actual: 0\n",
      "Image 1260 - Predicted: 6, Actual: 6\n",
      "Image 1261 - Predicted: 7, Actual: 2\n",
      "Image 1262 - Predicted: 0, Actual: 0\n",
      "Image 1263 - Predicted: 3, Actual: 3\n",
      "Image 1264 - Predicted: 0, Actual: 0\n",
      "Image 1265 - Predicted: 1, Actual: 1\n",
      "Image 1266 - Predicted: 7, Actual: 7\n",
      "Image 1267 - Predicted: 3, Actual: 3\n",
      "Image 1268 - Predicted: 0, Actual: 0\n",
      "Image 1269 - Predicted: 8, Actual: 8\n",
      "Image 1270 - Predicted: 1, Actual: 1\n",
      "Image 1271 - Predicted: 1, Actual: 1\n",
      "Image 1272 - Predicted: 0, Actual: 0\n",
      "Image 1273 - Predicted: 0, Actual: 0\n",
      "Image 1274 - Predicted: 9, Actual: 4\n",
      "Image 1275 - Predicted: 7, Actual: 7\n",
      "Image 1276 - Predicted: 6, Actual: 6\n",
      "Image 1277 - Predicted: 0, Actual: 0\n",
      "Image 1278 - Predicted: 2, Actual: 2\n",
      "Image 1279 - Predicted: 8, Actual: 4\n",
      "Image 1280 - Predicted: 2, Actual: 7\n",
      "Image 1281 - Predicted: 7, Actual: 7\n",
      "Image 1282 - Predicted: 0, Actual: 0\n",
      "Image 1283 - Predicted: 4, Actual: 4\n",
      "Image 1284 - Predicted: 9, Actual: 9\n",
      "Image 1285 - Predicted: 5, Actual: 5\n",
      "Image 1286 - Predicted: 0, Actual: 0\n",
      "Image 1287 - Predicted: 1, Actual: 1\n",
      "Image 1288 - Predicted: 1, Actual: 0\n",
      "Image 1289 - Predicted: 4, Actual: 4\n",
      "Image 1290 - Predicted: 3, Actual: 3\n",
      "Image 1291 - Predicted: 3, Actual: 3\n",
      "Image 1292 - Predicted: 5, Actual: 5\n",
      "Image 1293 - Predicted: 4, Actual: 4\n",
      "Image 1294 - Predicted: 7, Actual: 9\n",
      "Image 1295 - Predicted: 6, Actual: 6\n",
      "Image 1296 - Predicted: 7, Actual: 7\n",
      "Image 1297 - Predicted: 7, Actual: 7\n",
      "Image 1298 - Predicted: 8, Actual: 8\n",
      "Image 1299 - Predicted: 7, Actual: 7\n",
      "Image 1300 - Predicted: 5, Actual: 4\n",
      "Image 1301 - Predicted: 0, Actual: 0\n",
      "Image 1302 - Predicted: 5, Actual: 5\n",
      "Image 1303 - Predicted: 4, Actual: 8\n",
      "Image 1304 - Predicted: 3, Actual: 3\n",
      "Image 1305 - Predicted: 3, Actual: 3\n",
      "Image 1306 - Predicted: 1, Actual: 1\n",
      "Image 1307 - Predicted: 6, Actual: 6\n",
      "Image 1308 - Predicted: 7, Actual: 7\n",
      "Image 1309 - Predicted: 2, Actual: 9\n",
      "Image 1310 - Predicted: 0, Actual: 0\n",
      "Image 1311 - Predicted: 9, Actual: 9\n",
      "Image 1312 - Predicted: 4, Actual: 4\n",
      "Image 1313 - Predicted: 0, Actual: 0\n",
      "Image 1314 - Predicted: 3, Actual: 3\n",
      "Image 1315 - Predicted: 2, Actual: 2\n",
      "Image 1316 - Predicted: 3, Actual: 3\n",
      "Image 1317 - Predicted: 1, Actual: 1\n",
      "Image 1318 - Predicted: 5, Actual: 5\n",
      "Image 1319 - Predicted: 5, Actual: 5\n",
      "Image 1320 - Predicted: 0, Actual: 0\n",
      "Image 1321 - Predicted: 1, Actual: 1\n",
      "Image 1322 - Predicted: 7, Actual: 7\n",
      "Image 1323 - Predicted: 6, Actual: 6\n",
      "Image 1324 - Predicted: 6, Actual: 6\n",
      "Image 1325 - Predicted: 1, Actual: 1\n",
      "Image 1326 - Predicted: 2, Actual: 2\n",
      "Image 1327 - Predicted: 8, Actual: 8\n",
      "Image 1328 - Predicted: 8, Actual: 8\n",
      "Image 1329 - Predicted: 0, Actual: 0\n",
      "Image 1330 - Predicted: 9, Actual: 9\n",
      "Image 1331 - Predicted: 5, Actual: 5\n",
      "Image 1332 - Predicted: 0, Actual: 0\n",
      "Image 1333 - Predicted: 7, Actual: 0\n",
      "Image 1334 - Predicted: 4, Actual: 5\n",
      "Image 1335 - Predicted: 4, Actual: 4\n",
      "Image 1336 - Predicted: 8, Actual: 8\n",
      "Image 1337 - Predicted: 4, Actual: 4\n",
      "Image 1338 - Predicted: 3, Actual: 3\n",
      "Image 1339 - Predicted: 6, Actual: 6\n",
      "Image 1340 - Predicted: 5, Actual: 5\n",
      "Image 1341 - Predicted: 6, Actual: 6\n",
      "Image 1342 - Predicted: 0, Actual: 0\n",
      "Image 1343 - Predicted: 5, Actual: 8\n",
      "Image 1344 - Predicted: 3, Actual: 3\n",
      "Image 1345 - Predicted: 2, Actual: 2\n",
      "Image 1346 - Predicted: 7, Actual: 7\n",
      "Image 1347 - Predicted: 6, Actual: 6\n",
      "Image 1348 - Predicted: 9, Actual: 9\n",
      "Image 1349 - Predicted: 4, Actual: 8\n",
      "Image 1350 - Predicted: 5, Actual: 5\n",
      "Image 1351 - Predicted: 4, Actual: 4\n",
      "Image 1352 - Predicted: 9, Actual: 9\n",
      "Image 1353 - Predicted: 6, Actual: 6\n",
      "Image 1354 - Predicted: 6, Actual: 6\n",
      "Image 1355 - Predicted: 2, Actual: 2\n",
      "Image 1356 - Predicted: 0, Actual: 0\n",
      "Image 1357 - Predicted: 0, Actual: 0\n",
      "Image 1358 - Predicted: 1, Actual: 1\n",
      "Image 1359 - Predicted: 5, Actual: 5\n",
      "Image 1360 - Predicted: 9, Actual: 9\n",
      "Image 1361 - Predicted: 4, Actual: 4\n",
      "Image 1362 - Predicted: 9, Actual: 8\n",
      "Image 1363 - Predicted: 8, Actual: 1\n",
      "Image 1364 - Predicted: 3, Actual: 3\n",
      "Image 1365 - Predicted: 3, Actual: 3\n",
      "Image 1366 - Predicted: 3, Actual: 3\n",
      "Image 1367 - Predicted: 9, Actual: 9\n",
      "Image 1368 - Predicted: 8, Actual: 8\n",
      "Image 1369 - Predicted: 9, Actual: 9\n",
      "Image 1370 - Predicted: 1, Actual: 1\n",
      "Image 1371 - Predicted: 5, Actual: 5\n",
      "Image 1372 - Predicted: 7, Actual: 7\n",
      "Image 1373 - Predicted: 0, Actual: 7\n",
      "Image 1374 - Predicted: 2, Actual: 2\n",
      "Image 1375 - Predicted: 9, Actual: 9\n",
      "Image 1376 - Predicted: 9, Actual: 9\n",
      "Image 1377 - Predicted: 0, Actual: 0\n",
      "Image 1378 - Predicted: 0, Actual: 1\n",
      "Image 1379 - Predicted: 2, Actual: 2\n",
      "Image 1380 - Predicted: 0, Actual: 0\n",
      "Image 1381 - Predicted: 1, Actual: 1\n",
      "Image 1382 - Predicted: 3, Actual: 3\n",
      "Image 1383 - Predicted: 8, Actual: 8\n",
      "Image 1384 - Predicted: 7, Actual: 7\n",
      "Image 1385 - Predicted: 8, Actual: 4\n",
      "Image 1386 - Predicted: 8, Actual: 8\n",
      "Image 1387 - Predicted: 6, Actual: 6\n",
      "Image 1388 - Predicted: 7, Actual: 7\n",
      "Image 1389 - Predicted: 9, Actual: 9\n",
      "Image 1390 - Predicted: 8, Actual: 4\n",
      "Image 1391 - Predicted: 2, Actual: 2\n",
      "Image 1392 - Predicted: 3, Actual: 3\n",
      "Image 1393 - Predicted: 5, Actual: 5\n",
      "Image 1394 - Predicted: 0, Actual: 0\n",
      "Image 1395 - Predicted: 5, Actual: 5\n",
      "Image 1396 - Predicted: 8, Actual: 8\n",
      "Image 1397 - Predicted: 4, Actual: 4\n",
      "Image 1398 - Predicted: 3, Actual: 3\n",
      "Image 1399 - Predicted: 6, Actual: 6\n",
      "Image 1400 - Predicted: 8, Actual: 0\n",
      "Image 1401 - Predicted: 9, Actual: 9\n",
      "Image 1402 - Predicted: 6, Actual: 6\n",
      "Image 1403 - Predicted: 4, Actual: 8\n",
      "Image 1404 - Predicted: 7, Actual: 7\n",
      "Image 1405 - Predicted: 9, Actual: 7\n",
      "Image 1406 - Predicted: 1, Actual: 1\n",
      "Image 1407 - Predicted: 0, Actual: 0\n",
      "Image 1408 - Predicted: 9, Actual: 9\n",
      "Image 1409 - Predicted: 9, Actual: 9\n",
      "Image 1410 - Predicted: 0, Actual: 0\n",
      "Image 1411 - Predicted: 9, Actual: 9\n",
      "Image 1412 - Predicted: 8, Actual: 8\n",
      "Image 1413 - Predicted: 5, Actual: 5\n",
      "Image 1414 - Predicted: 8, Actual: 4\n",
      "Image 1415 - Predicted: 0, Actual: 0\n",
      "Image 1416 - Predicted: 7, Actual: 7\n",
      "Image 1417 - Predicted: 1, Actual: 1\n",
      "Image 1418 - Predicted: 6, Actual: 6\n",
      "Image 1419 - Predicted: 3, Actual: 3\n",
      "Image 1420 - Predicted: 7, Actual: 7\n",
      "Image 1421 - Predicted: 3, Actual: 3\n",
      "Image 1422 - Predicted: 0, Actual: 0\n",
      "Image 1423 - Predicted: 4, Actual: 4\n",
      "Image 1424 - Predicted: 4, Actual: 4\n",
      "Image 1425 - Predicted: 6, Actual: 2\n",
      "Image 1426 - Predicted: 9, Actual: 9\n",
      "Image 1427 - Predicted: 4, Actual: 4\n",
      "Image 1428 - Predicted: 7, Actual: 8\n",
      "Image 1429 - Predicted: 6, Actual: 7\n",
      "Image 1430 - Predicted: 7, Actual: 8\n",
      "Image 1431 - Predicted: 0, Actual: 0\n",
      "Image 1432 - Predicted: 4, Actual: 4\n",
      "Image 1433 - Predicted: 0, Actual: 0\n",
      "Image 1434 - Predicted: 9, Actual: 9\n",
      "Image 1435 - Predicted: 8, Actual: 8\n",
      "Image 1436 - Predicted: 0, Actual: 0\n",
      "Image 1437 - Predicted: 1, Actual: 1\n",
      "Image 1438 - Predicted: 8, Actual: 4\n",
      "Image 1439 - Predicted: 2, Actual: 6\n",
      "Image 1440 - Predicted: 8, Actual: 9\n",
      "Image 1441 - Predicted: 6, Actual: 6\n",
      "Image 1442 - Predicted: 5, Actual: 4\n",
      "Image 1443 - Predicted: 6, Actual: 6\n",
      "Image 1444 - Predicted: 1, Actual: 1\n",
      "Image 1445 - Predicted: 9, Actual: 9\n",
      "Image 1446 - Predicted: 2, Actual: 2\n",
      "Image 1447 - Predicted: 5, Actual: 5\n",
      "Image 1448 - Predicted: 3, Actual: 3\n",
      "Image 1449 - Predicted: 7, Actual: 7\n",
      "Image 1450 - Predicted: 8, Actual: 8\n",
      "Image 1451 - Predicted: 6, Actual: 6\n",
      "Image 1452 - Predicted: 9, Actual: 9\n",
      "Image 1453 - Predicted: 9, Actual: 4\n",
      "Image 1454 - Predicted: 5, Actual: 4\n",
      "Image 1455 - Predicted: 3, Actual: 2\n",
      "Image 1456 - Predicted: 8, Actual: 4\n",
      "Image 1457 - Predicted: 2, Actual: 2\n",
      "Image 1458 - Predicted: 1, Actual: 1\n",
      "Image 1459 - Predicted: 5, Actual: 5\n",
      "Image 1460 - Predicted: 8, Actual: 8\n",
      "Image 1461 - Predicted: 5, Actual: 5\n",
      "Image 1462 - Predicted: 6, Actual: 6\n",
      "Image 1463 - Predicted: 7, Actual: 7\n",
      "Image 1464 - Predicted: 9, Actual: 5\n",
      "Image 1465 - Predicted: 0, Actual: 0\n",
      "Image 1466 - Predicted: 9, Actual: 9\n",
      "Image 1467 - Predicted: 6, Actual: 6\n",
      "Image 1468 - Predicted: 9, Actual: 9\n",
      "Image 1469 - Predicted: 6, Actual: 6\n",
      "Image 1470 - Predicted: 5, Actual: 5\n",
      "Image 1471 - Predicted: 5, Actual: 5\n",
      "Image 1472 - Predicted: 5, Actual: 5\n",
      "Image 1473 - Predicted: 1, Actual: 1\n",
      "Image 1474 - Predicted: 5, Actual: 5\n",
      "Image 1475 - Predicted: 2, Actual: 2\n",
      "Image 1476 - Predicted: 6, Actual: 6\n",
      "Image 1477 - Predicted: 9, Actual: 9\n",
      "Image 1478 - Predicted: 8, Actual: 9\n",
      "Image 1479 - Predicted: 7, Actual: 7\n",
      "Image 1480 - Predicted: 9, Actual: 9\n",
      "Image 1481 - Predicted: 5, Actual: 9\n",
      "Image 1482 - Predicted: 9, Actual: 9\n",
      "Image 1483 - Predicted: 4, Actual: 4\n",
      "Image 1484 - Predicted: 0, Actual: 0\n",
      "Image 1485 - Predicted: 9, Actual: 9\n",
      "Image 1486 - Predicted: 7, Actual: 7\n",
      "Image 1487 - Predicted: 4, Actual: 4\n",
      "Image 1488 - Predicted: 9, Actual: 9\n",
      "Image 1489 - Predicted: 3, Actual: 3\n",
      "Image 1490 - Predicted: 1, Actual: 1\n",
      "Image 1491 - Predicted: 7, Actual: 7\n",
      "Image 1492 - Predicted: 6, Actual: 6\n",
      "Image 1493 - Predicted: 0, Actual: 0\n",
      "Image 1494 - Predicted: 9, Actual: 9\n",
      "Image 1495 - Predicted: 6, Actual: 6\n",
      "Image 1496 - Predicted: 0, Actual: 0\n",
      "Image 1497 - Predicted: 7, Actual: 7\n",
      "Image 1498 - Predicted: 7, Actual: 9\n",
      "Image 1499 - Predicted: 6, Actual: 2\n",
      "Image 1500 - Predicted: 8, Actual: 8\n",
      "Image 1501 - Predicted: 5, Actual: 0\n",
      "Image 1502 - Predicted: 8, Actual: 8\n",
      "Image 1503 - Predicted: 9, Actual: 9\n",
      "Image 1504 - Predicted: 0, Actual: 0\n",
      "Image 1505 - Predicted: 9, Actual: 8\n",
      "Image 1506 - Predicted: 6, Actual: 6\n",
      "Image 1507 - Predicted: 9, Actual: 9\n",
      "Image 1508 - Predicted: 2, Actual: 6\n",
      "Image 1509 - Predicted: 3, Actual: 3\n",
      "Image 1510 - Predicted: 7, Actual: 7\n",
      "Image 1511 - Predicted: 9, Actual: 4\n",
      "Image 1512 - Predicted: 6, Actual: 6\n",
      "Image 1513 - Predicted: 2, Actual: 2\n",
      "Image 1514 - Predicted: 7, Actual: 7\n",
      "Image 1515 - Predicted: 3, Actual: 3\n",
      "Image 1516 - Predicted: 0, Actual: 0\n",
      "Image 1517 - Predicted: 8, Actual: 8\n",
      "Image 1518 - Predicted: 5, Actual: 5\n",
      "Image 1519 - Predicted: 2, Actual: 6\n",
      "Image 1520 - Predicted: 1, Actual: 1\n",
      "Image 1521 - Predicted: 5, Actual: 5\n",
      "Image 1522 - Predicted: 7, Actual: 7\n",
      "Image 1523 - Predicted: 4, Actual: 4\n",
      "Image 1524 - Predicted: 1, Actual: 1\n",
      "Image 1525 - Predicted: 7, Actual: 7\n",
      "Image 1526 - Predicted: 1, Actual: 1\n",
      "Image 1527 - Predicted: 2, Actual: 2\n",
      "Image 1528 - Predicted: 3, Actual: 3\n",
      "Image 1529 - Predicted: 6, Actual: 6\n",
      "Image 1530 - Predicted: 5, Actual: 4\n",
      "Image 1531 - Predicted: 1, Actual: 1\n",
      "Image 1532 - Predicted: 8, Actual: 8\n",
      "Image 1533 - Predicted: 8, Actual: 8\n",
      "Image 1534 - Predicted: 4, Actual: 8\n",
      "Image 1535 - Predicted: 0, Actual: 0\n",
      "Image 1536 - Predicted: 9, Actual: 8\n",
      "Image 1537 - Predicted: 1, Actual: 1\n",
      "Image 1538 - Predicted: 4, Actual: 4\n",
      "Image 1539 - Predicted: 3, Actual: 3\n",
      "Image 1540 - Predicted: 1, Actual: 1\n",
      "Image 1541 - Predicted: 3, Actual: 3\n",
      "Image 1542 - Predicted: 6, Actual: 6\n",
      "Image 1543 - Predicted: 5, Actual: 9\n",
      "Image 1544 - Predicted: 9, Actual: 9\n",
      "Image 1545 - Predicted: 5, Actual: 5\n",
      "Image 1546 - Predicted: 4, Actual: 4\n",
      "Image 1547 - Predicted: 0, Actual: 0\n",
      "Image 1548 - Predicted: 2, Actual: 2\n",
      "Image 1549 - Predicted: 3, Actual: 3\n",
      "Image 1550 - Predicted: 5, Actual: 5\n",
      "Image 1551 - Predicted: 6, Actual: 6\n",
      "Image 1552 - Predicted: 4, Actual: 4\n",
      "Image 1553 - Predicted: 0, Actual: 0\n",
      "Image 1554 - Predicted: 6, Actual: 2\n",
      "Image 1555 - Predicted: 5, Actual: 5\n",
      "Image 1556 - Predicted: 3, Actual: 3\n",
      "Image 1557 - Predicted: 2, Actual: 2\n",
      "Image 1558 - Predicted: 8, Actual: 8\n",
      "Image 1559 - Predicted: 5, Actual: 5\n",
      "Image 1560 - Predicted: 2, Actual: 2\n",
      "Image 1561 - Predicted: 5, Actual: 5\n",
      "Image 1562 - Predicted: 8, Actual: 8\n",
      "Image 1563 - Predicted: 8, Actual: 8\n",
      "Image 1564 - Predicted: 1, Actual: 1\n",
      "Image 1565 - Predicted: 4, Actual: 8\n",
      "Image 1566 - Predicted: 3, Actual: 3\n",
      "Image 1567 - Predicted: 0, Actual: 1\n",
      "Image 1568 - Predicted: 9, Actual: 9\n",
      "Image 1569 - Predicted: 4, Actual: 4\n",
      "Image 1570 - Predicted: 2, Actual: 2\n",
      "Image 1571 - Predicted: 8, Actual: 9\n",
      "Image 1572 - Predicted: 2, Actual: 2\n",
      "Image 1573 - Predicted: 6, Actual: 5\n",
      "Image 1574 - Predicted: 2, Actual: 2\n",
      "Image 1575 - Predicted: 8, Actual: 8\n",
      "Image 1576 - Predicted: 2, Actual: 2\n",
      "Image 1577 - Predicted: 7, Actual: 2\n",
      "Image 1578 - Predicted: 3, Actual: 3\n",
      "Image 1579 - Predicted: 1, Actual: 1\n",
      "Image 1580 - Predicted: 8, Actual: 8\n",
      "Image 1581 - Predicted: 6, Actual: 6\n",
      "Image 1582 - Predicted: 0, Actual: 0\n",
      "Image 1583 - Predicted: 2, Actual: 2\n",
      "Image 1584 - Predicted: 2, Actual: 2\n",
      "Image 1585 - Predicted: 7, Actual: 2\n",
      "Image 1586 - Predicted: 0, Actual: 0\n",
      "Image 1587 - Predicted: 2, Actual: 2\n",
      "Image 1588 - Predicted: 9, Actual: 7\n",
      "Image 1589 - Predicted: 8, Actual: 8\n",
      "Image 1590 - Predicted: 8, Actual: 4\n",
      "Image 1591 - Predicted: 5, Actual: 9\n",
      "Image 1592 - Predicted: 1, Actual: 1\n",
      "Image 1593 - Predicted: 9, Actual: 9\n",
      "Image 1594 - Predicted: 5, Actual: 5\n",
      "Image 1595 - Predicted: 0, Actual: 0\n",
      "Image 1596 - Predicted: 2, Actual: 2\n",
      "Image 1597 - Predicted: 4, Actual: 4\n",
      "Image 1598 - Predicted: 4, Actual: 4\n",
      "Image 1599 - Predicted: 0, Actual: 0\n",
      "Image 1600 - Predicted: 3, Actual: 3\n",
      "Image 1601 - Predicted: 9, Actual: 9\n",
      "Image 1602 - Predicted: 7, Actual: 7\n",
      "Image 1603 - Predicted: 4, Actual: 8\n",
      "Image 1604 - Predicted: 3, Actual: 3\n",
      "Image 1605 - Predicted: 9, Actual: 4\n",
      "Image 1606 - Predicted: 7, Actual: 9\n",
      "Image 1607 - Predicted: 2, Actual: 2\n",
      "Image 1608 - Predicted: 2, Actual: 2\n",
      "Image 1609 - Predicted: 4, Actual: 4\n",
      "Image 1610 - Predicted: 4, Actual: 4\n",
      "Image 1611 - Predicted: 0, Actual: 0\n",
      "Image 1612 - Predicted: 5, Actual: 5\n",
      "Image 1613 - Predicted: 3, Actual: 3\n",
      "Image 1614 - Predicted: 2, Actual: 2\n",
      "Image 1615 - Predicted: 4, Actual: 4\n",
      "Image 1616 - Predicted: 0, Actual: 0\n",
      "Image 1617 - Predicted: 9, Actual: 9\n",
      "Image 1618 - Predicted: 4, Actual: 4\n",
      "Image 1619 - Predicted: 0, Actual: 0\n",
      "Image 1620 - Predicted: 5, Actual: 5\n",
      "Image 1621 - Predicted: 5, Actual: 5\n",
      "Image 1622 - Predicted: 8, Actual: 8\n",
      "Image 1623 - Predicted: 7, Actual: 7\n",
      "Image 1624 - Predicted: 8, Actual: 8\n",
      "Image 1625 - Predicted: 1, Actual: 1\n",
      "Image 1626 - Predicted: 8, Actual: 8\n",
      "Image 1627 - Predicted: 0, Actual: 0\n",
      "Image 1628 - Predicted: 1, Actual: 1\n",
      "Image 1629 - Predicted: 3, Actual: 3\n",
      "Image 1630 - Predicted: 2, Actual: 2\n",
      "Image 1631 - Predicted: 9, Actual: 9\n",
      "Image 1632 - Predicted: 9, Actual: 7\n",
      "Image 1633 - Predicted: 3, Actual: 3\n",
      "Image 1634 - Predicted: 9, Actual: 9\n",
      "Image 1635 - Predicted: 9, Actual: 9\n",
      "Image 1636 - Predicted: 6, Actual: 6\n",
      "Image 1637 - Predicted: 3, Actual: 3\n",
      "Image 1638 - Predicted: 5, Actual: 5\n",
      "Image 1639 - Predicted: 6, Actual: 6\n",
      "Image 1640 - Predicted: 6, Actual: 6\n",
      "Image 1641 - Predicted: 8, Actual: 8\n",
      "Image 1642 - Predicted: 5, Actual: 4\n",
      "Image 1643 - Predicted: 3, Actual: 3\n",
      "Image 1644 - Predicted: 0, Actual: 0\n",
      "Image 1645 - Predicted: 7, Actual: 7\n",
      "Image 1646 - Predicted: 6, Actual: 6\n",
      "Image 1647 - Predicted: 6, Actual: 6\n",
      "Image 1648 - Predicted: 5, Actual: 5\n",
      "Image 1649 - Predicted: 7, Actual: 9\n",
      "Image 1650 - Predicted: 2, Actual: 2\n",
      "Image 1651 - Predicted: 0, Actual: 0\n",
      "Image 1652 - Predicted: 0, Actual: 0\n",
      "Image 1653 - Predicted: 8, Actual: 8\n",
      "Image 1654 - Predicted: 6, Actual: 6\n",
      "Image 1655 - Predicted: 9, Actual: 9\n",
      "Image 1656 - Predicted: 7, Actual: 7\n",
      "Image 1657 - Predicted: 0, Actual: 0\n",
      "Image 1658 - Predicted: 9, Actual: 9\n",
      "Image 1659 - Predicted: 8, Actual: 8\n",
      "Image 1660 - Predicted: 5, Actual: 5\n",
      "Image 1661 - Predicted: 5, Actual: 4\n",
      "Image 1662 - Predicted: 1, Actual: 1\n",
      "Image 1663 - Predicted: 0, Actual: 0\n",
      "Image 1664 - Predicted: 2, Actual: 2\n",
      "Image 1665 - Predicted: 2, Actual: 2\n",
      "Image 1666 - Predicted: 0, Actual: 0\n",
      "Image 1667 - Predicted: 9, Actual: 9\n",
      "Image 1668 - Predicted: 8, Actual: 8\n",
      "Image 1669 - Predicted: 5, Actual: 4\n",
      "Image 1670 - Predicted: 0, Actual: 0\n",
      "Image 1671 - Predicted: 8, Actual: 8\n",
      "Image 1672 - Predicted: 8, Actual: 8\n",
      "Image 1673 - Predicted: 2, Actual: 2\n",
      "Image 1674 - Predicted: 8, Actual: 8\n",
      "Image 1675 - Predicted: 7, Actual: 7\n",
      "Image 1676 - Predicted: 6, Actual: 6\n",
      "Image 1677 - Predicted: 5, Actual: 5\n",
      "Image 1678 - Predicted: 5, Actual: 5\n",
      "Image 1679 - Predicted: 2, Actual: 2\n",
      "Image 1680 - Predicted: 5, Actual: 5\n",
      "Image 1681 - Predicted: 5, Actual: 5\n",
      "Image 1682 - Predicted: 4, Actual: 4\n",
      "Image 1683 - Predicted: 5, Actual: 4\n",
      "Image 1684 - Predicted: 0, Actual: 0\n",
      "Image 1685 - Predicted: 1, Actual: 1\n",
      "Image 1686 - Predicted: 9, Actual: 9\n",
      "Image 1687 - Predicted: 6, Actual: 6\n",
      "Image 1688 - Predicted: 9, Actual: 9\n",
      "Image 1689 - Predicted: 3, Actual: 3\n",
      "Image 1690 - Predicted: 0, Actual: 0\n",
      "Image 1691 - Predicted: 8, Actual: 8\n",
      "Image 1692 - Predicted: 7, Actual: 7\n",
      "Image 1693 - Predicted: 6, Actual: 6\n",
      "Image 1694 - Predicted: 1, Actual: 1\n",
      "Image 1695 - Predicted: 9, Actual: 9\n",
      "Image 1696 - Predicted: 1, Actual: 1\n",
      "Image 1697 - Predicted: 3, Actual: 3\n",
      "Image 1698 - Predicted: 7, Actual: 7\n",
      "Image 1699 - Predicted: 9, Actual: 9\n",
      "Image 1700 - Predicted: 6, Actual: 6\n",
      "Image 1701 - Predicted: 7, Actual: 7\n",
      "Image 1702 - Predicted: 8, Actual: 8\n",
      "Image 1703 - Predicted: 8, Actual: 4\n",
      "Image 1704 - Predicted: 7, Actual: 9\n",
      "Image 1705 - Predicted: 3, Actual: 9\n",
      "Image 1706 - Predicted: 2, Actual: 6\n",
      "Image 1707 - Predicted: 4, Actual: 5\n",
      "Image 1708 - Predicted: 7, Actual: 7\n",
      "Image 1709 - Predicted: 3, Actual: 3\n",
      "Image 1710 - Predicted: 4, Actual: 9\n",
      "Image 1711 - Predicted: 2, Actual: 6\n",
      "Image 1712 - Predicted: 2, Actual: 2\n",
      "Image 1713 - Predicted: 7, Actual: 7\n",
      "Image 1714 - Predicted: 5, Actual: 5\n",
      "Image 1715 - Predicted: 3, Actual: 3\n",
      "Image 1716 - Predicted: 8, Actual: 8\n",
      "Image 1717 - Predicted: 5, Actual: 5\n",
      "Image 1718 - Predicted: 5, Actual: 4\n",
      "Image 1719 - Predicted: 0, Actual: 0\n",
      "Image 1720 - Predicted: 4, Actual: 4\n",
      "Image 1721 - Predicted: 7, Actual: 7\n",
      "Image 1722 - Predicted: 0, Actual: 0\n",
      "Image 1723 - Predicted: 2, Actual: 2\n",
      "Image 1724 - Predicted: 4, Actual: 4\n",
      "Image 1725 - Predicted: 5, Actual: 5\n",
      "Image 1726 - Predicted: 5, Actual: 5\n",
      "Image 1727 - Predicted: 5, Actual: 8\n",
      "Image 1728 - Predicted: 4, Actual: 4\n",
      "Image 1729 - Predicted: 2, Actual: 2\n",
      "Image 1730 - Predicted: 0, Actual: 0\n",
      "Image 1731 - Predicted: 4, Actual: 5\n",
      "Image 1732 - Predicted: 7, Actual: 7\n",
      "Image 1733 - Predicted: 3, Actual: 3\n",
      "Image 1734 - Predicted: 4, Actual: 5\n",
      "Image 1735 - Predicted: 6, Actual: 6\n",
      "Image 1736 - Predicted: 2, Actual: 2\n",
      "Image 1737 - Predicted: 3, Actual: 3\n",
      "Image 1738 - Predicted: 4, Actual: 5\n",
      "Image 1739 - Predicted: 4, Actual: 4\n",
      "Image 1740 - Predicted: 7, Actual: 7\n",
      "Image 1741 - Predicted: 2, Actual: 2\n",
      "Image 1742 - Predicted: 5, Actual: 4\n",
      "Image 1743 - Predicted: 5, Actual: 5\n",
      "Image 1744 - Predicted: 9, Actual: 5\n",
      "Image 1745 - Predicted: 1, Actual: 0\n",
      "Image 1746 - Predicted: 7, Actual: 7\n",
      "Image 1747 - Predicted: 6, Actual: 6\n",
      "Image 1748 - Predicted: 9, Actual: 9\n",
      "Image 1749 - Predicted: 9, Actual: 4\n",
      "Image 1750 - Predicted: 2, Actual: 2\n",
      "Image 1751 - Predicted: 8, Actual: 8\n",
      "Image 1752 - Predicted: 7, Actual: 2\n",
      "Image 1753 - Predicted: 0, Actual: 0\n",
      "Image 1754 - Predicted: 3, Actual: 3\n",
      "Image 1755 - Predicted: 9, Actual: 9\n",
      "Image 1756 - Predicted: 1, Actual: 1\n",
      "Image 1757 - Predicted: 6, Actual: 6\n",
      "Image 1758 - Predicted: 8, Actual: 4\n",
      "Image 1759 - Predicted: 6, Actual: 2\n",
      "Image 1760 - Predicted: 0, Actual: 1\n",
      "Image 1761 - Predicted: 5, Actual: 4\n",
      "Image 1762 - Predicted: 6, Actual: 6\n",
      "Image 1763 - Predicted: 4, Actual: 5\n",
      "Image 1764 - Predicted: 8, Actual: 8\n",
      "Image 1765 - Predicted: 3, Actual: 3\n",
      "Image 1766 - Predicted: 5, Actual: 5\n",
      "Image 1767 - Predicted: 3, Actual: 3\n",
      "Image 1768 - Predicted: 9, Actual: 7\n",
      "Image 1769 - Predicted: 6, Actual: 6\n",
      "Image 1770 - Predicted: 3, Actual: 3\n",
      "Image 1771 - Predicted: 1, Actual: 1\n",
      "Image 1772 - Predicted: 6, Actual: 6\n",
      "Image 1773 - Predicted: 4, Actual: 2\n",
      "Image 1774 - Predicted: 1, Actual: 1\n",
      "Image 1775 - Predicted: 0, Actual: 0\n",
      "Image 1776 - Predicted: 0, Actual: 0\n",
      "Image 1777 - Predicted: 8, Actual: 4\n",
      "Image 1778 - Predicted: 2, Actual: 2\n",
      "Image 1779 - Predicted: 7, Actual: 8\n",
      "Image 1780 - Predicted: 4, Actual: 9\n",
      "Image 1781 - Predicted: 1, Actual: 1\n",
      "Image 1782 - Predicted: 1, Actual: 1\n",
      "Image 1783 - Predicted: 2, Actual: 2\n",
      "Image 1784 - Predicted: 1, Actual: 1\n",
      "Image 1785 - Predicted: 0, Actual: 0\n",
      "Image 1786 - Predicted: 5, Actual: 5\n",
      "Image 1787 - Predicted: 9, Actual: 9\n",
      "Image 1788 - Predicted: 4, Actual: 4\n",
      "Image 1789 - Predicted: 2, Actual: 1\n",
      "Image 1790 - Predicted: 1, Actual: 1\n",
      "Image 1791 - Predicted: 4, Actual: 4\n",
      "Image 1792 - Predicted: 8, Actual: 8\n",
      "Image 1793 - Predicted: 6, Actual: 6\n",
      "Image 1794 - Predicted: 8, Actual: 5\n",
      "Image 1795 - Predicted: 3, Actual: 3\n",
      "Image 1796 - Predicted: 6, Actual: 6\n",
      "Image 1797 - Predicted: 8, Actual: 4\n",
      "Image 1798 - Predicted: 6, Actual: 6\n",
      "Image 1799 - Predicted: 6, Actual: 2\n",
      "Image 1800 - Predicted: 0, Actual: 1\n",
      "Image 1801 - Predicted: 6, Actual: 6\n",
      "Image 1802 - Predicted: 5, Actual: 5\n",
      "Image 1803 - Predicted: 5, Actual: 5\n",
      "Image 1804 - Predicted: 4, Actual: 4\n",
      "Image 1805 - Predicted: 9, Actual: 9\n",
      "Image 1806 - Predicted: 2, Actual: 2\n",
      "Image 1807 - Predicted: 2, Actual: 2\n",
      "Image 1808 - Predicted: 0, Actual: 0\n",
      "Image 1809 - Predicted: 7, Actual: 7\n",
      "Image 1810 - Predicted: 3, Actual: 3\n",
      "Image 1811 - Predicted: 4, Actual: 5\n",
      "Image 1812 - Predicted: 9, Actual: 5\n",
      "Image 1813 - Predicted: 8, Actual: 8\n",
      "Image 1814 - Predicted: 0, Actual: 0\n",
      "Image 1815 - Predicted: 3, Actual: 3\n",
      "Image 1816 - Predicted: 5, Actual: 4\n",
      "Image 1817 - Predicted: 7, Actual: 7\n",
      "Image 1818 - Predicted: 1, Actual: 1\n",
      "Image 1819 - Predicted: 8, Actual: 8\n",
      "Image 1820 - Predicted: 0, Actual: 0\n",
      "Image 1821 - Predicted: 9, Actual: 7\n",
      "Image 1822 - Predicted: 5, Actual: 5\n",
      "Image 1823 - Predicted: 6, Actual: 6\n",
      "Image 1824 - Predicted: 7, Actual: 7\n",
      "Image 1825 - Predicted: 5, Actual: 5\n",
      "Image 1826 - Predicted: 8, Actual: 8\n",
      "Image 1827 - Predicted: 2, Actual: 2\n",
      "Image 1828 - Predicted: 5, Actual: 5\n",
      "Image 1829 - Predicted: 2, Actual: 6\n",
      "Image 1830 - Predicted: 5, Actual: 5\n",
      "Image 1831 - Predicted: 2, Actual: 2\n",
      "Image 1832 - Predicted: 7, Actual: 2\n",
      "Image 1833 - Predicted: 5, Actual: 5\n",
      "Image 1834 - Predicted: 1, Actual: 1\n",
      "Image 1835 - Predicted: 3, Actual: 3\n",
      "Image 1836 - Predicted: 4, Actual: 8\n",
      "Image 1837 - Predicted: 0, Actual: 1\n",
      "Image 1838 - Predicted: 2, Actual: 2\n",
      "Image 1839 - Predicted: 5, Actual: 5\n",
      "Image 1840 - Predicted: 8, Actual: 8\n",
      "Image 1841 - Predicted: 8, Actual: 4\n",
      "Image 1842 - Predicted: 4, Actual: 8\n",
      "Image 1843 - Predicted: 7, Actual: 8\n",
      "Image 1844 - Predicted: 7, Actual: 3\n",
      "Image 1845 - Predicted: 0, Actual: 0\n",
      "Image 1846 - Predicted: 4, Actual: 8\n",
      "Image 1847 - Predicted: 4, Actual: 7\n",
      "Image 1848 - Predicted: 2, Actual: 6\n",
      "Image 1849 - Predicted: 0, Actual: 0\n",
      "Image 1850 - Predicted: 8, Actual: 8\n",
      "Image 1851 - Predicted: 1, Actual: 1\n",
      "Image 1852 - Predicted: 3, Actual: 3\n",
      "Image 1853 - Predicted: 0, Actual: 0\n",
      "Image 1854 - Predicted: 4, Actual: 4\n",
      "Image 1855 - Predicted: 6, Actual: 6\n",
      "Image 1856 - Predicted: 4, Actual: 4\n",
      "Image 1857 - Predicted: 0, Actual: 0\n",
      "Image 1858 - Predicted: 2, Actual: 2\n",
      "Image 1859 - Predicted: 9, Actual: 7\n",
      "Image 1860 - Predicted: 0, Actual: 0\n",
      "Image 1861 - Predicted: 3, Actual: 3\n",
      "Image 1862 - Predicted: 7, Actual: 7\n",
      "Image 1863 - Predicted: 1, Actual: 1\n",
      "Image 1864 - Predicted: 3, Actual: 3\n",
      "Image 1865 - Predicted: 9, Actual: 9\n",
      "Image 1866 - Predicted: 6, Actual: 6\n",
      "Image 1867 - Predicted: 6, Actual: 6\n",
      "Image 1868 - Predicted: 3, Actual: 3\n",
      "Image 1869 - Predicted: 8, Actual: 9\n",
      "Image 1870 - Predicted: 6, Actual: 6\n",
      "Image 1871 - Predicted: 8, Actual: 8\n",
      "Image 1872 - Predicted: 6, Actual: 6\n",
      "Image 1873 - Predicted: 3, Actual: 3\n",
      "Image 1874 - Predicted: 2, Actual: 2\n",
      "Image 1875 - Predicted: 9, Actual: 5\n",
      "Image 1876 - Predicted: 3, Actual: 3\n",
      "Image 1877 - Predicted: 3, Actual: 3\n",
      "Image 1878 - Predicted: 2, Actual: 2\n",
      "Image 1879 - Predicted: 1, Actual: 0\n",
      "Image 1880 - Predicted: 7, Actual: 7\n",
      "Image 1881 - Predicted: 2, Actual: 2\n",
      "Image 1882 - Predicted: 9, Actual: 4\n",
      "Image 1883 - Predicted: 2, Actual: 2\n",
      "Image 1884 - Predicted: 4, Actual: 5\n",
      "Image 1885 - Predicted: 3, Actual: 3\n",
      "Image 1886 - Predicted: 9, Actual: 7\n",
      "Image 1887 - Predicted: 6, Actual: 6\n",
      "Image 1888 - Predicted: 8, Actual: 8\n",
      "Image 1889 - Predicted: 6, Actual: 6\n",
      "Image 1890 - Predicted: 9, Actual: 9\n",
      "Image 1891 - Predicted: 8, Actual: 8\n",
      "Image 1892 - Predicted: 4, Actual: 8\n",
      "Image 1893 - Predicted: 2, Actual: 6\n",
      "Image 1894 - Predicted: 3, Actual: 3\n",
      "Image 1895 - Predicted: 5, Actual: 5\n",
      "Image 1896 - Predicted: 0, Actual: 0\n",
      "Image 1897 - Predicted: 2, Actual: 2\n",
      "Image 1898 - Predicted: 8, Actual: 8\n",
      "Image 1899 - Predicted: 3, Actual: 3\n",
      "Image 1900 - Predicted: 2, Actual: 2\n",
      "Image 1901 - Predicted: 2, Actual: 2\n",
      "Image 1902 - Predicted: 7, Actual: 8\n",
      "Image 1903 - Predicted: 9, Actual: 9\n",
      "Image 1904 - Predicted: 2, Actual: 2\n",
      "Image 1905 - Predicted: 6, Actual: 6\n",
      "Image 1906 - Predicted: 9, Actual: 9\n",
      "Image 1907 - Predicted: 8, Actual: 7\n",
      "Image 1908 - Predicted: 5, Actual: 5\n",
      "Image 1909 - Predicted: 5, Actual: 5\n",
      "Image 1910 - Predicted: 7, Actual: 7\n",
      "Image 1911 - Predicted: 0, Actual: 0\n",
      "Image 1912 - Predicted: 8, Actual: 8\n",
      "Image 1913 - Predicted: 3, Actual: 3\n",
      "Image 1914 - Predicted: 9, Actual: 9\n",
      "Image 1915 - Predicted: 7, Actual: 7\n",
      "Image 1916 - Predicted: 5, Actual: 5\n",
      "Image 1917 - Predicted: 6, Actual: 6\n",
      "Image 1918 - Predicted: 0, Actual: 0\n",
      "Image 1919 - Predicted: 4, Actual: 4\n",
      "Image 1920 - Predicted: 3, Actual: 2\n",
      "Image 1921 - Predicted: 0, Actual: 0\n",
      "Image 1922 - Predicted: 3, Actual: 3\n",
      "Image 1923 - Predicted: 4, Actual: 4\n",
      "Image 1924 - Predicted: 0, Actual: 0\n",
      "Image 1925 - Predicted: 6, Actual: 6\n",
      "Image 1926 - Predicted: 0, Actual: 0\n",
      "Image 1927 - Predicted: 8, Actual: 8\n",
      "Image 1928 - Predicted: 4, Actual: 4\n",
      "Image 1929 - Predicted: 0, Actual: 0\n",
      "Image 1930 - Predicted: 9, Actual: 5\n",
      "Image 1931 - Predicted: 1, Actual: 1\n",
      "Image 1932 - Predicted: 3, Actual: 3\n",
      "Image 1933 - Predicted: 5, Actual: 5\n",
      "Image 1934 - Predicted: 8, Actual: 8\n",
      "Image 1935 - Predicted: 6, Actual: 6\n",
      "Image 1936 - Predicted: 7, Actual: 7\n",
      "Image 1937 - Predicted: 1, Actual: 1\n",
      "Image 1938 - Predicted: 3, Actual: 3\n",
      "Image 1939 - Predicted: 5, Actual: 5\n",
      "Image 1940 - Predicted: 2, Actual: 6\n",
      "Image 1941 - Predicted: 5, Actual: 8\n",
      "Image 1942 - Predicted: 4, Actual: 5\n",
      "Image 1943 - Predicted: 4, Actual: 4\n",
      "Image 1944 - Predicted: 7, Actual: 7\n",
      "Image 1945 - Predicted: 2, Actual: 2\n",
      "Image 1946 - Predicted: 4, Actual: 4\n",
      "Image 1947 - Predicted: 4, Actual: 4\n",
      "Image 1948 - Predicted: 3, Actual: 3\n",
      "Image 1949 - Predicted: 2, Actual: 5\n",
      "Image 1950 - Predicted: 3, Actual: 3\n",
      "Image 1951 - Predicted: 1, Actual: 1\n",
      "Image 1952 - Predicted: 0, Actual: 1\n",
      "Image 1953 - Predicted: 1, Actual: 1\n",
      "Image 1954 - Predicted: 4, Actual: 8\n",
      "Image 1955 - Predicted: 1, Actual: 1\n",
      "Image 1956 - Predicted: 1, Actual: 1\n",
      "Image 1957 - Predicted: 1, Actual: 1\n",
      "Image 1958 - Predicted: 0, Actual: 0\n",
      "Image 1959 - Predicted: 9, Actual: 4\n",
      "Image 1960 - Predicted: 3, Actual: 3\n",
      "Image 1961 - Predicted: 3, Actual: 3\n",
      "Image 1962 - Predicted: 8, Actual: 8\n",
      "Image 1963 - Predicted: 6, Actual: 6\n",
      "Image 1964 - Predicted: 7, Actual: 7\n",
      "Image 1965 - Predicted: 4, Actual: 4\n",
      "Image 1966 - Predicted: 0, Actual: 0\n",
      "Image 1967 - Predicted: 8, Actual: 8\n",
      "Image 1968 - Predicted: 3, Actual: 3\n",
      "Image 1969 - Predicted: 6, Actual: 6\n",
      "Image 1970 - Predicted: 8, Actual: 4\n",
      "Image 1971 - Predicted: 3, Actual: 3\n",
      "Image 1972 - Predicted: 6, Actual: 6\n",
      "Image 1973 - Predicted: 1, Actual: 1\n",
      "Image 1974 - Predicted: 0, Actual: 0\n",
      "Image 1975 - Predicted: 2, Actual: 2\n",
      "Image 1976 - Predicted: 6, Actual: 6\n",
      "Image 1977 - Predicted: 4, Actual: 5\n",
      "Image 1978 - Predicted: 1, Actual: 1\n",
      "Image 1979 - Predicted: 9, Actual: 4\n",
      "Image 1980 - Predicted: 9, Actual: 9\n",
      "Image 1981 - Predicted: 1, Actual: 3\n",
      "Image 1982 - Predicted: 1, Actual: 1\n",
      "Image 1983 - Predicted: 5, Actual: 5\n",
      "Image 1984 - Predicted: 2, Actual: 2\n",
      "Image 1985 - Predicted: 4, Actual: 4\n",
      "Image 1986 - Predicted: 6, Actual: 6\n",
      "Image 1987 - Predicted: 5, Actual: 5\n",
      "Image 1988 - Predicted: 5, Actual: 5\n",
      "Image 1989 - Predicted: 7, Actual: 7\n",
      "Image 1990 - Predicted: 4, Actual: 4\n",
      "Image 1991 - Predicted: 2, Actual: 2\n",
      "Image 1992 - Predicted: 1, Actual: 1\n",
      "Image 1993 - Predicted: 7, Actual: 7\n",
      "Image 1994 - Predicted: 8, Actual: 4\n",
      "Image 1995 - Predicted: 1, Actual: 1\n",
      "Image 1996 - Predicted: 8, Actual: 8\n",
      "Image 1997 - Predicted: 6, Actual: 6\n",
      "Image 1998 - Predicted: 5, Actual: 5\n",
      "Image 1999 - Predicted: 7, Actual: 7\n",
      "Image 2000 - Predicted: 2, Actual: 2\n",
      "Image 2001 - Predicted: 5, Actual: 5\n",
      "Image 2002 - Predicted: 9, Actual: 9\n",
      "Image 2003 - Predicted: 2, Actual: 2\n",
      "Image 2004 - Predicted: 8, Actual: 8\n",
      "Image 2005 - Predicted: 9, Actual: 7\n",
      "Image 2006 - Predicted: 6, Actual: 6\n",
      "Image 2007 - Predicted: 7, Actual: 0\n",
      "Image 2008 - Predicted: 3, Actual: 3\n",
      "Image 2009 - Predicted: 3, Actual: 3\n",
      "Image 2010 - Predicted: 6, Actual: 2\n",
      "Image 2011 - Predicted: 7, Actual: 7\n",
      "Image 2012 - Predicted: 4, Actual: 8\n",
      "Image 2013 - Predicted: 1, Actual: 1\n",
      "Image 2014 - Predicted: 9, Actual: 9\n",
      "Image 2015 - Predicted: 7, Actual: 7\n",
      "Image 2016 - Predicted: 2, Actual: 2\n",
      "Image 2017 - Predicted: 3, Actual: 3\n",
      "Image 2018 - Predicted: 6, Actual: 2\n",
      "Image 2019 - Predicted: 3, Actual: 3\n",
      "Image 2020 - Predicted: 8, Actual: 8\n",
      "Image 2021 - Predicted: 9, Actual: 7\n",
      "Image 2022 - Predicted: 9, Actual: 9\n",
      "Image 2023 - Predicted: 7, Actual: 7\n",
      "Image 2024 - Predicted: 8, Actual: 8\n",
      "Image 2025 - Predicted: 1, Actual: 1\n",
      "Image 2026 - Predicted: 2, Actual: 2\n",
      "Image 2027 - Predicted: 7, Actual: 7\n",
      "Image 2028 - Predicted: 3, Actual: 3\n",
      "Image 2029 - Predicted: 1, Actual: 1\n",
      "Image 2030 - Predicted: 9, Actual: 9\n",
      "Image 2031 - Predicted: 3, Actual: 3\n",
      "Image 2032 - Predicted: 6, Actual: 6\n",
      "Image 2033 - Predicted: 7, Actual: 7\n",
      "Image 2034 - Predicted: 8, Actual: 4\n",
      "Image 2035 - Predicted: 7, Actual: 7\n",
      "Image 2036 - Predicted: 8, Actual: 5\n",
      "Image 2037 - Predicted: 2, Actual: 2\n",
      "Image 2038 - Predicted: 6, Actual: 6\n",
      "Image 2039 - Predicted: 7, Actual: 9\n",
      "Image 2040 - Predicted: 8, Actual: 8\n",
      "Image 2041 - Predicted: 7, Actual: 7\n",
      "Image 2042 - Predicted: 9, Actual: 9\n",
      "Image 2043 - Predicted: 0, Actual: 0\n",
      "Image 2044 - Predicted: 3, Actual: 3\n",
      "Image 2045 - Predicted: 9, Actual: 9\n",
      "Image 2046 - Predicted: 6, Actual: 6\n",
      "Image 2047 - Predicted: 7, Actual: 7\n",
      "Image 2048 - Predicted: 0, Actual: 0\n",
      "Image 2049 - Predicted: 7, Actual: 7\n",
      "Image 2050 - Predicted: 9, Actual: 9\n",
      "Image 2051 - Predicted: 5, Actual: 5\n",
      "Image 2052 - Predicted: 6, Actual: 6\n",
      "Image 2053 - Predicted: 2, Actual: 2\n",
      "Image 2054 - Predicted: 3, Actual: 3\n",
      "Image 2055 - Predicted: 0, Actual: 0\n",
      "Image 2056 - Predicted: 0, Actual: 0\n",
      "Image 2057 - Predicted: 8, Actual: 8\n",
      "Image 2058 - Predicted: 9, Actual: 9\n",
      "Image 2059 - Predicted: 1, Actual: 1\n",
      "Image 2060 - Predicted: 2, Actual: 2\n",
      "Image 2061 - Predicted: 6, Actual: 6\n",
      "Image 2062 - Predicted: 8, Actual: 8\n",
      "Image 2063 - Predicted: 3, Actual: 3\n",
      "Image 2064 - Predicted: 4, Actual: 4\n",
      "Image 2065 - Predicted: 7, Actual: 7\n",
      "Image 2066 - Predicted: 7, Actual: 7\n",
      "Image 2067 - Predicted: 1, Actual: 1\n",
      "Image 2068 - Predicted: 6, Actual: 6\n",
      "Image 2069 - Predicted: 4, Actual: 8\n",
      "Image 2070 - Predicted: 1, Actual: 1\n",
      "Image 2071 - Predicted: 6, Actual: 6\n",
      "Image 2072 - Predicted: 5, Actual: 4\n",
      "Image 2073 - Predicted: 6, Actual: 6\n",
      "Image 2074 - Predicted: 0, Actual: 0\n",
      "Image 2075 - Predicted: 9, Actual: 7\n",
      "Image 2076 - Predicted: 2, Actual: 2\n",
      "Image 2077 - Predicted: 5, Actual: 5\n",
      "Image 2078 - Predicted: 3, Actual: 3\n",
      "Image 2079 - Predicted: 6, Actual: 7\n",
      "Image 2080 - Predicted: 6, Actual: 6\n",
      "Image 2081 - Predicted: 8, Actual: 4\n",
      "Image 2082 - Predicted: 1, Actual: 1\n",
      "Image 2083 - Predicted: 9, Actual: 9\n",
      "Image 2084 - Predicted: 3, Actual: 7\n",
      "Image 2085 - Predicted: 4, Actual: 4\n",
      "Image 2086 - Predicted: 4, Actual: 4\n",
      "Image 2087 - Predicted: 2, Actual: 2\n",
      "Image 2088 - Predicted: 8, Actual: 8\n",
      "Image 2089 - Predicted: 1, Actual: 1\n",
      "Image 2090 - Predicted: 8, Actual: 5\n",
      "Image 2091 - Predicted: 0, Actual: 0\n",
      "Image 2092 - Predicted: 0, Actual: 0\n",
      "Image 2093 - Predicted: 9, Actual: 9\n",
      "Image 2094 - Predicted: 0, Actual: 1\n",
      "Image 2095 - Predicted: 3, Actual: 3\n",
      "Image 2096 - Predicted: 5, Actual: 8\n",
      "Image 2097 - Predicted: 0, Actual: 0\n",
      "Image 2098 - Predicted: 2, Actual: 2\n",
      "Image 2099 - Predicted: 5, Actual: 8\n",
      "Image 2100 - Predicted: 2, Actual: 2\n",
      "Image 2101 - Predicted: 7, Actual: 7\n",
      "Image 2102 - Predicted: 0, Actual: 0\n",
      "Image 2103 - Predicted: 3, Actual: 3\n",
      "Image 2104 - Predicted: 4, Actual: 4\n",
      "Image 2105 - Predicted: 1, Actual: 1\n",
      "Image 2106 - Predicted: 0, Actual: 0\n",
      "Image 2107 - Predicted: 7, Actual: 7\n",
      "Image 2108 - Predicted: 9, Actual: 9\n",
      "Image 2109 - Predicted: 9, Actual: 9\n",
      "Image 2110 - Predicted: 5, Actual: 5\n",
      "Image 2111 - Predicted: 4, Actual: 4\n",
      "Image 2112 - Predicted: 7, Actual: 8\n",
      "Image 2113 - Predicted: 1, Actual: 1\n",
      "Image 2114 - Predicted: 5, Actual: 5\n",
      "Image 2115 - Predicted: 1, Actual: 1\n",
      "Image 2116 - Predicted: 4, Actual: 4\n",
      "Image 2117 - Predicted: 9, Actual: 7\n",
      "Image 2118 - Predicted: 1, Actual: 1\n",
      "Image 2119 - Predicted: 9, Actual: 9\n",
      "Image 2120 - Predicted: 0, Actual: 0\n",
      "Image 2121 - Predicted: 7, Actual: 7\n",
      "Image 2122 - Predicted: 1, Actual: 1\n",
      "Image 2123 - Predicted: 3, Actual: 3\n",
      "Image 2124 - Predicted: 6, Actual: 2\n",
      "Image 2125 - Predicted: 5, Actual: 5\n",
      "Image 2126 - Predicted: 4, Actual: 4\n",
      "Image 2127 - Predicted: 1, Actual: 1\n",
      "Image 2128 - Predicted: 1, Actual: 1\n",
      "Image 2129 - Predicted: 2, Actual: 2\n",
      "Image 2130 - Predicted: 6, Actual: 6\n",
      "Image 2131 - Predicted: 8, Actual: 8\n",
      "Image 2132 - Predicted: 6, Actual: 6\n",
      "Image 2133 - Predicted: 2, Actual: 2\n",
      "Image 2134 - Predicted: 8, Actual: 8\n",
      "Image 2135 - Predicted: 4, Actual: 4\n",
      "Image 2136 - Predicted: 2, Actual: 7\n",
      "Image 2137 - Predicted: 9, Actual: 9\n",
      "Image 2138 - Predicted: 9, Actual: 9\n",
      "Image 2139 - Predicted: 8, Actual: 4\n",
      "Image 2140 - Predicted: 9, Actual: 9\n",
      "Image 2141 - Predicted: 3, Actual: 3\n",
      "Image 2142 - Predicted: 4, Actual: 4\n",
      "Image 2143 - Predicted: 1, Actual: 1\n",
      "Image 2144 - Predicted: 5, Actual: 5\n",
      "Image 2145 - Predicted: 1, Actual: 1\n",
      "Image 2146 - Predicted: 8, Actual: 8\n",
      "Image 2147 - Predicted: 2, Actual: 2\n",
      "Image 2148 - Predicted: 7, Actual: 7\n",
      "Image 2149 - Predicted: 5, Actual: 5\n",
      "Image 2150 - Predicted: 1, Actual: 1\n",
      "Image 2151 - Predicted: 3, Actual: 3\n",
      "Image 2152 - Predicted: 2, Actual: 2\n",
      "Image 2153 - Predicted: 6, Actual: 6\n",
      "Image 2154 - Predicted: 6, Actual: 2\n",
      "Image 2155 - Predicted: 6, Actual: 6\n",
      "Image 2156 - Predicted: 1, Actual: 6\n",
      "Image 2157 - Predicted: 6, Actual: 6\n",
      "Image 2158 - Predicted: 7, Actual: 8\n",
      "Image 2159 - Predicted: 1, Actual: 1\n",
      "Image 2160 - Predicted: 4, Actual: 4\n",
      "Image 2161 - Predicted: 8, Actual: 8\n",
      "Image 2162 - Predicted: 9, Actual: 9\n",
      "Image 2163 - Predicted: 5, Actual: 5\n",
      "Image 2164 - Predicted: 1, Actual: 0\n",
      "Image 2165 - Predicted: 6, Actual: 6\n",
      "Image 2166 - Predicted: 3, Actual: 3\n",
      "Image 2167 - Predicted: 1, Actual: 0\n",
      "Image 2168 - Predicted: 3, Actual: 7\n",
      "Image 2169 - Predicted: 9, Actual: 9\n",
      "Image 2170 - Predicted: 6, Actual: 6\n",
      "Image 2171 - Predicted: 7, Actual: 7\n",
      "Image 2172 - Predicted: 1, Actual: 0\n",
      "Image 2173 - Predicted: 8, Actual: 7\n",
      "Image 2174 - Predicted: 6, Actual: 6\n",
      "Image 2175 - Predicted: 0, Actual: 0\n",
      "Image 2176 - Predicted: 5, Actual: 5\n",
      "Image 2177 - Predicted: 6, Actual: 6\n",
      "Image 2178 - Predicted: 4, Actual: 4\n",
      "Image 2179 - Predicted: 1, Actual: 1\n",
      "Image 2180 - Predicted: 9, Actual: 9\n",
      "Image 2181 - Predicted: 5, Actual: 5\n",
      "Image 2182 - Predicted: 0, Actual: 0\n",
      "Image 2183 - Predicted: 9, Actual: 9\n",
      "Image 2184 - Predicted: 1, Actual: 1\n",
      "Image 2185 - Predicted: 2, Actual: 2\n",
      "Image 2186 - Predicted: 1, Actual: 1\n",
      "Image 2187 - Predicted: 8, Actual: 8\n",
      "Image 2188 - Predicted: 8, Actual: 4\n",
      "Image 2189 - Predicted: 7, Actual: 9\n",
      "Image 2190 - Predicted: 1, Actual: 1\n",
      "Image 2191 - Predicted: 1, Actual: 1\n",
      "Image 2192 - Predicted: 3, Actual: 3\n",
      "Image 2193 - Predicted: 7, Actual: 9\n",
      "Image 2194 - Predicted: 8, Actual: 8\n",
      "Image 2195 - Predicted: 2, Actual: 2\n",
      "Image 2196 - Predicted: 6, Actual: 6\n",
      "Image 2197 - Predicted: 7, Actual: 7\n",
      "Image 2198 - Predicted: 1, Actual: 1\n",
      "Image 2199 - Predicted: 3, Actual: 3\n",
      "Image 2200 - Predicted: 0, Actual: 0\n",
      "Image 2201 - Predicted: 0, Actual: 1\n",
      "Image 2202 - Predicted: 2, Actual: 2\n",
      "Image 2203 - Predicted: 7, Actual: 7\n",
      "Image 2204 - Predicted: 5, Actual: 5\n",
      "Image 2205 - Predicted: 9, Actual: 9\n",
      "Image 2206 - Predicted: 6, Actual: 6\n",
      "Image 2207 - Predicted: 9, Actual: 9\n",
      "Image 2208 - Predicted: 3, Actual: 3\n",
      "Image 2209 - Predicted: 4, Actual: 4\n",
      "Image 2210 - Predicted: 1, Actual: 1\n",
      "Image 2211 - Predicted: 7, Actual: 3\n",
      "Image 2212 - Predicted: 1, Actual: 1\n",
      "Image 2213 - Predicted: 2, Actual: 2\n",
      "Image 2214 - Predicted: 6, Actual: 6\n",
      "Image 2215 - Predicted: 9, Actual: 9\n",
      "Image 2216 - Predicted: 9, Actual: 9\n",
      "Image 2217 - Predicted: 2, Actual: 2\n",
      "Image 2218 - Predicted: 7, Actual: 3\n",
      "Image 2219 - Predicted: 0, Actual: 3\n",
      "Image 2220 - Predicted: 1, Actual: 1\n",
      "Image 2221 - Predicted: 1, Actual: 1\n",
      "Image 2222 - Predicted: 5, Actual: 5\n",
      "Image 2223 - Predicted: 5, Actual: 5\n",
      "Image 2224 - Predicted: 9, Actual: 9\n",
      "Image 2225 - Predicted: 8, Actual: 4\n",
      "Image 2226 - Predicted: 4, Actual: 4\n",
      "Image 2227 - Predicted: 2, Actual: 2\n",
      "Image 2228 - Predicted: 7, Actual: 7\n",
      "Image 2229 - Predicted: 9, Actual: 9\n",
      "Image 2230 - Predicted: 5, Actual: 5\n",
      "Image 2231 - Predicted: 3, Actual: 5\n",
      "Image 2232 - Predicted: 5, Actual: 5\n",
      "Image 2233 - Predicted: 7, Actual: 7\n",
      "Image 2234 - Predicted: 1, Actual: 1\n",
      "Image 2235 - Predicted: 6, Actual: 1\n",
      "Image 2236 - Predicted: 7, Actual: 9\n",
      "Image 2237 - Predicted: 3, Actual: 3\n",
      "Image 2238 - Predicted: 9, Actual: 9\n",
      "Image 2239 - Predicted: 5, Actual: 5\n",
      "Image 2240 - Predicted: 8, Actual: 4\n",
      "Image 2241 - Predicted: 5, Actual: 5\n",
      "Image 2242 - Predicted: 6, Actual: 6\n",
      "Image 2243 - Predicted: 9, Actual: 7\n",
      "Image 2244 - Predicted: 0, Actual: 0\n",
      "Image 2245 - Predicted: 8, Actual: 4\n",
      "Image 2246 - Predicted: 9, Actual: 7\n",
      "Image 2247 - Predicted: 0, Actual: 0\n",
      "Image 2248 - Predicted: 6, Actual: 6\n",
      "Image 2249 - Predicted: 7, Actual: 7\n",
      "Image 2250 - Predicted: 5, Actual: 4\n",
      "Image 2251 - Predicted: 4, Actual: 5\n",
      "Image 2252 - Predicted: 0, Actual: 0\n",
      "Image 2253 - Predicted: 2, Actual: 2\n",
      "Image 2254 - Predicted: 6, Actual: 0\n",
      "Image 2255 - Predicted: 8, Actual: 8\n",
      "Image 2256 - Predicted: 5, Actual: 5\n",
      "Image 2257 - Predicted: 4, Actual: 4\n",
      "Image 2258 - Predicted: 3, Actual: 3\n",
      "Image 2259 - Predicted: 7, Actual: 7\n",
      "Image 2260 - Predicted: 6, Actual: 6\n",
      "Image 2261 - Predicted: 2, Actual: 2\n",
      "Image 2262 - Predicted: 6, Actual: 6\n",
      "Image 2263 - Predicted: 0, Actual: 0\n",
      "Image 2264 - Predicted: 1, Actual: 1\n",
      "Image 2265 - Predicted: 3, Actual: 3\n",
      "Image 2266 - Predicted: 2, Actual: 2\n",
      "Image 2267 - Predicted: 1, Actual: 1\n",
      "Image 2268 - Predicted: 0, Actual: 0\n",
      "Image 2269 - Predicted: 0, Actual: 0\n",
      "Image 2270 - Predicted: 0, Actual: 0\n",
      "Image 2271 - Predicted: 1, Actual: 1\n",
      "Image 2272 - Predicted: 0, Actual: 0\n",
      "Image 2273 - Predicted: 0, Actual: 0\n",
      "Image 2274 - Predicted: 3, Actual: 3\n",
      "Image 2275 - Predicted: 6, Actual: 6\n",
      "Image 2276 - Predicted: 9, Actual: 9\n",
      "Image 2277 - Predicted: 5, Actual: 5\n",
      "Image 2278 - Predicted: 8, Actual: 8\n",
      "Image 2279 - Predicted: 6, Actual: 6\n",
      "Image 2280 - Predicted: 8, Actual: 4\n",
      "Image 2281 - Predicted: 4, Actual: 5\n",
      "Image 2282 - Predicted: 9, Actual: 7\n",
      "Image 2283 - Predicted: 0, Actual: 0\n",
      "Image 2284 - Predicted: 5, Actual: 5\n",
      "Image 2285 - Predicted: 2, Actual: 2\n",
      "Image 2286 - Predicted: 1, Actual: 1\n",
      "Image 2287 - Predicted: 0, Actual: 0\n",
      "Image 2288 - Predicted: 4, Actual: 8\n",
      "Image 2289 - Predicted: 3, Actual: 3\n",
      "Image 2290 - Predicted: 9, Actual: 9\n",
      "Image 2291 - Predicted: 9, Actual: 9\n",
      "Image 2292 - Predicted: 7, Actual: 7\n",
      "Image 2293 - Predicted: 9, Actual: 9\n",
      "Image 2294 - Predicted: 3, Actual: 3\n",
      "Image 2295 - Predicted: 3, Actual: 3\n",
      "Image 2296 - Predicted: 3, Actual: 3\n",
      "Image 2297 - Predicted: 4, Actual: 8\n",
      "Image 2298 - Predicted: 7, Actual: 7\n",
      "Image 2299 - Predicted: 4, Actual: 4\n",
      "Image 2300 - Predicted: 6, Actual: 2\n",
      "Image 2301 - Predicted: 5, Actual: 5\n",
      "Image 2302 - Predicted: 0, Actual: 0\n",
      "Image 2303 - Predicted: 9, Actual: 9\n",
      "Image 2304 - Predicted: 1, Actual: 1\n",
      "Image 2305 - Predicted: 0, Actual: 0\n",
      "Image 2306 - Predicted: 3, Actual: 3\n",
      "Image 2307 - Predicted: 6, Actual: 6\n",
      "Image 2308 - Predicted: 7, Actual: 4\n",
      "Image 2309 - Predicted: 2, Actual: 2\n",
      "Image 2310 - Predicted: 0, Actual: 0\n",
      "Image 2311 - Predicted: 7, Actual: 7\n",
      "Image 2312 - Predicted: 9, Actual: 9\n",
      "Image 2313 - Predicted: 8, Actual: 8\n",
      "Image 2314 - Predicted: 4, Actual: 9\n",
      "Image 2315 - Predicted: 0, Actual: 0\n",
      "Image 2316 - Predicted: 0, Actual: 0\n",
      "Image 2317 - Predicted: 9, Actual: 9\n",
      "Image 2318 - Predicted: 5, Actual: 5\n",
      "Image 2319 - Predicted: 4, Actual: 4\n",
      "Image 2320 - Predicted: 2, Actual: 2\n",
      "Image 2321 - Predicted: 6, Actual: 6\n",
      "Image 2322 - Predicted: 2, Actual: 2\n",
      "Image 2323 - Predicted: 2, Actual: 2\n",
      "Image 2324 - Predicted: 2, Actual: 2\n",
      "Image 2325 - Predicted: 4, Actual: 4\n",
      "Image 2326 - Predicted: 5, Actual: 4\n",
      "Image 2327 - Predicted: 5, Actual: 8\n",
      "Image 2328 - Predicted: 3, Actual: 3\n",
      "Image 2329 - Predicted: 8, Actual: 3\n",
      "Image 2330 - Predicted: 9, Actual: 7\n",
      "Image 2331 - Predicted: 6, Actual: 6\n",
      "Image 2332 - Predicted: 8, Actual: 8\n",
      "Image 2333 - Predicted: 9, Actual: 9\n",
      "Image 2334 - Predicted: 0, Actual: 0\n",
      "Image 2335 - Predicted: 3, Actual: 3\n",
      "Image 2336 - Predicted: 9, Actual: 9\n",
      "Image 2337 - Predicted: 4, Actual: 4\n",
      "Image 2338 - Predicted: 0, Actual: 0\n",
      "Image 2339 - Predicted: 8, Actual: 8\n",
      "Image 2340 - Predicted: 4, Actual: 9\n",
      "Image 2341 - Predicted: 9, Actual: 4\n",
      "Image 2342 - Predicted: 7, Actual: 7\n",
      "Image 2343 - Predicted: 3, Actual: 3\n",
      "Image 2344 - Predicted: 3, Actual: 3\n",
      "Image 2345 - Predicted: 5, Actual: 5\n",
      "Image 2346 - Predicted: 5, Actual: 9\n",
      "Image 2347 - Predicted: 4, Actual: 8\n",
      "Image 2348 - Predicted: 0, Actual: 0\n",
      "Image 2349 - Predicted: 3, Actual: 8\n",
      "Image 2350 - Predicted: 5, Actual: 5\n",
      "Image 2351 - Predicted: 6, Actual: 6\n",
      "Image 2352 - Predicted: 8, Actual: 8\n",
      "Image 2353 - Predicted: 8, Actual: 8\n",
      "Image 2354 - Predicted: 8, Actual: 8\n",
      "Image 2355 - Predicted: 3, Actual: 3\n",
      "Image 2356 - Predicted: 0, Actual: 0\n",
      "Image 2357 - Predicted: 8, Actual: 8\n",
      "Image 2358 - Predicted: 6, Actual: 6\n",
      "Image 2359 - Predicted: 1, Actual: 1\n",
      "Image 2360 - Predicted: 4, Actual: 5\n",
      "Image 2361 - Predicted: 9, Actual: 9\n",
      "Image 2362 - Predicted: 3, Actual: 3\n",
      "Image 2363 - Predicted: 7, Actual: 9\n",
      "Image 2364 - Predicted: 1, Actual: 1\n",
      "Image 2365 - Predicted: 9, Actual: 7\n",
      "Image 2366 - Predicted: 9, Actual: 9\n",
      "Image 2367 - Predicted: 7, Actual: 7\n",
      "Image 2368 - Predicted: 2, Actual: 2\n",
      "Image 2369 - Predicted: 2, Actual: 2\n",
      "Image 2370 - Predicted: 2, Actual: 2\n",
      "Image 2371 - Predicted: 1, Actual: 1\n",
      "Image 2372 - Predicted: 2, Actual: 2\n",
      "Image 2373 - Predicted: 5, Actual: 5\n",
      "Image 2374 - Predicted: 2, Actual: 2\n",
      "Image 2375 - Predicted: 3, Actual: 3\n",
      "Image 2376 - Predicted: 7, Actual: 7\n",
      "Image 2377 - Predicted: 1, Actual: 1\n",
      "Image 2378 - Predicted: 9, Actual: 7\n",
      "Image 2379 - Predicted: 7, Actual: 7\n",
      "Image 2380 - Predicted: 9, Actual: 9\n",
      "Image 2381 - Predicted: 6, Actual: 6\n",
      "Image 2382 - Predicted: 8, Actual: 4\n",
      "Image 2383 - Predicted: 5, Actual: 8\n",
      "Image 2384 - Predicted: 5, Actual: 4\n",
      "Image 2385 - Predicted: 9, Actual: 9\n",
      "Image 2386 - Predicted: 2, Actual: 2\n",
      "Image 2387 - Predicted: 9, Actual: 9\n",
      "Image 2388 - Predicted: 1, Actual: 1\n",
      "Image 2389 - Predicted: 1, Actual: 1\n",
      "Image 2390 - Predicted: 1, Actual: 1\n",
      "Image 2391 - Predicted: 6, Actual: 2\n",
      "Image 2392 - Predicted: 3, Actual: 3\n",
      "Image 2393 - Predicted: 9, Actual: 9\n",
      "Image 2394 - Predicted: 2, Actual: 2\n",
      "Image 2395 - Predicted: 2, Actual: 2\n",
      "Image 2396 - Predicted: 6, Actual: 6\n",
      "Image 2397 - Predicted: 4, Actual: 4\n",
      "Image 2398 - Predicted: 9, Actual: 4\n",
      "Image 2399 - Predicted: 1, Actual: 1\n",
      "Image 2400 - Predicted: 5, Actual: 5\n",
      "Image 2401 - Predicted: 2, Actual: 2\n",
      "Image 2402 - Predicted: 8, Actual: 8\n",
      "Image 2403 - Predicted: 6, Actual: 6\n",
      "Image 2404 - Predicted: 6, Actual: 8\n",
      "Image 2405 - Predicted: 2, Actual: 2\n",
      "Image 2406 - Predicted: 6, Actual: 6\n",
      "Image 2407 - Predicted: 9, Actual: 9\n",
      "Image 2408 - Predicted: 7, Actual: 7\n",
      "Image 2409 - Predicted: 4, Actual: 4\n",
      "Image 2410 - Predicted: 6, Actual: 6\n",
      "Image 2411 - Predicted: 1, Actual: 1\n",
      "Image 2412 - Predicted: 2, Actual: 2\n",
      "Image 2413 - Predicted: 6, Actual: 6\n",
      "Image 2414 - Predicted: 2, Actual: 2\n",
      "Image 2415 - Predicted: 6, Actual: 6\n",
      "Image 2416 - Predicted: 9, Actual: 9\n",
      "Image 2417 - Predicted: 9, Actual: 7\n",
      "Image 2418 - Predicted: 5, Actual: 5\n",
      "Image 2419 - Predicted: 9, Actual: 9\n",
      "Image 2420 - Predicted: 6, Actual: 6\n",
      "Image 2421 - Predicted: 7, Actual: 9\n",
      "Image 2422 - Predicted: 5, Actual: 5\n",
      "Image 2423 - Predicted: 1, Actual: 1\n",
      "Image 2424 - Predicted: 0, Actual: 0\n",
      "Image 2425 - Predicted: 8, Actual: 7\n",
      "Image 2426 - Predicted: 0, Actual: 0\n",
      "Image 2427 - Predicted: 0, Actual: 0\n",
      "Image 2428 - Predicted: 6, Actual: 6\n",
      "Image 2429 - Predicted: 8, Actual: 8\n",
      "Image 2430 - Predicted: 4, Actual: 4\n",
      "Image 2431 - Predicted: 8, Actual: 8\n",
      "Image 2432 - Predicted: 3, Actual: 3\n",
      "Image 2433 - Predicted: 8, Actual: 8\n",
      "Image 2434 - Predicted: 5, Actual: 5\n",
      "Image 2435 - Predicted: 3, Actual: 3\n",
      "Image 2436 - Predicted: 5, Actual: 5\n",
      "Image 2437 - Predicted: 8, Actual: 8\n",
      "Image 2438 - Predicted: 7, Actual: 9\n",
      "Image 2439 - Predicted: 8, Actual: 8\n",
      "Image 2440 - Predicted: 1, Actual: 1\n",
      "Image 2441 - Predicted: 0, Actual: 0\n",
      "Image 2442 - Predicted: 6, Actual: 6\n",
      "Image 2443 - Predicted: 5, Actual: 5\n",
      "Image 2444 - Predicted: 2, Actual: 2\n",
      "Image 2445 - Predicted: 1, Actual: 1\n",
      "Image 2446 - Predicted: 9, Actual: 8\n",
      "Image 2447 - Predicted: 9, Actual: 9\n",
      "Image 2448 - Predicted: 9, Actual: 7\n",
      "Image 2449 - Predicted: 3, Actual: 3\n",
      "Image 2450 - Predicted: 6, Actual: 6\n",
      "Image 2451 - Predicted: 6, Actual: 6\n",
      "Image 2452 - Predicted: 7, Actual: 9\n",
      "Image 2453 - Predicted: 7, Actual: 7\n",
      "Image 2454 - Predicted: 5, Actual: 5\n",
      "Image 2455 - Predicted: 5, Actual: 5\n",
      "Image 2456 - Predicted: 2, Actual: 2\n",
      "Image 2457 - Predicted: 9, Actual: 9\n",
      "Image 2458 - Predicted: 4, Actual: 8\n",
      "Image 2459 - Predicted: 8, Actual: 8\n",
      "Image 2460 - Predicted: 7, Actual: 7\n",
      "Image 2461 - Predicted: 2, Actual: 2\n",
      "Image 2462 - Predicted: 8, Actual: 8\n",
      "Image 2463 - Predicted: 6, Actual: 6\n",
      "Image 2464 - Predicted: 2, Actual: 2\n",
      "Image 2465 - Predicted: 8, Actual: 4\n",
      "Image 2466 - Predicted: 0, Actual: 0\n",
      "Image 2467 - Predicted: 2, Actual: 2\n",
      "Image 2468 - Predicted: 2, Actual: 6\n",
      "Image 2469 - Predicted: 6, Actual: 6\n",
      "Image 2470 - Predicted: 7, Actual: 7\n",
      "Image 2471 - Predicted: 4, Actual: 4\n",
      "Image 2472 - Predicted: 3, Actual: 3\n",
      "Image 2473 - Predicted: 4, Actual: 4\n",
      "Image 2474 - Predicted: 3, Actual: 3\n",
      "Image 2475 - Predicted: 5, Actual: 5\n",
      "Image 2476 - Predicted: 5, Actual: 4\n",
      "Image 2477 - Predicted: 9, Actual: 9\n",
      "Image 2478 - Predicted: 4, Actual: 4\n",
      "Image 2479 - Predicted: 1, Actual: 1\n",
      "Image 2480 - Predicted: 6, Actual: 6\n",
      "Image 2481 - Predicted: 6, Actual: 6\n",
      "Image 2482 - Predicted: 1, Actual: 1\n",
      "Image 2483 - Predicted: 8, Actual: 8\n",
      "Image 2484 - Predicted: 6, Actual: 6\n",
      "Image 2485 - Predicted: 5, Actual: 5\n",
      "Image 2486 - Predicted: 8, Actual: 8\n",
      "Image 2487 - Predicted: 5, Actual: 5\n",
      "Image 2488 - Predicted: 7, Actual: 7\n",
      "Image 2489 - Predicted: 2, Actual: 2\n",
      "Image 2490 - Predicted: 6, Actual: 6\n",
      "Image 2491 - Predicted: 7, Actual: 9\n",
      "Image 2492 - Predicted: 9, Actual: 9\n",
      "Image 2493 - Predicted: 7, Actual: 1\n",
      "Image 2494 - Predicted: 0, Actual: 0\n",
      "Image 2495 - Predicted: 3, Actual: 3\n",
      "Image 2496 - Predicted: 1, Actual: 1\n",
      "Image 2497 - Predicted: 9, Actual: 9\n",
      "Image 2498 - Predicted: 6, Actual: 6\n",
      "Image 2499 - Predicted: 4, Actual: 7\n",
      "Image 2500 - Predicted: 5, Actual: 5\n",
      "Image 2501 - Predicted: 0, Actual: 0\n",
      "Image 2502 - Predicted: 1, Actual: 1\n",
      "Image 2503 - Predicted: 6, Actual: 6\n",
      "Image 2504 - Predicted: 6, Actual: 6\n",
      "Image 2505 - Predicted: 2, Actual: 2\n",
      "Image 2506 - Predicted: 4, Actual: 4\n",
      "Image 2507 - Predicted: 6, Actual: 2\n",
      "Image 2508 - Predicted: 9, Actual: 9\n",
      "Image 2509 - Predicted: 9, Actual: 9\n",
      "Image 2510 - Predicted: 0, Actual: 1\n",
      "Image 2511 - Predicted: 7, Actual: 7\n",
      "Image 2512 - Predicted: 7, Actual: 7\n",
      "Image 2513 - Predicted: 8, Actual: 8\n",
      "Image 2514 - Predicted: 4, Actual: 8\n",
      "Image 2515 - Predicted: 8, Actual: 5\n",
      "Image 2516 - Predicted: 6, Actual: 6\n",
      "Image 2517 - Predicted: 1, Actual: 1\n",
      "Image 2518 - Predicted: 0, Actual: 0\n",
      "Image 2519 - Predicted: 0, Actual: 0\n",
      "Image 2520 - Predicted: 9, Actual: 9\n",
      "Image 2521 - Predicted: 2, Actual: 4\n",
      "Image 2522 - Predicted: 5, Actual: 4\n",
      "Image 2523 - Predicted: 4, Actual: 4\n",
      "Image 2524 - Predicted: 7, Actual: 7\n",
      "Image 2525 - Predicted: 8, Actual: 8\n",
      "Image 2526 - Predicted: 9, Actual: 9\n",
      "Image 2527 - Predicted: 1, Actual: 1\n",
      "Image 2528 - Predicted: 7, Actual: 7\n",
      "Image 2529 - Predicted: 6, Actual: 6\n",
      "Image 2530 - Predicted: 0, Actual: 0\n",
      "Image 2531 - Predicted: 6, Actual: 6\n",
      "Image 2532 - Predicted: 7, Actual: 7\n",
      "Image 2533 - Predicted: 9, Actual: 9\n",
      "Image 2534 - Predicted: 0, Actual: 0\n",
      "Image 2535 - Predicted: 7, Actual: 8\n",
      "Image 2536 - Predicted: 2, Actual: 2\n",
      "Image 2537 - Predicted: 0, Actual: 0\n",
      "Image 2538 - Predicted: 5, Actual: 4\n",
      "Image 2539 - Predicted: 5, Actual: 5\n",
      "Image 2540 - Predicted: 1, Actual: 1\n",
      "Image 2541 - Predicted: 9, Actual: 9\n",
      "Image 2542 - Predicted: 1, Actual: 1\n",
      "Image 2543 - Predicted: 9, Actual: 9\n",
      "Image 2544 - Predicted: 9, Actual: 9\n",
      "Image 2545 - Predicted: 6, Actual: 6\n",
      "Image 2546 - Predicted: 6, Actual: 6\n",
      "Image 2547 - Predicted: 3, Actual: 3\n",
      "Image 2548 - Predicted: 5, Actual: 5\n",
      "Image 2549 - Predicted: 7, Actual: 7\n",
      "Image 2550 - Predicted: 6, Actual: 6\n",
      "Image 2551 - Predicted: 4, Actual: 4\n",
      "Image 2552 - Predicted: 8, Actual: 8\n",
      "Image 2553 - Predicted: 6, Actual: 6\n",
      "Image 2554 - Predicted: 7, Actual: 2\n",
      "Image 2555 - Predicted: 6, Actual: 6\n",
      "Image 2556 - Predicted: 1, Actual: 1\n",
      "Image 2557 - Predicted: 1, Actual: 1\n",
      "Image 2558 - Predicted: 9, Actual: 8\n",
      "Image 2559 - Predicted: 3, Actual: 3\n",
      "Image 2560 - Predicted: 7, Actual: 9\n",
      "Image 2561 - Predicted: 7, Actual: 7\n",
      "Image 2562 - Predicted: 4, Actual: 5\n",
      "Image 2563 - Predicted: 3, Actual: 3\n",
      "Image 2564 - Predicted: 6, Actual: 6\n",
      "Image 2565 - Predicted: 8, Actual: 4\n",
      "Image 2566 - Predicted: 0, Actual: 0\n",
      "Image 2567 - Predicted: 9, Actual: 9\n",
      "Image 2568 - Predicted: 6, Actual: 6\n",
      "Image 2569 - Predicted: 6, Actual: 2\n",
      "Image 2570 - Predicted: 8, Actual: 8\n",
      "Image 2571 - Predicted: 6, Actual: 6\n",
      "Image 2572 - Predicted: 2, Actual: 2\n",
      "Image 2573 - Predicted: 0, Actual: 0\n",
      "Image 2574 - Predicted: 4, Actual: 4\n",
      "Image 2575 - Predicted: 5, Actual: 5\n",
      "Image 2576 - Predicted: 8, Actual: 8\n",
      "Image 2577 - Predicted: 7, Actual: 7\n",
      "Image 2578 - Predicted: 7, Actual: 7\n",
      "Image 2579 - Predicted: 5, Actual: 5\n",
      "Image 2580 - Predicted: 2, Actual: 2\n",
      "Image 2581 - Predicted: 3, Actual: 3\n",
      "Image 2582 - Predicted: 3, Actual: 3\n",
      "Image 2583 - Predicted: 3, Actual: 3\n",
      "Image 2584 - Predicted: 4, Actual: 4\n",
      "Image 2585 - Predicted: 7, Actual: 7\n",
      "Image 2586 - Predicted: 0, Actual: 0\n",
      "Image 2587 - Predicted: 3, Actual: 3\n",
      "Image 2588 - Predicted: 2, Actual: 2\n",
      "Image 2589 - Predicted: 2, Actual: 2\n",
      "Image 2590 - Predicted: 5, Actual: 5\n",
      "Image 2591 - Predicted: 7, Actual: 7\n",
      "Image 2592 - Predicted: 5, Actual: 5\n",
      "Image 2593 - Predicted: 0, Actual: 0\n",
      "Image 2594 - Predicted: 3, Actual: 1\n",
      "Image 2595 - Predicted: 9, Actual: 8\n",
      "Image 2596 - Predicted: 3, Actual: 1\n",
      "Image 2597 - Predicted: 5, Actual: 5\n",
      "Image 2598 - Predicted: 6, Actual: 6\n",
      "Image 2599 - Predicted: 8, Actual: 8\n",
      "Image 2600 - Predicted: 0, Actual: 0\n",
      "Image 2601 - Predicted: 6, Actual: 6\n",
      "Image 2602 - Predicted: 6, Actual: 6\n",
      "Image 2603 - Predicted: 3, Actual: 3\n",
      "Image 2604 - Predicted: 0, Actual: 0\n",
      "Image 2605 - Predicted: 8, Actual: 8\n",
      "Image 2606 - Predicted: 6, Actual: 6\n",
      "Image 2607 - Predicted: 5, Actual: 4\n",
      "Image 2608 - Predicted: 5, Actual: 4\n",
      "Image 2609 - Predicted: 3, Actual: 3\n",
      "Image 2610 - Predicted: 1, Actual: 1\n",
      "Image 2611 - Predicted: 4, Actual: 4\n",
      "Image 2612 - Predicted: 6, Actual: 6\n",
      "Image 2613 - Predicted: 8, Actual: 9\n",
      "Image 2614 - Predicted: 8, Actual: 8\n",
      "Image 2615 - Predicted: 3, Actual: 3\n",
      "Image 2616 - Predicted: 9, Actual: 4\n",
      "Image 2617 - Predicted: 6, Actual: 6\n",
      "Image 2618 - Predicted: 7, Actual: 7\n",
      "Image 2619 - Predicted: 8, Actual: 8\n",
      "Image 2620 - Predicted: 7, Actual: 7\n",
      "Image 2621 - Predicted: 8, Actual: 5\n",
      "Image 2622 - Predicted: 2, Actual: 2\n",
      "Image 2623 - Predicted: 7, Actual: 7\n",
      "Image 2624 - Predicted: 3, Actual: 3\n",
      "Image 2625 - Predicted: 2, Actual: 3\n",
      "Image 2626 - Predicted: 4, Actual: 4\n",
      "Image 2627 - Predicted: 5, Actual: 5\n",
      "Image 2628 - Predicted: 6, Actual: 6\n",
      "Image 2629 - Predicted: 6, Actual: 6\n",
      "Image 2630 - Predicted: 3, Actual: 3\n",
      "Image 2631 - Predicted: 7, Actual: 7\n",
      "Image 2632 - Predicted: 9, Actual: 7\n",
      "Image 2633 - Predicted: 3, Actual: 3\n",
      "Image 2634 - Predicted: 0, Actual: 0\n",
      "Image 2635 - Predicted: 2, Actual: 2\n",
      "Image 2636 - Predicted: 7, Actual: 7\n",
      "Image 2637 - Predicted: 7, Actual: 7\n",
      "Image 2638 - Predicted: 8, Actual: 8\n",
      "Image 2639 - Predicted: 4, Actual: 5\n",
      "Image 2640 - Predicted: 7, Actual: 7\n",
      "Image 2641 - Predicted: 6, Actual: 6\n",
      "Image 2642 - Predicted: 7, Actual: 7\n",
      "Image 2643 - Predicted: 0, Actual: 0\n",
      "Image 2644 - Predicted: 1, Actual: 1\n",
      "Image 2645 - Predicted: 9, Actual: 9\n",
      "Image 2646 - Predicted: 1, Actual: 1\n",
      "Image 2647 - Predicted: 6, Actual: 6\n",
      "Image 2648 - Predicted: 6, Actual: 2\n",
      "Image 2649 - Predicted: 5, Actual: 5\n",
      "Image 2650 - Predicted: 2, Actual: 2\n",
      "Image 2651 - Predicted: 2, Actual: 2\n",
      "Image 2652 - Predicted: 9, Actual: 5\n",
      "Image 2653 - Predicted: 6, Actual: 6\n",
      "Image 2654 - Predicted: 4, Actual: 4\n",
      "Image 2655 - Predicted: 6, Actual: 6\n",
      "Image 2656 - Predicted: 4, Actual: 8\n",
      "Image 2657 - Predicted: 4, Actual: 4\n",
      "Image 2658 - Predicted: 4, Actual: 4\n",
      "Image 2659 - Predicted: 7, Actual: 9\n",
      "Image 2660 - Predicted: 0, Actual: 0\n",
      "Image 2661 - Predicted: 6, Actual: 6\n",
      "Image 2662 - Predicted: 2, Actual: 6\n",
      "Image 2663 - Predicted: 0, Actual: 0\n",
      "Image 2664 - Predicted: 3, Actual: 3\n",
      "Image 2665 - Predicted: 8, Actual: 8\n",
      "Image 2666 - Predicted: 7, Actual: 7\n",
      "Image 2667 - Predicted: 5, Actual: 5\n",
      "Image 2668 - Predicted: 7, Actual: 7\n",
      "Image 2669 - Predicted: 4, Actual: 4\n",
      "Image 2670 - Predicted: 9, Actual: 9\n",
      "Image 2671 - Predicted: 2, Actual: 2\n",
      "Image 2672 - Predicted: 6, Actual: 6\n",
      "Image 2673 - Predicted: 6, Actual: 6\n",
      "Image 2674 - Predicted: 3, Actual: 3\n",
      "Image 2675 - Predicted: 3, Actual: 3\n",
      "Image 2676 - Predicted: 5, Actual: 5\n",
      "Image 2677 - Predicted: 1, Actual: 1\n",
      "Image 2678 - Predicted: 5, Actual: 5\n",
      "Image 2679 - Predicted: 0, Actual: 0\n",
      "Image 2680 - Predicted: 1, Actual: 3\n",
      "Image 2681 - Predicted: 0, Actual: 0\n",
      "Image 2682 - Predicted: 8, Actual: 8\n",
      "Image 2683 - Predicted: 0, Actual: 4\n",
      "Image 2684 - Predicted: 5, Actual: 5\n",
      "Image 2685 - Predicted: 8, Actual: 4\n",
      "Image 2686 - Predicted: 0, Actual: 1\n",
      "Image 2687 - Predicted: 6, Actual: 6\n",
      "Image 2688 - Predicted: 0, Actual: 0\n",
      "Image 2689 - Predicted: 6, Actual: 6\n",
      "Image 2690 - Predicted: 0, Actual: 0\n",
      "Image 2691 - Predicted: 7, Actual: 7\n",
      "Image 2692 - Predicted: 9, Actual: 5\n",
      "Image 2693 - Predicted: 0, Actual: 0\n",
      "Image 2694 - Predicted: 0, Actual: 0\n",
      "Image 2695 - Predicted: 7, Actual: 7\n",
      "Image 2696 - Predicted: 4, Actual: 5\n",
      "Image 2697 - Predicted: 3, Actual: 3\n",
      "Image 2698 - Predicted: 0, Actual: 0\n",
      "Image 2699 - Predicted: 7, Actual: 7\n",
      "Image 2700 - Predicted: 6, Actual: 6\n",
      "Image 2701 - Predicted: 8, Actual: 4\n",
      "Image 2702 - Predicted: 7, Actual: 7\n",
      "Image 2703 - Predicted: 4, Actual: 4\n",
      "Image 2704 - Predicted: 2, Actual: 2\n",
      "Image 2705 - Predicted: 7, Actual: 7\n",
      "Image 2706 - Predicted: 5, Actual: 5\n",
      "Image 2707 - Predicted: 8, Actual: 8\n",
      "Image 2708 - Predicted: 7, Actual: 7\n",
      "Image 2709 - Predicted: 0, Actual: 0\n",
      "Image 2710 - Predicted: 3, Actual: 3\n",
      "Image 2711 - Predicted: 2, Actual: 2\n",
      "Image 2712 - Predicted: 1, Actual: 1\n",
      "Image 2713 - Predicted: 8, Actual: 4\n",
      "Image 2714 - Predicted: 2, Actual: 2\n",
      "Image 2715 - Predicted: 7, Actual: 7\n",
      "Image 2716 - Predicted: 3, Actual: 3\n",
      "Image 2717 - Predicted: 5, Actual: 5\n",
      "Image 2718 - Predicted: 1, Actual: 1\n",
      "Image 2719 - Predicted: 1, Actual: 1\n",
      "Image 2720 - Predicted: 0, Actual: 0\n",
      "Image 2721 - Predicted: 8, Actual: 8\n",
      "Image 2722 - Predicted: 9, Actual: 4\n",
      "Image 2723 - Predicted: 7, Actual: 7\n",
      "Image 2724 - Predicted: 5, Actual: 5\n",
      "Image 2725 - Predicted: 0, Actual: 0\n",
      "Image 2726 - Predicted: 7, Actual: 7\n",
      "Image 2727 - Predicted: 3, Actual: 3\n",
      "Image 2728 - Predicted: 6, Actual: 6\n",
      "Image 2729 - Predicted: 3, Actual: 3\n",
      "Image 2730 - Predicted: 8, Actual: 8\n",
      "Image 2731 - Predicted: 4, Actual: 5\n",
      "Image 2732 - Predicted: 3, Actual: 3\n",
      "Image 2733 - Predicted: 1, Actual: 1\n",
      "Image 2734 - Predicted: 0, Actual: 0\n",
      "Image 2735 - Predicted: 1, Actual: 1\n",
      "Image 2736 - Predicted: 3, Actual: 3\n",
      "Image 2737 - Predicted: 6, Actual: 6\n",
      "Image 2738 - Predicted: 8, Actual: 7\n",
      "Image 2739 - Predicted: 8, Actual: 4\n",
      "Image 2740 - Predicted: 8, Actual: 4\n",
      "Image 2741 - Predicted: 7, Actual: 9\n",
      "Image 2742 - Predicted: 2, Actual: 2\n",
      "Image 2743 - Predicted: 8, Actual: 7\n",
      "Image 2744 - Predicted: 7, Actual: 7\n",
      "Image 2745 - Predicted: 7, Actual: 7\n",
      "Image 2746 - Predicted: 4, Actual: 5\n",
      "Image 2747 - Predicted: 7, Actual: 7\n",
      "Image 2748 - Predicted: 5, Actual: 5\n",
      "Image 2749 - Predicted: 2, Actual: 6\n",
      "Image 2750 - Predicted: 0, Actual: 0\n",
      "Image 2751 - Predicted: 3, Actual: 3\n",
      "Image 2752 - Predicted: 0, Actual: 0\n",
      "Image 2753 - Predicted: 0, Actual: 0\n",
      "Image 2754 - Predicted: 0, Actual: 0\n",
      "Image 2755 - Predicted: 3, Actual: 3\n",
      "Image 2756 - Predicted: 4, Actual: 4\n",
      "Image 2757 - Predicted: 4, Actual: 4\n",
      "Image 2758 - Predicted: 2, Actual: 2\n",
      "Image 2759 - Predicted: 4, Actual: 8\n",
      "Image 2760 - Predicted: 2, Actual: 2\n",
      "Image 2761 - Predicted: 5, Actual: 9\n",
      "Image 2762 - Predicted: 2, Actual: 2\n",
      "Image 2763 - Predicted: 9, Actual: 9\n",
      "Image 2764 - Predicted: 9, Actual: 9\n",
      "Image 2765 - Predicted: 3, Actual: 3\n",
      "Image 2766 - Predicted: 0, Actual: 0\n",
      "Image 2767 - Predicted: 7, Actual: 8\n",
      "Image 2768 - Predicted: 9, Actual: 9\n",
      "Image 2769 - Predicted: 6, Actual: 6\n",
      "Image 2770 - Predicted: 5, Actual: 5\n",
      "Image 2771 - Predicted: 3, Actual: 3\n",
      "Image 2772 - Predicted: 8, Actual: 4\n",
      "Image 2773 - Predicted: 3, Actual: 3\n",
      "Image 2774 - Predicted: 0, Actual: 0\n",
      "Image 2775 - Predicted: 2, Actual: 2\n",
      "Image 2776 - Predicted: 8, Actual: 4\n",
      "Image 2777 - Predicted: 6, Actual: 6\n",
      "Image 2778 - Predicted: 0, Actual: 0\n",
      "Image 2779 - Predicted: 4, Actual: 5\n",
      "Image 2780 - Predicted: 3, Actual: 3\n",
      "Image 2781 - Predicted: 8, Actual: 8\n",
      "Image 2782 - Predicted: 3, Actual: 3\n",
      "Image 2783 - Predicted: 5, Actual: 4\n",
      "Image 2784 - Predicted: 0, Actual: 0\n",
      "Image 2785 - Predicted: 5, Actual: 4\n",
      "Image 2786 - Predicted: 1, Actual: 1\n",
      "Image 2787 - Predicted: 8, Actual: 8\n",
      "Image 2788 - Predicted: 8, Actual: 8\n",
      "Image 2789 - Predicted: 1, Actual: 1\n",
      "Image 2790 - Predicted: 8, Actual: 8\n",
      "Image 2791 - Predicted: 7, Actual: 7\n",
      "Image 2792 - Predicted: 1, Actual: 1\n",
      "Image 2793 - Predicted: 9, Actual: 5\n",
      "Image 2794 - Predicted: 6, Actual: 6\n",
      "Image 2795 - Predicted: 7, Actual: 7\n",
      "Image 2796 - Predicted: 5, Actual: 5\n",
      "Image 2797 - Predicted: 8, Actual: 7\n",
      "Image 2798 - Predicted: 6, Actual: 6\n",
      "Image 2799 - Predicted: 6, Actual: 6\n",
      "Image 2800 - Predicted: 8, Actual: 8\n",
      "Image 2801 - Predicted: 2, Actual: 2\n",
      "Image 2802 - Predicted: 0, Actual: 0\n",
      "Image 2803 - Predicted: 6, Actual: 6\n",
      "Image 2804 - Predicted: 1, Actual: 1\n",
      "Image 2805 - Predicted: 9, Actual: 9\n",
      "Image 2806 - Predicted: 6, Actual: 6\n",
      "Image 2807 - Predicted: 3, Actual: 7\n",
      "Image 2808 - Predicted: 9, Actual: 9\n",
      "Image 2809 - Predicted: 7, Actual: 7\n",
      "Image 2810 - Predicted: 8, Actual: 8\n",
      "Image 2811 - Predicted: 2, Actual: 2\n",
      "Image 2812 - Predicted: 8, Actual: 8\n",
      "Image 2813 - Predicted: 7, Actual: 7\n",
      "Image 2814 - Predicted: 3, Actual: 3\n",
      "Image 2815 - Predicted: 1, Actual: 1\n",
      "Image 2816 - Predicted: 0, Actual: 0\n",
      "Image 2817 - Predicted: 6, Actual: 6\n",
      "Image 2818 - Predicted: 8, Actual: 4\n",
      "Image 2819 - Predicted: 6, Actual: 6\n",
      "Image 2820 - Predicted: 5, Actual: 4\n",
      "Image 2821 - Predicted: 6, Actual: 2\n",
      "Image 2822 - Predicted: 7, Actual: 9\n",
      "Image 2823 - Predicted: 0, Actual: 0\n",
      "Image 2824 - Predicted: 1, Actual: 1\n",
      "Image 2825 - Predicted: 5, Actual: 9\n",
      "Image 2826 - Predicted: 9, Actual: 9\n",
      "Image 2827 - Predicted: 2, Actual: 2\n",
      "Image 2828 - Predicted: 7, Actual: 9\n",
      "Image 2829 - Predicted: 3, Actual: 3\n",
      "Image 2830 - Predicted: 8, Actual: 8\n",
      "Image 2831 - Predicted: 0, Actual: 0\n",
      "Image 2832 - Predicted: 5, Actual: 4\n",
      "Image 2833 - Predicted: 6, Actual: 6\n",
      "Image 2834 - Predicted: 1, Actual: 1\n",
      "Image 2835 - Predicted: 5, Actual: 5\n",
      "Image 2836 - Predicted: 3, Actual: 3\n",
      "Image 2837 - Predicted: 6, Actual: 6\n",
      "Image 2838 - Predicted: 4, Actual: 4\n",
      "Image 2839 - Predicted: 5, Actual: 5\n",
      "Image 2840 - Predicted: 1, Actual: 1\n",
      "Image 2841 - Predicted: 6, Actual: 2\n",
      "Image 2842 - Predicted: 0, Actual: 0\n",
      "Image 2843 - Predicted: 9, Actual: 7\n",
      "Image 2844 - Predicted: 1, Actual: 1\n",
      "Image 2845 - Predicted: 0, Actual: 0\n",
      "Image 2846 - Predicted: 6, Actual: 2\n",
      "Image 2847 - Predicted: 9, Actual: 8\n",
      "Image 2848 - Predicted: 5, Actual: 4\n",
      "Image 2849 - Predicted: 4, Actual: 4\n",
      "Image 2850 - Predicted: 1, Actual: 1\n",
      "Image 2851 - Predicted: 5, Actual: 5\n",
      "Image 2852 - Predicted: 3, Actual: 3\n",
      "Image 2853 - Predicted: 2, Actual: 2\n",
      "Image 2854 - Predicted: 5, Actual: 5\n",
      "Image 2855 - Predicted: 2, Actual: 2\n",
      "Image 2856 - Predicted: 1, Actual: 1\n",
      "Image 2857 - Predicted: 3, Actual: 3\n",
      "Image 2858 - Predicted: 8, Actual: 4\n",
      "Image 2859 - Predicted: 5, Actual: 5\n",
      "Image 2860 - Predicted: 6, Actual: 2\n",
      "Image 2861 - Predicted: 0, Actual: 0\n",
      "Image 2862 - Predicted: 9, Actual: 9\n",
      "Image 2863 - Predicted: 0, Actual: 0\n",
      "Image 2864 - Predicted: 0, Actual: 0\n",
      "Image 2865 - Predicted: 8, Actual: 4\n",
      "Image 2866 - Predicted: 9, Actual: 9\n",
      "Image 2867 - Predicted: 0, Actual: 0\n",
      "Image 2868 - Predicted: 7, Actual: 7\n",
      "Image 2869 - Predicted: 9, Actual: 9\n",
      "Image 2870 - Predicted: 3, Actual: 3\n",
      "Image 2871 - Predicted: 8, Actual: 8\n",
      "Image 2872 - Predicted: 4, Actual: 5\n",
      "Image 2873 - Predicted: 0, Actual: 0\n",
      "Image 2874 - Predicted: 1, Actual: 3\n",
      "Image 2875 - Predicted: 3, Actual: 3\n",
      "Image 2876 - Predicted: 4, Actual: 5\n",
      "Image 2877 - Predicted: 3, Actual: 3\n",
      "Image 2878 - Predicted: 6, Actual: 2\n",
      "Image 2879 - Predicted: 7, Actual: 7\n",
      "Image 2880 - Predicted: 0, Actual: 0\n",
      "Image 2881 - Predicted: 9, Actual: 9\n",
      "Image 2882 - Predicted: 0, Actual: 0\n",
      "Image 2883 - Predicted: 8, Actual: 8\n",
      "Image 2884 - Predicted: 4, Actual: 4\n",
      "Image 2885 - Predicted: 8, Actual: 5\n",
      "Image 2886 - Predicted: 0, Actual: 0\n",
      "Image 2887 - Predicted: 6, Actual: 6\n",
      "Image 2888 - Predicted: 2, Actual: 2\n",
      "Image 2889 - Predicted: 0, Actual: 0\n",
      "Image 2890 - Predicted: 8, Actual: 8\n",
      "Image 2891 - Predicted: 0, Actual: 0\n",
      "Image 2892 - Predicted: 3, Actual: 3\n",
      "Image 2893 - Predicted: 5, Actual: 5\n",
      "Image 2894 - Predicted: 1, Actual: 1\n",
      "Image 2895 - Predicted: 6, Actual: 6\n",
      "Image 2896 - Predicted: 5, Actual: 5\n",
      "Image 2897 - Predicted: 0, Actual: 0\n",
      "Image 2898 - Predicted: 5, Actual: 8\n",
      "Image 2899 - Predicted: 4, Actual: 4\n",
      "Image 2900 - Predicted: 4, Actual: 9\n",
      "Image 2901 - Predicted: 3, Actual: 3\n",
      "Image 2902 - Predicted: 0, Actual: 0\n",
      "Image 2903 - Predicted: 5, Actual: 5\n",
      "Image 2904 - Predicted: 6, Actual: 6\n",
      "Image 2905 - Predicted: 6, Actual: 6\n",
      "Image 2906 - Predicted: 0, Actual: 0\n",
      "Image 2907 - Predicted: 0, Actual: 0\n",
      "Image 2908 - Predicted: 8, Actual: 8\n",
      "Image 2909 - Predicted: 8, Actual: 5\n",
      "Image 2910 - Predicted: 2, Actual: 2\n",
      "Image 2911 - Predicted: 0, Actual: 0\n",
      "Image 2912 - Predicted: 7, Actual: 9\n",
      "Image 2913 - Predicted: 6, Actual: 6\n",
      "Image 2914 - Predicted: 8, Actual: 8\n",
      "Image 2915 - Predicted: 0, Actual: 1\n",
      "Image 2916 - Predicted: 0, Actual: 0\n",
      "Image 2917 - Predicted: 0, Actual: 0\n",
      "Image 2918 - Predicted: 7, Actual: 7\n",
      "Image 2919 - Predicted: 0, Actual: 0\n",
      "Image 2920 - Predicted: 9, Actual: 7\n",
      "Image 2921 - Predicted: 7, Actual: 7\n",
      "Image 2922 - Predicted: 2, Actual: 2\n",
      "Image 2923 - Predicted: 6, Actual: 6\n",
      "Image 2924 - Predicted: 5, Actual: 5\n",
      "Image 2925 - Predicted: 0, Actual: 0\n",
      "Image 2926 - Predicted: 9, Actual: 9\n",
      "Image 2927 - Predicted: 2, Actual: 2\n",
      "Image 2928 - Predicted: 8, Actual: 8\n",
      "Image 2929 - Predicted: 3, Actual: 3\n",
      "Image 2930 - Predicted: 0, Actual: 0\n",
      "Image 2931 - Predicted: 7, Actual: 7\n",
      "Image 2932 - Predicted: 1, Actual: 1\n",
      "Image 2933 - Predicted: 2, Actual: 2\n",
      "Image 2934 - Predicted: 3, Actual: 3\n",
      "Image 2935 - Predicted: 5, Actual: 5\n",
      "Image 2936 - Predicted: 5, Actual: 5\n",
      "Image 2937 - Predicted: 1, Actual: 1\n",
      "Image 2938 - Predicted: 2, Actual: 2\n",
      "Image 2939 - Predicted: 4, Actual: 4\n",
      "Image 2940 - Predicted: 0, Actual: 0\n",
      "Image 2941 - Predicted: 3, Actual: 3\n",
      "Image 2942 - Predicted: 4, Actual: 4\n",
      "Image 2943 - Predicted: 4, Actual: 4\n",
      "Image 2944 - Predicted: 4, Actual: 4\n",
      "Image 2945 - Predicted: 9, Actual: 9\n",
      "Image 2946 - Predicted: 9, Actual: 9\n",
      "Image 2947 - Predicted: 9, Actual: 9\n",
      "Image 2948 - Predicted: 7, Actual: 7\n",
      "Image 2949 - Predicted: 9, Actual: 9\n",
      "Image 2950 - Predicted: 0, Actual: 0\n",
      "Image 2951 - Predicted: 9, Actual: 9\n",
      "Image 2952 - Predicted: 5, Actual: 5\n",
      "Image 2953 - Predicted: 1, Actual: 1\n",
      "Image 2954 - Predicted: 5, Actual: 5\n",
      "Image 2955 - Predicted: 5, Actual: 5\n",
      "Image 2956 - Predicted: 4, Actual: 4\n",
      "Image 2957 - Predicted: 3, Actual: 3\n",
      "Image 2958 - Predicted: 7, Actual: 7\n",
      "Image 2959 - Predicted: 1, Actual: 0\n",
      "Image 2960 - Predicted: 1, Actual: 1\n",
      "Image 2961 - Predicted: 8, Actual: 5\n",
      "Image 2962 - Predicted: 5, Actual: 4\n",
      "Image 2963 - Predicted: 8, Actual: 4\n",
      "Image 2964 - Predicted: 9, Actual: 9\n",
      "Image 2965 - Predicted: 6, Actual: 6\n",
      "Image 2966 - Predicted: 5, Actual: 8\n",
      "Image 2967 - Predicted: 1, Actual: 1\n",
      "Image 2968 - Predicted: 9, Actual: 9\n",
      "Image 2969 - Predicted: 2, Actual: 2\n",
      "Image 2970 - Predicted: 4, Actual: 8\n",
      "Image 2971 - Predicted: 8, Actual: 8\n",
      "Image 2972 - Predicted: 0, Actual: 0\n",
      "Image 2973 - Predicted: 4, Actual: 4\n",
      "Image 2974 - Predicted: 9, Actual: 9\n",
      "Image 2975 - Predicted: 7, Actual: 7\n",
      "Image 2976 - Predicted: 3, Actual: 3\n",
      "Image 2977 - Predicted: 7, Actual: 9\n",
      "Image 2978 - Predicted: 9, Actual: 8\n",
      "Image 2979 - Predicted: 3, Actual: 3\n",
      "Image 2980 - Predicted: 7, Actual: 7\n",
      "Image 2981 - Predicted: 1, Actual: 1\n",
      "Image 2982 - Predicted: 3, Actual: 3\n",
      "Image 2983 - Predicted: 9, Actual: 9\n",
      "Image 2984 - Predicted: 7, Actual: 7\n",
      "Image 2985 - Predicted: 1, Actual: 1\n",
      "Image 2986 - Predicted: 2, Actual: 2\n",
      "Image 2987 - Predicted: 5, Actual: 5\n",
      "Image 2988 - Predicted: 8, Actual: 8\n",
      "Image 2989 - Predicted: 4, Actual: 4\n",
      "Image 2990 - Predicted: 4, Actual: 4\n",
      "Image 2991 - Predicted: 9, Actual: 9\n",
      "Image 2992 - Predicted: 2, Actual: 2\n",
      "Image 2993 - Predicted: 6, Actual: 6\n",
      "Image 2994 - Predicted: 9, Actual: 9\n",
      "Image 2995 - Predicted: 8, Actual: 8\n",
      "Image 2996 - Predicted: 2, Actual: 2\n",
      "Image 2997 - Predicted: 9, Actual: 9\n",
      "Image 2998 - Predicted: 5, Actual: 5\n",
      "Image 2999 - Predicted: 1, Actual: 1\n",
      "Image 3000 - Predicted: 3, Actual: 3\n",
      "Image 3001 - Predicted: 5, Actual: 5\n",
      "Image 3002 - Predicted: 6, Actual: 6\n",
      "Image 3003 - Predicted: 7, Actual: 9\n",
      "Image 3004 - Predicted: 3, Actual: 3\n",
      "Image 3005 - Predicted: 7, Actual: 7\n",
      "Image 3006 - Predicted: 4, Actual: 5\n",
      "Image 3007 - Predicted: 9, Actual: 9\n",
      "Image 3008 - Predicted: 1, Actual: 1\n",
      "Image 3009 - Predicted: 3, Actual: 3\n",
      "Image 3010 - Predicted: 1, Actual: 1\n",
      "Image 3011 - Predicted: 0, Actual: 0\n",
      "Image 3012 - Predicted: 4, Actual: 8\n",
      "Image 3013 - Predicted: 3, Actual: 3\n",
      "Image 3014 - Predicted: 2, Actual: 2\n",
      "Image 3015 - Predicted: 2, Actual: 2\n",
      "Image 3016 - Predicted: 4, Actual: 4\n",
      "Image 3017 - Predicted: 6, Actual: 2\n",
      "Image 3018 - Predicted: 0, Actual: 0\n",
      "Image 3019 - Predicted: 2, Actual: 2\n",
      "Image 3020 - Predicted: 7, Actual: 7\n",
      "Image 3021 - Predicted: 5, Actual: 4\n",
      "Image 3022 - Predicted: 9, Actual: 7\n",
      "Image 3023 - Predicted: 0, Actual: 0\n",
      "Image 3024 - Predicted: 5, Actual: 5\n",
      "Image 3025 - Predicted: 1, Actual: 1\n",
      "Image 3026 - Predicted: 2, Actual: 2\n",
      "Image 3027 - Predicted: 4, Actual: 4\n",
      "Image 3028 - Predicted: 9, Actual: 9\n",
      "Image 3029 - Predicted: 1, Actual: 1\n",
      "Image 3030 - Predicted: 6, Actual: 6\n",
      "Image 3031 - Predicted: 3, Actual: 3\n",
      "Image 3032 - Predicted: 9, Actual: 5\n",
      "Image 3033 - Predicted: 5, Actual: 5\n",
      "Image 3034 - Predicted: 3, Actual: 3\n",
      "Image 3035 - Predicted: 0, Actual: 8\n",
      "Image 3036 - Predicted: 9, Actual: 9\n",
      "Image 3037 - Predicted: 7, Actual: 7\n",
      "Image 3038 - Predicted: 9, Actual: 9\n",
      "Image 3039 - Predicted: 8, Actual: 8\n",
      "Image 3040 - Predicted: 4, Actual: 4\n",
      "Image 3041 - Predicted: 6, Actual: 6\n",
      "Image 3042 - Predicted: 3, Actual: 3\n",
      "Image 3043 - Predicted: 1, Actual: 1\n",
      "Image 3044 - Predicted: 8, Actual: 8\n",
      "Image 3045 - Predicted: 0, Actual: 0\n",
      "Image 3046 - Predicted: 9, Actual: 7\n",
      "Image 3047 - Predicted: 9, Actual: 9\n",
      "Image 3048 - Predicted: 8, Actual: 8\n",
      "Image 3049 - Predicted: 7, Actual: 7\n",
      "Image 3050 - Predicted: 1, Actual: 1\n",
      "Image 3051 - Predicted: 4, Actual: 4\n",
      "Image 3052 - Predicted: 2, Actual: 2\n",
      "Image 3053 - Predicted: 2, Actual: 7\n",
      "Image 3054 - Predicted: 6, Actual: 6\n",
      "Image 3055 - Predicted: 5, Actual: 8\n",
      "Image 3056 - Predicted: 1, Actual: 1\n",
      "Image 3057 - Predicted: 3, Actual: 3\n",
      "Image 3058 - Predicted: 7, Actual: 7\n",
      "Image 3059 - Predicted: 9, Actual: 9\n",
      "Image 3060 - Predicted: 8, Actual: 8\n",
      "Image 3061 - Predicted: 9, Actual: 9\n",
      "Image 3062 - Predicted: 1, Actual: 1\n",
      "Image 3063 - Predicted: 9, Actual: 9\n",
      "Image 3064 - Predicted: 5, Actual: 4\n",
      "Image 3065 - Predicted: 1, Actual: 1\n",
      "Image 3066 - Predicted: 1, Actual: 1\n",
      "Image 3067 - Predicted: 2, Actual: 2\n",
      "Image 3068 - Predicted: 0, Actual: 0\n",
      "Image 3069 - Predicted: 2, Actual: 2\n",
      "Image 3070 - Predicted: 2, Actual: 2\n",
      "Image 3071 - Predicted: 5, Actual: 5\n",
      "Image 3072 - Predicted: 3, Actual: 3\n",
      "Image 3073 - Predicted: 2, Actual: 2\n",
      "Image 3074 - Predicted: 2, Actual: 2\n",
      "Image 3075 - Predicted: 1, Actual: 1\n",
      "Image 3076 - Predicted: 8, Actual: 8\n",
      "Image 3077 - Predicted: 1, Actual: 1\n",
      "Image 3078 - Predicted: 1, Actual: 1\n",
      "Image 3079 - Predicted: 1, Actual: 1\n",
      "Image 3080 - Predicted: 5, Actual: 5\n",
      "Image 3081 - Predicted: 8, Actual: 8\n",
      "Image 3082 - Predicted: 0, Actual: 7\n",
      "Image 3083 - Predicted: 5, Actual: 5\n",
      "Image 3084 - Predicted: 8, Actual: 8\n",
      "Image 3085 - Predicted: 7, Actual: 7\n",
      "Image 3086 - Predicted: 8, Actual: 8\n",
      "Image 3087 - Predicted: 2, Actual: 2\n",
      "Image 3088 - Predicted: 9, Actual: 9\n",
      "Image 3089 - Predicted: 3, Actual: 3\n",
      "Image 3090 - Predicted: 8, Actual: 8\n",
      "Image 3091 - Predicted: 3, Actual: 3\n",
      "Image 3092 - Predicted: 3, Actual: 3\n",
      "Image 3093 - Predicted: 4, Actual: 4\n",
      "Image 3094 - Predicted: 8, Actual: 4\n",
      "Image 3095 - Predicted: 3, Actual: 3\n",
      "Image 3096 - Predicted: 6, Actual: 2\n",
      "Image 3097 - Predicted: 1, Actual: 1\n",
      "Image 3098 - Predicted: 0, Actual: 0\n",
      "Image 3099 - Predicted: 9, Actual: 9\n",
      "Image 3100 - Predicted: 6, Actual: 6\n",
      "Image 3101 - Predicted: 7, Actual: 7\n",
      "Image 3102 - Predicted: 4, Actual: 4\n",
      "Image 3103 - Predicted: 1, Actual: 1\n",
      "Image 3104 - Predicted: 7, Actual: 7\n",
      "Image 3105 - Predicted: 8, Actual: 8\n",
      "Image 3106 - Predicted: 5, Actual: 5\n",
      "Image 3107 - Predicted: 7, Actual: 7\n",
      "Image 3108 - Predicted: 0, Actual: 0\n",
      "Image 3109 - Predicted: 8, Actual: 8\n",
      "Image 3110 - Predicted: 7, Actual: 7\n",
      "Image 3111 - Predicted: 8, Actual: 8\n",
      "Image 3112 - Predicted: 2, Actual: 2\n",
      "Image 3113 - Predicted: 8, Actual: 8\n",
      "Image 3114 - Predicted: 9, Actual: 9\n",
      "Image 3115 - Predicted: 2, Actual: 2\n",
      "Image 3116 - Predicted: 8, Actual: 8\n",
      "Image 3117 - Predicted: 6, Actual: 6\n",
      "Image 3118 - Predicted: 4, Actual: 9\n",
      "Image 3119 - Predicted: 2, Actual: 2\n",
      "Image 3120 - Predicted: 9, Actual: 9\n",
      "Image 3121 - Predicted: 6, Actual: 6\n",
      "Image 3122 - Predicted: 4, Actual: 4\n",
      "Image 3123 - Predicted: 5, Actual: 4\n",
      "Image 3124 - Predicted: 3, Actual: 3\n",
      "Image 3125 - Predicted: 9, Actual: 9\n",
      "Image 3126 - Predicted: 9, Actual: 8\n",
      "Image 3127 - Predicted: 2, Actual: 2\n",
      "Image 3128 - Predicted: 7, Actual: 7\n",
      "Image 3129 - Predicted: 4, Actual: 4\n",
      "Image 3130 - Predicted: 5, Actual: 5\n",
      "Image 3131 - Predicted: 5, Actual: 5\n",
      "Image 3132 - Predicted: 6, Actual: 6\n",
      "Image 3133 - Predicted: 7, Actual: 7\n",
      "Image 3134 - Predicted: 0, Actual: 0\n",
      "Image 3135 - Predicted: 3, Actual: 3\n",
      "Image 3136 - Predicted: 1, Actual: 1\n",
      "Image 3137 - Predicted: 4, Actual: 4\n",
      "Image 3138 - Predicted: 2, Actual: 2\n",
      "Image 3139 - Predicted: 2, Actual: 2\n",
      "Image 3140 - Predicted: 9, Actual: 7\n",
      "Image 3141 - Predicted: 2, Actual: 2\n",
      "Image 3142 - Predicted: 5, Actual: 4\n",
      "Image 3143 - Predicted: 8, Actual: 7\n",
      "Image 3144 - Predicted: 8, Actual: 8\n",
      "Image 3145 - Predicted: 1, Actual: 1\n",
      "Image 3146 - Predicted: 0, Actual: 0\n",
      "Image 3147 - Predicted: 5, Actual: 5\n",
      "Image 3148 - Predicted: 9, Actual: 9\n",
      "Image 3149 - Predicted: 6, Actual: 6\n",
      "Image 3150 - Predicted: 7, Actual: 7\n",
      "Image 3151 - Predicted: 2, Actual: 2\n",
      "Image 3152 - Predicted: 3, Actual: 3\n",
      "Image 3153 - Predicted: 4, Actual: 4\n",
      "Image 3154 - Predicted: 8, Actual: 8\n",
      "Image 3155 - Predicted: 7, Actual: 7\n",
      "Image 3156 - Predicted: 3, Actual: 3\n",
      "Image 3157 - Predicted: 7, Actual: 7\n",
      "Image 3158 - Predicted: 0, Actual: 0\n",
      "Image 3159 - Predicted: 9, Actual: 9\n",
      "Image 3160 - Predicted: 8, Actual: 8\n",
      "Image 3161 - Predicted: 8, Actual: 8\n",
      "Image 3162 - Predicted: 3, Actual: 3\n",
      "Image 3163 - Predicted: 0, Actual: 0\n",
      "Image 3164 - Predicted: 8, Actual: 8\n",
      "Image 3165 - Predicted: 1, Actual: 1\n",
      "Image 3166 - Predicted: 0, Actual: 0\n",
      "Image 3167 - Predicted: 7, Actual: 7\n",
      "Image 3168 - Predicted: 8, Actual: 5\n",
      "Image 3169 - Predicted: 6, Actual: 6\n",
      "Image 3170 - Predicted: 5, Actual: 5\n",
      "Image 3171 - Predicted: 7, Actual: 7\n",
      "Image 3172 - Predicted: 2, Actual: 2\n",
      "Image 3173 - Predicted: 7, Actual: 7\n",
      "Image 3174 - Predicted: 3, Actual: 3\n",
      "Image 3175 - Predicted: 2, Actual: 2\n",
      "Image 3176 - Predicted: 4, Actual: 4\n",
      "Image 3177 - Predicted: 4, Actual: 4\n",
      "Image 3178 - Predicted: 0, Actual: 0\n",
      "Image 3179 - Predicted: 7, Actual: 7\n",
      "Image 3180 - Predicted: 9, Actual: 9\n",
      "Image 3181 - Predicted: 6, Actual: 6\n",
      "Image 3182 - Predicted: 1, Actual: 1\n",
      "Image 3183 - Predicted: 2, Actual: 2\n",
      "Image 3184 - Predicted: 2, Actual: 2\n",
      "Image 3185 - Predicted: 7, Actual: 8\n",
      "Image 3186 - Predicted: 4, Actual: 4\n",
      "Image 3187 - Predicted: 4, Actual: 3\n",
      "Image 3188 - Predicted: 9, Actual: 9\n",
      "Image 3189 - Predicted: 7, Actual: 7\n",
      "Image 3190 - Predicted: 8, Actual: 8\n",
      "Image 3191 - Predicted: 9, Actual: 9\n",
      "Image 3192 - Predicted: 9, Actual: 9\n",
      "Image 3193 - Predicted: 8, Actual: 5\n",
      "Image 3194 - Predicted: 4, Actual: 4\n",
      "Image 3195 - Predicted: 0, Actual: 0\n",
      "Image 3196 - Predicted: 1, Actual: 1\n",
      "Image 3197 - Predicted: 1, Actual: 1\n",
      "Image 3198 - Predicted: 5, Actual: 5\n",
      "Image 3199 - Predicted: 5, Actual: 5\n",
      "Image 3200 - Predicted: 6, Actual: 6\n",
      "Image 3201 - Predicted: 0, Actual: 0\n",
      "Image 3202 - Predicted: 3, Actual: 3\n",
      "Image 3203 - Predicted: 7, Actual: 7\n",
      "Image 3204 - Predicted: 7, Actual: 7\n",
      "Image 3205 - Predicted: 0, Actual: 0\n",
      "Image 3206 - Predicted: 4, Actual: 4\n",
      "Image 3207 - Predicted: 1, Actual: 1\n",
      "Image 3208 - Predicted: 1, Actual: 1\n",
      "Image 3209 - Predicted: 5, Actual: 5\n",
      "Image 3210 - Predicted: 4, Actual: 4\n",
      "Image 3211 - Predicted: 9, Actual: 9\n",
      "Image 3212 - Predicted: 3, Actual: 3\n",
      "Image 3213 - Predicted: 7, Actual: 2\n",
      "Image 3214 - Predicted: 4, Actual: 4\n",
      "Image 3215 - Predicted: 0, Actual: 0\n",
      "Image 3216 - Predicted: 5, Actual: 5\n",
      "Image 3217 - Predicted: 7, Actual: 7\n",
      "Image 3218 - Predicted: 8, Actual: 8\n",
      "Image 3219 - Predicted: 1, Actual: 1\n",
      "Image 3220 - Predicted: 9, Actual: 9\n",
      "Image 3221 - Predicted: 8, Actual: 8\n",
      "Image 3222 - Predicted: 2, Actual: 2\n",
      "Image 3223 - Predicted: 6, Actual: 6\n",
      "Image 3224 - Predicted: 4, Actual: 7\n",
      "Image 3225 - Predicted: 4, Actual: 4\n",
      "Image 3226 - Predicted: 2, Actual: 2\n",
      "Image 3227 - Predicted: 2, Actual: 2\n",
      "Image 3228 - Predicted: 1, Actual: 1\n",
      "Image 3229 - Predicted: 5, Actual: 4\n",
      "Image 3230 - Predicted: 2, Actual: 2\n",
      "Image 3231 - Predicted: 3, Actual: 3\n",
      "Image 3232 - Predicted: 7, Actual: 7\n",
      "Image 3233 - Predicted: 0, Actual: 0\n",
      "Image 3234 - Predicted: 5, Actual: 5\n",
      "Image 3235 - Predicted: 6, Actual: 6\n",
      "Image 3236 - Predicted: 1, Actual: 1\n",
      "Image 3237 - Predicted: 3, Actual: 3\n",
      "Image 3238 - Predicted: 7, Actual: 7\n",
      "Image 3239 - Predicted: 5, Actual: 5\n",
      "Image 3240 - Predicted: 5, Actual: 5\n",
      "Image 3241 - Predicted: 6, Actual: 1\n",
      "Image 3242 - Predicted: 1, Actual: 1\n",
      "Image 3243 - Predicted: 0, Actual: 0\n",
      "Image 3244 - Predicted: 3, Actual: 3\n",
      "Image 3245 - Predicted: 7, Actual: 7\n",
      "Image 3246 - Predicted: 6, Actual: 6\n",
      "Image 3247 - Predicted: 2, Actual: 2\n",
      "Image 3248 - Predicted: 9, Actual: 9\n",
      "Image 3249 - Predicted: 6, Actual: 6\n",
      "Image 3250 - Predicted: 7, Actual: 7\n",
      "Image 3251 - Predicted: 1, Actual: 1\n",
      "Image 3252 - Predicted: 3, Actual: 3\n",
      "Image 3253 - Predicted: 0, Actual: 0\n",
      "Image 3254 - Predicted: 6, Actual: 6\n",
      "Image 3255 - Predicted: 9, Actual: 9\n",
      "Image 3256 - Predicted: 1, Actual: 1\n",
      "Image 3257 - Predicted: 3, Actual: 3\n",
      "Image 3258 - Predicted: 4, Actual: 5\n",
      "Image 3259 - Predicted: 1, Actual: 1\n",
      "Image 3260 - Predicted: 3, Actual: 3\n",
      "Image 3261 - Predicted: 1, Actual: 1\n",
      "Image 3262 - Predicted: 1, Actual: 1\n",
      "Image 3263 - Predicted: 9, Actual: 9\n",
      "Image 3264 - Predicted: 9, Actual: 9\n",
      "Image 3265 - Predicted: 2, Actual: 2\n",
      "Image 3266 - Predicted: 8, Actual: 8\n",
      "Image 3267 - Predicted: 9, Actual: 8\n",
      "Image 3268 - Predicted: 6, Actual: 6\n",
      "Image 3269 - Predicted: 1, Actual: 1\n",
      "Image 3270 - Predicted: 6, Actual: 6\n",
      "Image 3271 - Predicted: 1, Actual: 1\n",
      "Image 3272 - Predicted: 3, Actual: 3\n",
      "Image 3273 - Predicted: 3, Actual: 3\n",
      "Image 3274 - Predicted: 9, Actual: 9\n",
      "Image 3275 - Predicted: 3, Actual: 3\n",
      "Image 3276 - Predicted: 0, Actual: 0\n",
      "Image 3277 - Predicted: 8, Actual: 8\n",
      "Image 3278 - Predicted: 5, Actual: 5\n",
      "Image 3279 - Predicted: 2, Actual: 2\n",
      "Image 3280 - Predicted: 0, Actual: 0\n",
      "Image 3281 - Predicted: 8, Actual: 8\n",
      "Image 3282 - Predicted: 0, Actual: 0\n",
      "Image 3283 - Predicted: 9, Actual: 9\n",
      "Image 3284 - Predicted: 0, Actual: 0\n",
      "Image 3285 - Predicted: 9, Actual: 9\n",
      "Image 3286 - Predicted: 1, Actual: 1\n",
      "Image 3287 - Predicted: 0, Actual: 0\n",
      "Image 3288 - Predicted: 0, Actual: 0\n",
      "Image 3289 - Predicted: 3, Actual: 3\n",
      "Image 3290 - Predicted: 2, Actual: 2\n",
      "Image 3291 - Predicted: 1, Actual: 1\n",
      "Image 3292 - Predicted: 7, Actual: 7\n",
      "Image 3293 - Predicted: 6, Actual: 6\n",
      "Image 3294 - Predicted: 0, Actual: 0\n",
      "Image 3295 - Predicted: 0, Actual: 0\n",
      "Image 3296 - Predicted: 1, Actual: 1\n",
      "Image 3297 - Predicted: 9, Actual: 9\n",
      "Image 3298 - Predicted: 6, Actual: 6\n",
      "Image 3299 - Predicted: 9, Actual: 8\n",
      "Image 3300 - Predicted: 2, Actual: 2\n",
      "Image 3301 - Predicted: 3, Actual: 3\n",
      "Image 3302 - Predicted: 3, Actual: 3\n",
      "Image 3303 - Predicted: 9, Actual: 8\n",
      "Image 3304 - Predicted: 9, Actual: 9\n",
      "Image 3305 - Predicted: 4, Actual: 5\n",
      "Image 3306 - Predicted: 3, Actual: 3\n",
      "Image 3307 - Predicted: 4, Actual: 4\n",
      "Image 3308 - Predicted: 4, Actual: 5\n",
      "Image 3309 - Predicted: 4, Actual: 4\n",
      "Image 3310 - Predicted: 0, Actual: 0\n",
      "Image 3311 - Predicted: 9, Actual: 9\n",
      "Image 3312 - Predicted: 8, Actual: 8\n",
      "Image 3313 - Predicted: 4, Actual: 4\n",
      "Image 3314 - Predicted: 9, Actual: 9\n",
      "Image 3315 - Predicted: 1, Actual: 1\n",
      "Image 3316 - Predicted: 9, Actual: 9\n",
      "Image 3317 - Predicted: 9, Actual: 9\n",
      "Image 3318 - Predicted: 7, Actual: 7\n",
      "Image 3319 - Predicted: 9, Actual: 9\n",
      "Image 3320 - Predicted: 6, Actual: 6\n",
      "Image 3321 - Predicted: 5, Actual: 8\n",
      "Image 3322 - Predicted: 2, Actual: 6\n",
      "Image 3323 - Predicted: 0, Actual: 0\n",
      "Image 3324 - Predicted: 3, Actual: 3\n",
      "Image 3325 - Predicted: 7, Actual: 7\n",
      "Image 3326 - Predicted: 1, Actual: 1\n",
      "Image 3327 - Predicted: 5, Actual: 5\n",
      "Image 3328 - Predicted: 5, Actual: 5\n",
      "Image 3329 - Predicted: 7, Actual: 7\n",
      "Image 3330 - Predicted: 7, Actual: 7\n",
      "Image 3331 - Predicted: 7, Actual: 7\n",
      "Image 3332 - Predicted: 5, Actual: 5\n",
      "Image 3333 - Predicted: 0, Actual: 0\n",
      "Image 3334 - Predicted: 4, Actual: 4\n",
      "Image 3335 - Predicted: 6, Actual: 6\n",
      "Image 3336 - Predicted: 5, Actual: 5\n",
      "Image 3337 - Predicted: 6, Actual: 6\n",
      "Image 3338 - Predicted: 9, Actual: 4\n",
      "Image 3339 - Predicted: 6, Actual: 6\n",
      "Image 3340 - Predicted: 6, Actual: 6\n",
      "Image 3341 - Predicted: 9, Actual: 9\n",
      "Image 3342 - Predicted: 5, Actual: 9\n",
      "Image 3343 - Predicted: 2, Actual: 2\n",
      "Image 3344 - Predicted: 4, Actual: 9\n",
      "Image 3345 - Predicted: 2, Actual: 2\n",
      "Image 3346 - Predicted: 3, Actual: 3\n",
      "Image 3347 - Predicted: 0, Actual: 0\n",
      "Image 3348 - Predicted: 7, Actual: 7\n",
      "Image 3349 - Predicted: 5, Actual: 5\n",
      "Image 3350 - Predicted: 5, Actual: 5\n",
      "Image 3351 - Predicted: 9, Actual: 9\n",
      "Image 3352 - Predicted: 4, Actual: 4\n",
      "Image 3353 - Predicted: 6, Actual: 2\n",
      "Image 3354 - Predicted: 4, Actual: 4\n",
      "Image 3355 - Predicted: 0, Actual: 0\n",
      "Image 3356 - Predicted: 5, Actual: 5\n",
      "Image 3357 - Predicted: 8, Actual: 9\n",
      "Image 3358 - Predicted: 9, Actual: 9\n",
      "Image 3359 - Predicted: 3, Actual: 3\n",
      "Image 3360 - Predicted: 5, Actual: 9\n",
      "Image 3361 - Predicted: 5, Actual: 5\n",
      "Image 3362 - Predicted: 2, Actual: 2\n",
      "Image 3363 - Predicted: 0, Actual: 0\n",
      "Image 3364 - Predicted: 3, Actual: 3\n",
      "Image 3365 - Predicted: 1, Actual: 1\n",
      "Image 3366 - Predicted: 1, Actual: 1\n",
      "Image 3367 - Predicted: 2, Actual: 2\n",
      "Image 3368 - Predicted: 9, Actual: 4\n",
      "Image 3369 - Predicted: 8, Actual: 8\n",
      "Image 3370 - Predicted: 8, Actual: 8\n",
      "Image 3371 - Predicted: 1, Actual: 1\n",
      "Image 3372 - Predicted: 7, Actual: 7\n",
      "Image 3373 - Predicted: 0, Actual: 0\n",
      "Image 3374 - Predicted: 0, Actual: 0\n",
      "Image 3375 - Predicted: 4, Actual: 9\n",
      "Image 3376 - Predicted: 7, Actual: 8\n",
      "Image 3377 - Predicted: 1, Actual: 1\n",
      "Image 3378 - Predicted: 7, Actual: 2\n",
      "Image 3379 - Predicted: 0, Actual: 0\n",
      "Image 3380 - Predicted: 3, Actual: 3\n",
      "Image 3381 - Predicted: 3, Actual: 3\n",
      "Image 3382 - Predicted: 7, Actual: 0\n",
      "Image 3383 - Predicted: 5, Actual: 5\n",
      "Image 3384 - Predicted: 4, Actual: 4\n",
      "Image 3385 - Predicted: 8, Actual: 8\n",
      "Image 3386 - Predicted: 4, Actual: 4\n",
      "Image 3387 - Predicted: 3, Actual: 3\n",
      "Image 3388 - Predicted: 5, Actual: 8\n",
      "Image 3389 - Predicted: 0, Actual: 0\n",
      "Image 3390 - Predicted: 6, Actual: 6\n",
      "Image 3391 - Predicted: 3, Actual: 3\n",
      "Image 3392 - Predicted: 8, Actual: 7\n",
      "Image 3393 - Predicted: 3, Actual: 3\n",
      "Image 3394 - Predicted: 7, Actual: 6\n",
      "Image 3395 - Predicted: 7, Actual: 7\n",
      "Image 3396 - Predicted: 3, Actual: 3\n",
      "Image 3397 - Predicted: 5, Actual: 4\n",
      "Image 3398 - Predicted: 0, Actual: 0\n",
      "Image 3399 - Predicted: 0, Actual: 0\n",
      "Image 3400 - Predicted: 4, Actual: 4\n",
      "Image 3401 - Predicted: 1, Actual: 1\n",
      "Image 3402 - Predicted: 5, Actual: 5\n",
      "Image 3403 - Predicted: 2, Actual: 2\n",
      "Image 3404 - Predicted: 9, Actual: 9\n",
      "Image 3405 - Predicted: 1, Actual: 1\n",
      "Image 3406 - Predicted: 3, Actual: 3\n",
      "Image 3407 - Predicted: 1, Actual: 1\n",
      "Image 3408 - Predicted: 5, Actual: 8\n",
      "Image 3409 - Predicted: 5, Actual: 8\n",
      "Image 3410 - Predicted: 8, Actual: 5\n",
      "Image 3411 - Predicted: 6, Actual: 2\n",
      "Image 3412 - Predicted: 7, Actual: 9\n",
      "Image 3413 - Predicted: 6, Actual: 1\n",
      "Image 3414 - Predicted: 6, Actual: 6\n",
      "Image 3415 - Predicted: 6, Actual: 6\n",
      "Image 3416 - Predicted: 5, Actual: 5\n",
      "Image 3417 - Predicted: 9, Actual: 4\n",
      "Image 3418 - Predicted: 4, Actual: 4\n",
      "Image 3419 - Predicted: 4, Actual: 4\n",
      "Image 3420 - Predicted: 0, Actual: 0\n",
      "Image 3421 - Predicted: 3, Actual: 7\n",
      "Image 3422 - Predicted: 4, Actual: 4\n",
      "Image 3423 - Predicted: 6, Actual: 6\n",
      "Image 3424 - Predicted: 3, Actual: 3\n",
      "Image 3425 - Predicted: 7, Actual: 7\n",
      "Image 3426 - Predicted: 8, Actual: 8\n",
      "Image 3427 - Predicted: 4, Actual: 4\n",
      "Image 3428 - Predicted: 9, Actual: 4\n",
      "Image 3429 - Predicted: 7, Actual: 9\n",
      "Image 3430 - Predicted: 6, Actual: 6\n",
      "Image 3431 - Predicted: 6, Actual: 6\n",
      "Image 3432 - Predicted: 5, Actual: 5\n",
      "Image 3433 - Predicted: 5, Actual: 5\n",
      "Image 3434 - Predicted: 0, Actual: 1\n",
      "Image 3435 - Predicted: 7, Actual: 7\n",
      "Image 3436 - Predicted: 2, Actual: 2\n",
      "Image 3437 - Predicted: 2, Actual: 2\n",
      "Image 3438 - Predicted: 0, Actual: 0\n",
      "Image 3439 - Predicted: 7, Actual: 7\n",
      "Image 3440 - Predicted: 1, Actual: 1\n",
      "Image 3441 - Predicted: 3, Actual: 3\n",
      "Image 3442 - Predicted: 8, Actual: 4\n",
      "Image 3443 - Predicted: 5, Actual: 4\n",
      "Image 3444 - Predicted: 1, Actual: 1\n",
      "Image 3445 - Predicted: 6, Actual: 6\n",
      "Image 3446 - Predicted: 3, Actual: 3\n",
      "Image 3447 - Predicted: 8, Actual: 9\n",
      "Image 3448 - Predicted: 8, Actual: 4\n",
      "Image 3449 - Predicted: 0, Actual: 0\n",
      "Image 3450 - Predicted: 7, Actual: 7\n",
      "Image 3451 - Predicted: 9, Actual: 7\n",
      "Image 3452 - Predicted: 0, Actual: 0\n",
      "Image 3453 - Predicted: 2, Actual: 2\n",
      "Image 3454 - Predicted: 0, Actual: 0\n",
      "Image 3455 - Predicted: 1, Actual: 1\n",
      "Image 3456 - Predicted: 9, Actual: 5\n",
      "Image 3457 - Predicted: 3, Actual: 3\n",
      "Image 3458 - Predicted: 3, Actual: 3\n",
      "Image 3459 - Predicted: 9, Actual: 9\n",
      "Image 3460 - Predicted: 5, Actual: 5\n",
      "Image 3461 - Predicted: 9, Actual: 7\n",
      "Image 3462 - Predicted: 9, Actual: 9\n",
      "Image 3463 - Predicted: 2, Actual: 2\n",
      "Image 3464 - Predicted: 1, Actual: 1\n",
      "Image 3465 - Predicted: 2, Actual: 2\n",
      "Image 3466 - Predicted: 4, Actual: 5\n",
      "Image 3467 - Predicted: 8, Actual: 8\n",
      "Image 3468 - Predicted: 6, Actual: 6\n",
      "Image 3469 - Predicted: 7, Actual: 7\n",
      "Image 3470 - Predicted: 7, Actual: 2\n",
      "Image 3471 - Predicted: 1, Actual: 1\n",
      "Image 3472 - Predicted: 4, Actual: 8\n",
      "Image 3473 - Predicted: 7, Actual: 8\n",
      "Image 3474 - Predicted: 7, Actual: 7\n",
      "Image 3475 - Predicted: 8, Actual: 8\n",
      "Image 3476 - Predicted: 5, Actual: 5\n",
      "Image 3477 - Predicted: 9, Actual: 9\n",
      "Image 3478 - Predicted: 8, Actual: 5\n",
      "Image 3479 - Predicted: 0, Actual: 0\n",
      "Image 3480 - Predicted: 9, Actual: 9\n",
      "Image 3481 - Predicted: 6, Actual: 6\n",
      "Image 3482 - Predicted: 7, Actual: 7\n",
      "Image 3483 - Predicted: 8, Actual: 8\n",
      "Image 3484 - Predicted: 3, Actual: 3\n",
      "Image 3485 - Predicted: 3, Actual: 3\n",
      "Image 3486 - Predicted: 2, Actual: 2\n",
      "Image 3487 - Predicted: 8, Actual: 8\n",
      "Image 3488 - Predicted: 9, Actual: 9\n",
      "Image 3489 - Predicted: 1, Actual: 0\n",
      "Image 3490 - Predicted: 4, Actual: 4\n",
      "Image 3491 - Predicted: 8, Actual: 8\n",
      "Image 3492 - Predicted: 1, Actual: 1\n",
      "Image 3493 - Predicted: 1, Actual: 1\n",
      "Image 3494 - Predicted: 8, Actual: 8\n",
      "Image 3495 - Predicted: 8, Actual: 8\n",
      "Image 3496 - Predicted: 8, Actual: 8\n",
      "Image 3497 - Predicted: 9, Actual: 9\n",
      "Image 3498 - Predicted: 1, Actual: 1\n",
      "Image 3499 - Predicted: 3, Actual: 3\n",
      "Image 3500 - Predicted: 1, Actual: 1\n",
      "Image 3501 - Predicted: 6, Actual: 5\n",
      "Image 3502 - Predicted: 4, Actual: 4\n",
      "Image 3503 - Predicted: 7, Actual: 7\n",
      "Image 3504 - Predicted: 3, Actual: 3\n",
      "Image 3505 - Predicted: 0, Actual: 0\n",
      "Image 3506 - Predicted: 3, Actual: 3\n",
      "Image 3507 - Predicted: 5, Actual: 4\n",
      "Image 3508 - Predicted: 1, Actual: 1\n",
      "Image 3509 - Predicted: 8, Actual: 8\n",
      "Image 3510 - Predicted: 6, Actual: 6\n",
      "Image 3511 - Predicted: 1, Actual: 1\n",
      "Image 3512 - Predicted: 2, Actual: 2\n",
      "Image 3513 - Predicted: 1, Actual: 1\n",
      "Image 3514 - Predicted: 8, Actual: 8\n",
      "Image 3515 - Predicted: 2, Actual: 2\n",
      "Image 3516 - Predicted: 0, Actual: 0\n",
      "Image 3517 - Predicted: 0, Actual: 0\n",
      "Image 3518 - Predicted: 7, Actual: 7\n",
      "Image 3519 - Predicted: 4, Actual: 8\n",
      "Image 3520 - Predicted: 2, Actual: 2\n",
      "Image 3521 - Predicted: 6, Actual: 6\n",
      "Image 3522 - Predicted: 3, Actual: 3\n",
      "Image 3523 - Predicted: 1, Actual: 1\n",
      "Image 3524 - Predicted: 7, Actual: 2\n",
      "Image 3525 - Predicted: 0, Actual: 0\n",
      "Image 3526 - Predicted: 6, Actual: 6\n",
      "Image 3527 - Predicted: 9, Actual: 8\n",
      "Image 3528 - Predicted: 8, Actual: 8\n",
      "Image 3529 - Predicted: 5, Actual: 5\n",
      "Image 3530 - Predicted: 9, Actual: 8\n",
      "Image 3531 - Predicted: 2, Actual: 2\n",
      "Image 3532 - Predicted: 2, Actual: 2\n",
      "Image 3533 - Predicted: 0, Actual: 0\n",
      "Image 3534 - Predicted: 6, Actual: 6\n",
      "Image 3535 - Predicted: 3, Actual: 3\n",
      "Image 3536 - Predicted: 0, Actual: 0\n",
      "Image 3537 - Predicted: 9, Actual: 9\n",
      "Image 3538 - Predicted: 6, Actual: 6\n",
      "Image 3539 - Predicted: 7, Actual: 7\n",
      "Image 3540 - Predicted: 2, Actual: 2\n",
      "Image 3541 - Predicted: 0, Actual: 0\n",
      "Image 3542 - Predicted: 5, Actual: 5\n",
      "Image 3543 - Predicted: 1, Actual: 1\n",
      "Image 3544 - Predicted: 9, Actual: 4\n",
      "Image 3545 - Predicted: 9, Actual: 8\n",
      "Image 3546 - Predicted: 2, Actual: 2\n",
      "Image 3547 - Predicted: 1, Actual: 1\n",
      "Image 3548 - Predicted: 9, Actual: 9\n",
      "Image 3549 - Predicted: 7, Actual: 7\n",
      "Image 3550 - Predicted: 4, Actual: 7\n",
      "Image 3551 - Predicted: 6, Actual: 6\n",
      "Image 3552 - Predicted: 3, Actual: 3\n",
      "Image 3553 - Predicted: 1, Actual: 1\n",
      "Image 3554 - Predicted: 1, Actual: 1\n",
      "Image 3555 - Predicted: 4, Actual: 4\n",
      "Image 3556 - Predicted: 7, Actual: 7\n",
      "Image 3557 - Predicted: 6, Actual: 2\n",
      "Image 3558 - Predicted: 7, Actual: 8\n",
      "Image 3559 - Predicted: 1, Actual: 1\n",
      "Image 3560 - Predicted: 8, Actual: 8\n",
      "Image 3561 - Predicted: 8, Actual: 8\n",
      "Image 3562 - Predicted: 5, Actual: 5\n",
      "Image 3563 - Predicted: 7, Actual: 7\n",
      "Image 3564 - Predicted: 9, Actual: 7\n",
      "Image 3565 - Predicted: 4, Actual: 4\n",
      "Image 3566 - Predicted: 8, Actual: 8\n",
      "Image 3567 - Predicted: 1, Actual: 1\n",
      "Image 3568 - Predicted: 0, Actual: 0\n",
      "Image 3569 - Predicted: 3, Actual: 3\n",
      "Image 3570 - Predicted: 2, Actual: 2\n",
      "Image 3571 - Predicted: 8, Actual: 8\n",
      "Image 3572 - Predicted: 4, Actual: 4\n",
      "Image 3573 - Predicted: 7, Actual: 7\n",
      "Image 3574 - Predicted: 7, Actual: 7\n",
      "Image 3575 - Predicted: 0, Actual: 0\n",
      "Image 3576 - Predicted: 6, Actual: 6\n",
      "Image 3577 - Predicted: 0, Actual: 0\n",
      "Image 3578 - Predicted: 3, Actual: 3\n",
      "Image 3579 - Predicted: 6, Actual: 2\n",
      "Image 3580 - Predicted: 4, Actual: 9\n",
      "Image 3581 - Predicted: 5, Actual: 4\n",
      "Image 3582 - Predicted: 7, Actual: 4\n",
      "Image 3583 - Predicted: 9, Actual: 7\n",
      "Image 3584 - Predicted: 5, Actual: 5\n",
      "Image 3585 - Predicted: 8, Actual: 4\n",
      "Image 3586 - Predicted: 8, Actual: 8\n",
      "Image 3587 - Predicted: 8, Actual: 8\n",
      "Image 3588 - Predicted: 0, Actual: 0\n",
      "Image 3589 - Predicted: 9, Actual: 8\n",
      "Image 3590 - Predicted: 4, Actual: 4\n",
      "Image 3591 - Predicted: 3, Actual: 3\n",
      "Image 3592 - Predicted: 3, Actual: 3\n",
      "Image 3593 - Predicted: 2, Actual: 2\n",
      "Image 3594 - Predicted: 6, Actual: 6\n",
      "Image 3595 - Predicted: 2, Actual: 2\n",
      "Image 3596 - Predicted: 4, Actual: 5\n",
      "Image 3597 - Predicted: 5, Actual: 9\n",
      "Image 3598 - Predicted: 6, Actual: 6\n",
      "Image 3599 - Predicted: 6, Actual: 6\n",
      "Image 3600 - Predicted: 8, Actual: 8\n",
      "Image 3601 - Predicted: 3, Actual: 3\n",
      "Image 3602 - Predicted: 0, Actual: 0\n",
      "Image 3603 - Predicted: 8, Actual: 8\n",
      "Image 3604 - Predicted: 8, Actual: 8\n",
      "Image 3605 - Predicted: 5, Actual: 5\n",
      "Image 3606 - Predicted: 3, Actual: 3\n",
      "Image 3607 - Predicted: 7, Actual: 7\n",
      "Image 3608 - Predicted: 5, Actual: 5\n",
      "Image 3609 - Predicted: 3, Actual: 3\n",
      "Image 3610 - Predicted: 3, Actual: 3\n",
      "Image 3611 - Predicted: 4, Actual: 5\n",
      "Image 3612 - Predicted: 3, Actual: 3\n",
      "Image 3613 - Predicted: 8, Actual: 8\n",
      "Image 3614 - Predicted: 0, Actual: 0\n",
      "Image 3615 - Predicted: 7, Actual: 7\n",
      "Image 3616 - Predicted: 0, Actual: 0\n",
      "Image 3617 - Predicted: 5, Actual: 5\n",
      "Image 3618 - Predicted: 6, Actual: 6\n",
      "Image 3619 - Predicted: 9, Actual: 9\n",
      "Image 3620 - Predicted: 4, Actual: 4\n",
      "Image 3621 - Predicted: 3, Actual: 3\n",
      "Image 3622 - Predicted: 8, Actual: 8\n",
      "Image 3623 - Predicted: 9, Actual: 9\n",
      "Image 3624 - Predicted: 8, Actual: 8\n",
      "Image 3625 - Predicted: 5, Actual: 5\n",
      "Image 3626 - Predicted: 5, Actual: 5\n",
      "Image 3627 - Predicted: 3, Actual: 3\n",
      "Image 3628 - Predicted: 5, Actual: 5\n",
      "Image 3629 - Predicted: 3, Actual: 3\n",
      "Image 3630 - Predicted: 2, Actual: 2\n",
      "Image 3631 - Predicted: 5, Actual: 4\n",
      "Image 3632 - Predicted: 3, Actual: 3\n",
      "Image 3633 - Predicted: 7, Actual: 7\n",
      "Image 3634 - Predicted: 1, Actual: 1\n",
      "Image 3635 - Predicted: 3, Actual: 3\n",
      "Image 3636 - Predicted: 5, Actual: 4\n",
      "Image 3637 - Predicted: 4, Actual: 5\n",
      "Image 3638 - Predicted: 2, Actual: 2\n",
      "Image 3639 - Predicted: 5, Actual: 5\n",
      "Image 3640 - Predicted: 8, Actual: 8\n",
      "Image 3641 - Predicted: 1, Actual: 1\n",
      "Image 3642 - Predicted: 6, Actual: 6\n",
      "Image 3643 - Predicted: 6, Actual: 6\n",
      "Image 3644 - Predicted: 3, Actual: 3\n",
      "Image 3645 - Predicted: 7, Actual: 8\n",
      "Image 3646 - Predicted: 0, Actual: 0\n",
      "Image 3647 - Predicted: 5, Actual: 5\n",
      "Image 3648 - Predicted: 6, Actual: 6\n",
      "Image 3649 - Predicted: 6, Actual: 6\n",
      "Image 3650 - Predicted: 8, Actual: 8\n",
      "Image 3651 - Predicted: 3, Actual: 3\n",
      "Image 3652 - Predicted: 6, Actual: 6\n",
      "Image 3653 - Predicted: 3, Actual: 3\n",
      "Image 3654 - Predicted: 9, Actual: 9\n",
      "Image 3655 - Predicted: 0, Actual: 0\n",
      "Image 3656 - Predicted: 8, Actual: 8\n",
      "Image 3657 - Predicted: 8, Actual: 8\n",
      "Image 3658 - Predicted: 5, Actual: 5\n",
      "Image 3659 - Predicted: 3, Actual: 3\n",
      "Image 3660 - Predicted: 5, Actual: 5\n",
      "Image 3661 - Predicted: 1, Actual: 1\n",
      "Image 3662 - Predicted: 6, Actual: 6\n",
      "Image 3663 - Predicted: 1, Actual: 0\n",
      "Image 3664 - Predicted: 4, Actual: 4\n",
      "Image 3665 - Predicted: 9, Actual: 7\n",
      "Image 3666 - Predicted: 5, Actual: 5\n",
      "Image 3667 - Predicted: 4, Actual: 4\n",
      "Image 3668 - Predicted: 3, Actual: 3\n",
      "Image 3669 - Predicted: 7, Actual: 7\n",
      "Image 3670 - Predicted: 1, Actual: 1\n",
      "Image 3671 - Predicted: 7, Actual: 7\n",
      "Image 3672 - Predicted: 2, Actual: 2\n",
      "Image 3673 - Predicted: 6, Actual: 2\n",
      "Image 3674 - Predicted: 5, Actual: 5\n",
      "Image 3675 - Predicted: 8, Actual: 4\n",
      "Image 3676 - Predicted: 2, Actual: 2\n",
      "Image 3677 - Predicted: 8, Actual: 8\n",
      "Image 3678 - Predicted: 6, Actual: 6\n",
      "Image 3679 - Predicted: 9, Actual: 9\n",
      "Image 3680 - Predicted: 8, Actual: 5\n",
      "Image 3681 - Predicted: 9, Actual: 9\n",
      "Image 3682 - Predicted: 1, Actual: 1\n",
      "Image 3683 - Predicted: 8, Actual: 8\n",
      "Image 3684 - Predicted: 1, Actual: 1\n",
      "Image 3685 - Predicted: 5, Actual: 5\n",
      "Image 3686 - Predicted: 2, Actual: 2\n",
      "Image 3687 - Predicted: 7, Actual: 9\n",
      "Image 3688 - Predicted: 3, Actual: 3\n",
      "Image 3689 - Predicted: 4, Actual: 4\n",
      "Image 3690 - Predicted: 1, Actual: 1\n",
      "Image 3691 - Predicted: 0, Actual: 0\n",
      "Image 3692 - Predicted: 2, Actual: 2\n",
      "Image 3693 - Predicted: 2, Actual: 2\n",
      "Image 3694 - Predicted: 8, Actual: 4\n",
      "Image 3695 - Predicted: 2, Actual: 2\n",
      "Image 3696 - Predicted: 5, Actual: 5\n",
      "Image 3697 - Predicted: 2, Actual: 2\n",
      "Image 3698 - Predicted: 5, Actual: 8\n",
      "Image 3699 - Predicted: 0, Actual: 0\n",
      "Image 3700 - Predicted: 9, Actual: 9\n",
      "Image 3701 - Predicted: 9, Actual: 9\n",
      "Image 3702 - Predicted: 2, Actual: 2\n",
      "Image 3703 - Predicted: 6, Actual: 6\n",
      "Image 3704 - Predicted: 8, Actual: 8\n",
      "Image 3705 - Predicted: 7, Actual: 7\n",
      "Image 3706 - Predicted: 6, Actual: 6\n",
      "Image 3707 - Predicted: 2, Actual: 2\n",
      "Image 3708 - Predicted: 9, Actual: 9\n",
      "Image 3709 - Predicted: 8, Actual: 8\n",
      "Image 3710 - Predicted: 7, Actual: 7\n",
      "Image 3711 - Predicted: 9, Actual: 9\n",
      "Image 3712 - Predicted: 6, Actual: 6\n",
      "Image 3713 - Predicted: 2, Actual: 2\n",
      "Image 3714 - Predicted: 0, Actual: 0\n",
      "Image 3715 - Predicted: 9, Actual: 4\n",
      "Image 3716 - Predicted: 7, Actual: 7\n",
      "Image 3717 - Predicted: 3, Actual: 3\n",
      "Image 3718 - Predicted: 0, Actual: 0\n",
      "Image 3719 - Predicted: 8, Actual: 8\n",
      "Image 3720 - Predicted: 6, Actual: 6\n",
      "Image 3721 - Predicted: 3, Actual: 1\n",
      "Image 3722 - Predicted: 6, Actual: 6\n",
      "Image 3723 - Predicted: 2, Actual: 2\n",
      "Image 3724 - Predicted: 6, Actual: 2\n",
      "Image 3725 - Predicted: 6, Actual: 6\n",
      "Image 3726 - Predicted: 1, Actual: 1\n",
      "Image 3727 - Predicted: 7, Actual: 7\n",
      "Image 3728 - Predicted: 9, Actual: 9\n",
      "Image 3729 - Predicted: 6, Actual: 6\n",
      "Image 3730 - Predicted: 2, Actual: 2\n",
      "Image 3731 - Predicted: 6, Actual: 6\n",
      "Image 3732 - Predicted: 1, Actual: 1\n",
      "Image 3733 - Predicted: 0, Actual: 0\n",
      "Image 3734 - Predicted: 0, Actual: 0\n",
      "Image 3735 - Predicted: 9, Actual: 7\n",
      "Image 3736 - Predicted: 8, Actual: 7\n",
      "Image 3737 - Predicted: 3, Actual: 3\n",
      "Image 3738 - Predicted: 9, Actual: 4\n",
      "Image 3739 - Predicted: 2, Actual: 2\n",
      "Image 3740 - Predicted: 8, Actual: 7\n",
      "Image 3741 - Predicted: 2, Actual: 2\n",
      "Image 3742 - Predicted: 1, Actual: 1\n",
      "Image 3743 - Predicted: 7, Actual: 7\n",
      "Image 3744 - Predicted: 0, Actual: 0\n",
      "Image 3745 - Predicted: 1, Actual: 1\n",
      "Image 3746 - Predicted: 7, Actual: 7\n",
      "Image 3747 - Predicted: 9, Actual: 9\n",
      "Image 3748 - Predicted: 4, Actual: 4\n",
      "Image 3749 - Predicted: 7, Actual: 7\n",
      "Image 3750 - Predicted: 2, Actual: 2\n",
      "Image 3751 - Predicted: 5, Actual: 5\n",
      "Image 3752 - Predicted: 1, Actual: 1\n",
      "Image 3753 - Predicted: 7, Actual: 7\n",
      "Image 3754 - Predicted: 2, Actual: 2\n",
      "Image 3755 - Predicted: 1, Actual: 1\n",
      "Image 3756 - Predicted: 3, Actual: 3\n",
      "Image 3757 - Predicted: 8, Actual: 4\n",
      "Image 3758 - Predicted: 7, Actual: 7\n",
      "Image 3759 - Predicted: 6, Actual: 6\n",
      "Image 3760 - Predicted: 3, Actual: 3\n",
      "Image 3761 - Predicted: 1, Actual: 1\n",
      "Image 3762 - Predicted: 8, Actual: 8\n",
      "Image 3763 - Predicted: 7, Actual: 7\n",
      "Image 3764 - Predicted: 0, Actual: 0\n",
      "Image 3765 - Predicted: 7, Actual: 7\n",
      "Image 3766 - Predicted: 1, Actual: 1\n",
      "Image 3767 - Predicted: 3, Actual: 3\n",
      "Image 3768 - Predicted: 3, Actual: 3\n",
      "Image 3769 - Predicted: 7, Actual: 7\n",
      "Image 3770 - Predicted: 2, Actual: 2\n",
      "Image 3771 - Predicted: 9, Actual: 9\n",
      "Image 3772 - Predicted: 5, Actual: 5\n",
      "Image 3773 - Predicted: 1, Actual: 1\n",
      "Image 3774 - Predicted: 7, Actual: 8\n",
      "Image 3775 - Predicted: 9, Actual: 9\n",
      "Image 3776 - Predicted: 9, Actual: 9\n",
      "Image 3777 - Predicted: 1, Actual: 1\n",
      "Image 3778 - Predicted: 1, Actual: 1\n",
      "Image 3779 - Predicted: 7, Actual: 7\n",
      "Image 3780 - Predicted: 1, Actual: 1\n",
      "Image 3781 - Predicted: 6, Actual: 6\n",
      "Image 3782 - Predicted: 7, Actual: 8\n",
      "Image 3783 - Predicted: 4, Actual: 8\n",
      "Image 3784 - Predicted: 6, Actual: 4\n",
      "Image 3785 - Predicted: 4, Actual: 8\n",
      "Image 3786 - Predicted: 6, Actual: 6\n",
      "Image 3787 - Predicted: 7, Actual: 7\n",
      "Image 3788 - Predicted: 7, Actual: 7\n",
      "Image 3789 - Predicted: 0, Actual: 0\n",
      "Image 3790 - Predicted: 6, Actual: 2\n",
      "Image 3791 - Predicted: 0, Actual: 0\n",
      "Image 3792 - Predicted: 7, Actual: 7\n",
      "Image 3793 - Predicted: 3, Actual: 3\n",
      "Image 3794 - Predicted: 0, Actual: 0\n",
      "Image 3795 - Predicted: 7, Actual: 7\n",
      "Image 3796 - Predicted: 7, Actual: 7\n",
      "Image 3797 - Predicted: 3, Actual: 3\n",
      "Image 3798 - Predicted: 5, Actual: 5\n",
      "Image 3799 - Predicted: 5, Actual: 5\n",
      "Image 3800 - Predicted: 2, Actual: 2\n",
      "Image 3801 - Predicted: 4, Actual: 2\n",
      "Image 3802 - Predicted: 7, Actual: 7\n",
      "Image 3803 - Predicted: 2, Actual: 2\n",
      "Image 3804 - Predicted: 9, Actual: 8\n",
      "Image 3805 - Predicted: 5, Actual: 4\n",
      "Image 3806 - Predicted: 6, Actual: 6\n",
      "Image 3807 - Predicted: 9, Actual: 9\n",
      "Image 3808 - Predicted: 9, Actual: 9\n",
      "Image 3809 - Predicted: 4, Actual: 4\n",
      "Image 3810 - Predicted: 7, Actual: 7\n",
      "Image 3811 - Predicted: 9, Actual: 8\n",
      "Image 3812 - Predicted: 1, Actual: 1\n",
      "Image 3813 - Predicted: 7, Actual: 2\n",
      "Image 3814 - Predicted: 7, Actual: 7\n",
      "Image 3815 - Predicted: 3, Actual: 3\n",
      "Image 3816 - Predicted: 0, Actual: 0\n",
      "Image 3817 - Predicted: 5, Actual: 5\n",
      "Image 3818 - Predicted: 3, Actual: 3\n",
      "Image 3819 - Predicted: 3, Actual: 3\n",
      "Image 3820 - Predicted: 9, Actual: 9\n",
      "Image 3821 - Predicted: 3, Actual: 3\n",
      "Image 3822 - Predicted: 9, Actual: 9\n",
      "Image 3823 - Predicted: 9, Actual: 4\n",
      "Image 3824 - Predicted: 5, Actual: 4\n",
      "Image 3825 - Predicted: 1, Actual: 1\n",
      "Image 3826 - Predicted: 1, Actual: 1\n",
      "Image 3827 - Predicted: 3, Actual: 3\n",
      "Image 3828 - Predicted: 8, Actual: 8\n",
      "Image 3829 - Predicted: 2, Actual: 2\n",
      "Image 3830 - Predicted: 2, Actual: 2\n",
      "Image 3831 - Predicted: 8, Actual: 4\n",
      "Image 3832 - Predicted: 3, Actual: 3\n",
      "Image 3833 - Predicted: 7, Actual: 7\n",
      "Image 3834 - Predicted: 1, Actual: 1\n",
      "Image 3835 - Predicted: 4, Actual: 4\n",
      "Image 3836 - Predicted: 2, Actual: 2\n",
      "Image 3837 - Predicted: 9, Actual: 9\n",
      "Image 3838 - Predicted: 6, Actual: 0\n",
      "Image 3839 - Predicted: 0, Actual: 0\n",
      "Image 3840 - Predicted: 2, Actual: 6\n",
      "Image 3841 - Predicted: 9, Actual: 8\n",
      "Image 3842 - Predicted: 2, Actual: 2\n",
      "Image 3843 - Predicted: 2, Actual: 2\n",
      "Image 3844 - Predicted: 7, Actual: 0\n",
      "Image 3845 - Predicted: 6, Actual: 6\n",
      "Image 3846 - Predicted: 1, Actual: 1\n",
      "Image 3847 - Predicted: 9, Actual: 7\n",
      "Image 3848 - Predicted: 2, Actual: 2\n",
      "Image 3849 - Predicted: 3, Actual: 3\n",
      "Image 3850 - Predicted: 2, Actual: 2\n",
      "Image 3851 - Predicted: 8, Actual: 8\n",
      "Image 3852 - Predicted: 7, Actual: 7\n",
      "Image 3853 - Predicted: 8, Actual: 8\n",
      "Image 3854 - Predicted: 5, Actual: 5\n",
      "Image 3855 - Predicted: 7, Actual: 9\n",
      "Image 3856 - Predicted: 6, Actual: 6\n",
      "Image 3857 - Predicted: 7, Actual: 9\n",
      "Image 3858 - Predicted: 2, Actual: 2\n",
      "Image 3859 - Predicted: 1, Actual: 1\n",
      "Image 3860 - Predicted: 1, Actual: 1\n",
      "Image 3861 - Predicted: 5, Actual: 5\n",
      "Image 3862 - Predicted: 9, Actual: 9\n",
      "Image 3863 - Predicted: 0, Actual: 0\n",
      "Image 3864 - Predicted: 2, Actual: 2\n",
      "Image 3865 - Predicted: 4, Actual: 4\n",
      "Image 3866 - Predicted: 4, Actual: 5\n",
      "Image 3867 - Predicted: 8, Actual: 8\n",
      "Image 3868 - Predicted: 6, Actual: 6\n",
      "Image 3869 - Predicted: 0, Actual: 0\n",
      "Image 3870 - Predicted: 5, Actual: 4\n",
      "Image 3871 - Predicted: 0, Actual: 0\n",
      "Image 3872 - Predicted: 1, Actual: 6\n",
      "Image 3873 - Predicted: 8, Actual: 8\n",
      "Image 3874 - Predicted: 3, Actual: 3\n",
      "Image 3875 - Predicted: 5, Actual: 8\n",
      "Image 3876 - Predicted: 1, Actual: 1\n",
      "Image 3877 - Predicted: 3, Actual: 3\n",
      "Image 3878 - Predicted: 0, Actual: 0\n",
      "Image 3879 - Predicted: 1, Actual: 1\n",
      "Image 3880 - Predicted: 1, Actual: 1\n",
      "Image 3881 - Predicted: 9, Actual: 7\n",
      "Image 3882 - Predicted: 4, Actual: 4\n",
      "Image 3883 - Predicted: 9, Actual: 9\n",
      "Image 3884 - Predicted: 6, Actual: 6\n",
      "Image 3885 - Predicted: 1, Actual: 1\n",
      "Image 3886 - Predicted: 0, Actual: 0\n",
      "Image 3887 - Predicted: 8, Actual: 8\n",
      "Image 3888 - Predicted: 9, Actual: 8\n",
      "Image 3889 - Predicted: 9, Actual: 9\n",
      "Image 3890 - Predicted: 7, Actual: 7\n",
      "Image 3891 - Predicted: 8, Actual: 4\n",
      "Image 3892 - Predicted: 3, Actual: 3\n",
      "Image 3893 - Predicted: 7, Actual: 7\n",
      "Image 3894 - Predicted: 0, Actual: 0\n",
      "Image 3895 - Predicted: 9, Actual: 9\n",
      "Image 3896 - Predicted: 9, Actual: 8\n",
      "Image 3897 - Predicted: 7, Actual: 7\n",
      "Image 3898 - Predicted: 4, Actual: 5\n",
      "Image 3899 - Predicted: 5, Actual: 5\n",
      "Image 3900 - Predicted: 1, Actual: 1\n",
      "Image 3901 - Predicted: 1, Actual: 1\n",
      "Image 3902 - Predicted: 7, Actual: 7\n",
      "Image 3903 - Predicted: 1, Actual: 1\n",
      "Image 3904 - Predicted: 6, Actual: 6\n",
      "Image 3905 - Predicted: 3, Actual: 3\n",
      "Image 3906 - Predicted: 4, Actual: 9\n",
      "Image 3907 - Predicted: 8, Actual: 7\n",
      "Image 3908 - Predicted: 7, Actual: 7\n",
      "Image 3909 - Predicted: 5, Actual: 5\n",
      "Image 3910 - Predicted: 8, Actual: 8\n",
      "Image 3911 - Predicted: 3, Actual: 3\n",
      "Image 3912 - Predicted: 4, Actual: 5\n",
      "Image 3913 - Predicted: 7, Actual: 7\n",
      "Image 3914 - Predicted: 1, Actual: 1\n",
      "Image 3915 - Predicted: 9, Actual: 9\n",
      "Image 3916 - Predicted: 5, Actual: 5\n",
      "Image 3917 - Predicted: 3, Actual: 3\n",
      "Image 3918 - Predicted: 0, Actual: 1\n",
      "Image 3919 - Predicted: 3, Actual: 3\n",
      "Image 3920 - Predicted: 7, Actual: 7\n",
      "Image 3921 - Predicted: 9, Actual: 9\n",
      "Image 3922 - Predicted: 3, Actual: 3\n",
      "Image 3923 - Predicted: 8, Actual: 8\n",
      "Image 3924 - Predicted: 2, Actual: 2\n",
      "Image 3925 - Predicted: 0, Actual: 0\n",
      "Image 3926 - Predicted: 2, Actual: 2\n",
      "Image 3927 - Predicted: 3, Actual: 3\n",
      "Image 3928 - Predicted: 6, Actual: 6\n",
      "Image 3929 - Predicted: 4, Actual: 4\n",
      "Image 3930 - Predicted: 1, Actual: 1\n",
      "Image 3931 - Predicted: 8, Actual: 9\n",
      "Image 3932 - Predicted: 1, Actual: 1\n",
      "Image 3933 - Predicted: 5, Actual: 4\n",
      "Image 3934 - Predicted: 5, Actual: 5\n",
      "Image 3935 - Predicted: 6, Actual: 6\n",
      "Image 3936 - Predicted: 2, Actual: 2\n",
      "Image 3937 - Predicted: 0, Actual: 0\n",
      "Image 3938 - Predicted: 6, Actual: 6\n",
      "Image 3939 - Predicted: 4, Actual: 4\n",
      "Image 3940 - Predicted: 3, Actual: 3\n",
      "Image 3941 - Predicted: 3, Actual: 3\n",
      "Image 3942 - Predicted: 7, Actual: 7\n",
      "Image 3943 - Predicted: 1, Actual: 6\n",
      "Image 3944 - Predicted: 5, Actual: 4\n",
      "Image 3945 - Predicted: 6, Actual: 0\n",
      "Image 3946 - Predicted: 6, Actual: 6\n",
      "Image 3947 - Predicted: 3, Actual: 3\n",
      "Image 3948 - Predicted: 2, Actual: 2\n",
      "Image 3949 - Predicted: 2, Actual: 2\n",
      "Image 3950 - Predicted: 3, Actual: 3\n",
      "Image 3951 - Predicted: 2, Actual: 2\n",
      "Image 3952 - Predicted: 8, Actual: 8\n",
      "Image 3953 - Predicted: 0, Actual: 0\n",
      "Image 3954 - Predicted: 5, Actual: 4\n",
      "Image 3955 - Predicted: 3, Actual: 3\n",
      "Image 3956 - Predicted: 3, Actual: 3\n",
      "Image 3957 - Predicted: 1, Actual: 1\n",
      "Image 3958 - Predicted: 2, Actual: 2\n",
      "Image 3959 - Predicted: 5, Actual: 5\n",
      "Image 3960 - Predicted: 4, Actual: 8\n",
      "Image 3961 - Predicted: 8, Actual: 5\n",
      "Image 3962 - Predicted: 3, Actual: 3\n",
      "Image 3963 - Predicted: 2, Actual: 2\n",
      "Image 3964 - Predicted: 2, Actual: 2\n",
      "Image 3965 - Predicted: 0, Actual: 0\n",
      "Image 3966 - Predicted: 2, Actual: 2\n",
      "Image 3967 - Predicted: 8, Actual: 8\n",
      "Image 3968 - Predicted: 7, Actual: 7\n",
      "Image 3969 - Predicted: 3, Actual: 3\n",
      "Image 3970 - Predicted: 6, Actual: 6\n",
      "Image 3971 - Predicted: 0, Actual: 1\n",
      "Image 3972 - Predicted: 5, Actual: 4\n",
      "Image 3973 - Predicted: 3, Actual: 3\n",
      "Image 3974 - Predicted: 2, Actual: 2\n",
      "Image 3975 - Predicted: 0, Actual: 0\n",
      "Image 3976 - Predicted: 2, Actual: 2\n",
      "Image 3977 - Predicted: 7, Actual: 7\n",
      "Image 3978 - Predicted: 5, Actual: 5\n",
      "Image 3979 - Predicted: 9, Actual: 9\n",
      "Image 3980 - Predicted: 9, Actual: 7\n",
      "Image 3981 - Predicted: 6, Actual: 6\n",
      "Image 3982 - Predicted: 8, Actual: 8\n",
      "Image 3983 - Predicted: 8, Actual: 9\n",
      "Image 3984 - Predicted: 8, Actual: 8\n",
      "Image 3985 - Predicted: 5, Actual: 5\n",
      "Image 3986 - Predicted: 7, Actual: 7\n",
      "Image 3987 - Predicted: 1, Actual: 1\n",
      "Image 3988 - Predicted: 7, Actual: 2\n",
      "Image 3989 - Predicted: 5, Actual: 5\n",
      "Image 3990 - Predicted: 7, Actual: 7\n",
      "Image 3991 - Predicted: 8, Actual: 5\n",
      "Image 3992 - Predicted: 2, Actual: 2\n",
      "Image 3993 - Predicted: 8, Actual: 8\n",
      "Image 3994 - Predicted: 7, Actual: 4\n",
      "Image 3995 - Predicted: 8, Actual: 9\n",
      "Image 3996 - Predicted: 7, Actual: 7\n",
      "Image 3997 - Predicted: 9, Actual: 4\n",
      "Image 3998 - Predicted: 1, Actual: 1\n",
      "Image 3999 - Predicted: 2, Actual: 2\n",
      "Image 4000 - Predicted: 9, Actual: 7\n",
      "Image 4001 - Predicted: 6, Actual: 6\n",
      "Image 4002 - Predicted: 1, Actual: 6\n",
      "Image 4003 - Predicted: 9, Actual: 8\n",
      "Image 4004 - Predicted: 5, Actual: 5\n",
      "Image 4005 - Predicted: 8, Actual: 8\n",
      "Image 4006 - Predicted: 3, Actual: 3\n",
      "Image 4007 - Predicted: 6, Actual: 6\n",
      "Image 4008 - Predicted: 9, Actual: 9\n",
      "Image 4009 - Predicted: 9, Actual: 9\n",
      "Image 4010 - Predicted: 5, Actual: 5\n",
      "Image 4011 - Predicted: 2, Actual: 2\n",
      "Image 4012 - Predicted: 2, Actual: 5\n",
      "Image 4013 - Predicted: 2, Actual: 2\n",
      "Image 4014 - Predicted: 7, Actual: 7\n",
      "Image 4015 - Predicted: 3, Actual: 3\n",
      "Image 4016 - Predicted: 2, Actual: 2\n",
      "Image 4017 - Predicted: 9, Actual: 9\n",
      "Image 4018 - Predicted: 4, Actual: 4\n",
      "Image 4019 - Predicted: 9, Actual: 9\n",
      "Image 4020 - Predicted: 1, Actual: 1\n",
      "Image 4021 - Predicted: 6, Actual: 2\n",
      "Image 4022 - Predicted: 2, Actual: 4\n",
      "Image 4023 - Predicted: 6, Actual: 6\n",
      "Image 4024 - Predicted: 5, Actual: 5\n",
      "Image 4025 - Predicted: 6, Actual: 6\n",
      "Image 4026 - Predicted: 4, Actual: 4\n",
      "Image 4027 - Predicted: 9, Actual: 4\n",
      "Image 4028 - Predicted: 0, Actual: 1\n",
      "Image 4029 - Predicted: 3, Actual: 3\n",
      "Image 4030 - Predicted: 9, Actual: 4\n",
      "Image 4031 - Predicted: 6, Actual: 6\n",
      "Image 4032 - Predicted: 9, Actual: 9\n",
      "Image 4033 - Predicted: 0, Actual: 0\n",
      "Image 4034 - Predicted: 9, Actual: 9\n",
      "Image 4035 - Predicted: 1, Actual: 1\n",
      "Image 4036 - Predicted: 3, Actual: 3\n",
      "Image 4037 - Predicted: 2, Actual: 2\n",
      "Image 4038 - Predicted: 2, Actual: 2\n",
      "Image 4039 - Predicted: 0, Actual: 0\n",
      "Image 4040 - Predicted: 5, Actual: 5\n",
      "Image 4041 - Predicted: 6, Actual: 6\n",
      "Image 4042 - Predicted: 9, Actual: 9\n",
      "Image 4043 - Predicted: 8, Actual: 8\n",
      "Image 4044 - Predicted: 1, Actual: 1\n",
      "Image 4045 - Predicted: 0, Actual: 0\n",
      "Image 4046 - Predicted: 4, Actual: 8\n",
      "Image 4047 - Predicted: 9, Actual: 9\n",
      "Image 4048 - Predicted: 7, Actual: 7\n",
      "Image 4049 - Predicted: 8, Actual: 4\n",
      "Image 4050 - Predicted: 5, Actual: 5\n",
      "Image 4051 - Predicted: 5, Actual: 5\n",
      "Image 4052 - Predicted: 3, Actual: 3\n",
      "Image 4053 - Predicted: 4, Actual: 4\n",
      "Image 4054 - Predicted: 6, Actual: 6\n",
      "Image 4055 - Predicted: 5, Actual: 5\n",
      "Image 4056 - Predicted: 0, Actual: 0\n",
      "Image 4057 - Predicted: 9, Actual: 9\n",
      "Image 4058 - Predicted: 6, Actual: 6\n",
      "Image 4059 - Predicted: 6, Actual: 4\n",
      "Image 4060 - Predicted: 9, Actual: 7\n",
      "Image 4061 - Predicted: 9, Actual: 9\n",
      "Image 4062 - Predicted: 2, Actual: 2\n",
      "Image 4063 - Predicted: 5, Actual: 5\n",
      "Image 4064 - Predicted: 7, Actual: 9\n",
      "Image 4065 - Predicted: 3, Actual: 3\n",
      "Image 4066 - Predicted: 1, Actual: 1\n",
      "Image 4067 - Predicted: 9, Actual: 9\n",
      "Image 4068 - Predicted: 5, Actual: 5\n",
      "Image 4069 - Predicted: 8, Actual: 7\n",
      "Image 4070 - Predicted: 2, Actual: 2\n",
      "Image 4071 - Predicted: 0, Actual: 0\n",
      "Image 4072 - Predicted: 9, Actual: 9\n",
      "Image 4073 - Predicted: 6, Actual: 6\n",
      "Image 4074 - Predicted: 7, Actual: 0\n",
      "Image 4075 - Predicted: 9, Actual: 9\n",
      "Image 4076 - Predicted: 7, Actual: 7\n",
      "Image 4077 - Predicted: 0, Actual: 0\n",
      "Image 4078 - Predicted: 4, Actual: 4\n",
      "Image 4079 - Predicted: 6, Actual: 6\n",
      "Image 4080 - Predicted: 3, Actual: 3\n",
      "Image 4081 - Predicted: 3, Actual: 3\n",
      "Image 4082 - Predicted: 6, Actual: 6\n",
      "Image 4083 - Predicted: 3, Actual: 3\n",
      "Image 4084 - Predicted: 0, Actual: 1\n",
      "Image 4085 - Predicted: 0, Actual: 0\n",
      "Image 4086 - Predicted: 0, Actual: 4\n",
      "Image 4087 - Predicted: 9, Actual: 9\n",
      "Image 4088 - Predicted: 6, Actual: 6\n",
      "Image 4089 - Predicted: 7, Actual: 9\n",
      "Image 4090 - Predicted: 0, Actual: 0\n",
      "Image 4091 - Predicted: 4, Actual: 4\n",
      "Image 4092 - Predicted: 7, Actual: 7\n",
      "Image 4093 - Predicted: 7, Actual: 8\n",
      "Image 4094 - Predicted: 6, Actual: 6\n",
      "Image 4095 - Predicted: 3, Actual: 3\n",
      "Image 4096 - Predicted: 8, Actual: 8\n",
      "Image 4097 - Predicted: 6, Actual: 6\n",
      "Image 4098 - Predicted: 0, Actual: 0\n",
      "Image 4099 - Predicted: 5, Actual: 5\n",
      "Image 4100 - Predicted: 1, Actual: 1\n",
      "Image 4101 - Predicted: 3, Actual: 3\n",
      "Image 4102 - Predicted: 8, Actual: 7\n",
      "Image 4103 - Predicted: 0, Actual: 2\n",
      "Image 4104 - Predicted: 2, Actual: 2\n",
      "Image 4105 - Predicted: 2, Actual: 6\n",
      "Image 4106 - Predicted: 9, Actual: 7\n",
      "Image 4107 - Predicted: 6, Actual: 6\n",
      "Image 4108 - Predicted: 3, Actual: 3\n",
      "Image 4109 - Predicted: 8, Actual: 8\n",
      "Image 4110 - Predicted: 8, Actual: 4\n",
      "Image 4111 - Predicted: 5, Actual: 5\n",
      "Image 4112 - Predicted: 5, Actual: 5\n",
      "Image 4113 - Predicted: 5, Actual: 5\n",
      "Image 4114 - Predicted: 4, Actual: 4\n",
      "Image 4115 - Predicted: 0, Actual: 0\n",
      "Image 4116 - Predicted: 3, Actual: 3\n",
      "Image 4117 - Predicted: 3, Actual: 3\n",
      "Image 4118 - Predicted: 7, Actual: 7\n",
      "Image 4119 - Predicted: 2, Actual: 2\n",
      "Image 4120 - Predicted: 9, Actual: 9\n",
      "Image 4121 - Predicted: 7, Actual: 7\n",
      "Image 4122 - Predicted: 5, Actual: 5\n",
      "Image 4123 - Predicted: 9, Actual: 9\n",
      "Image 4124 - Predicted: 0, Actual: 0\n",
      "Image 4125 - Predicted: 7, Actual: 8\n",
      "Image 4126 - Predicted: 4, Actual: 4\n",
      "Image 4127 - Predicted: 3, Actual: 3\n",
      "Image 4128 - Predicted: 9, Actual: 9\n",
      "Image 4129 - Predicted: 9, Actual: 9\n",
      "Image 4130 - Predicted: 4, Actual: 4\n",
      "Image 4131 - Predicted: 8, Actual: 8\n",
      "Image 4132 - Predicted: 5, Actual: 5\n",
      "Image 4133 - Predicted: 9, Actual: 9\n",
      "Image 4134 - Predicted: 1, Actual: 1\n",
      "Image 4135 - Predicted: 3, Actual: 3\n",
      "Image 4136 - Predicted: 9, Actual: 9\n",
      "Image 4137 - Predicted: 4, Actual: 4\n",
      "Image 4138 - Predicted: 4, Actual: 4\n",
      "Image 4139 - Predicted: 4, Actual: 4\n",
      "Image 4140 - Predicted: 5, Actual: 5\n",
      "Image 4141 - Predicted: 9, Actual: 8\n",
      "Image 4142 - Predicted: 1, Actual: 1\n",
      "Image 4143 - Predicted: 4, Actual: 5\n",
      "Image 4144 - Predicted: 0, Actual: 0\n",
      "Image 4145 - Predicted: 6, Actual: 6\n",
      "Image 4146 - Predicted: 0, Actual: 1\n",
      "Image 4147 - Predicted: 7, Actual: 6\n",
      "Image 4148 - Predicted: 2, Actual: 2\n",
      "Image 4149 - Predicted: 8, Actual: 8\n",
      "Image 4150 - Predicted: 0, Actual: 1\n",
      "Image 4151 - Predicted: 5, Actual: 8\n",
      "Image 4152 - Predicted: 8, Actual: 8\n",
      "Image 4153 - Predicted: 8, Actual: 8\n",
      "Image 4154 - Predicted: 1, Actual: 1\n",
      "Image 4155 - Predicted: 6, Actual: 6\n",
      "Image 4156 - Predicted: 0, Actual: 0\n",
      "Image 4157 - Predicted: 6, Actual: 6\n",
      "Image 4158 - Predicted: 8, Actual: 8\n",
      "Image 4159 - Predicted: 0, Actual: 0\n",
      "Image 4160 - Predicted: 6, Actual: 6\n",
      "Image 4161 - Predicted: 3, Actual: 3\n",
      "Image 4162 - Predicted: 4, Actual: 9\n",
      "Image 4163 - Predicted: 3, Actual: 3\n",
      "Image 4164 - Predicted: 3, Actual: 3\n",
      "Image 4165 - Predicted: 6, Actual: 6\n",
      "Image 4166 - Predicted: 4, Actual: 4\n",
      "Image 4167 - Predicted: 7, Actual: 7\n",
      "Image 4168 - Predicted: 9, Actual: 9\n",
      "Image 4169 - Predicted: 6, Actual: 6\n",
      "Image 4170 - Predicted: 4, Actual: 5\n",
      "Image 4171 - Predicted: 0, Actual: 0\n",
      "Image 4172 - Predicted: 3, Actual: 3\n",
      "Image 4173 - Predicted: 3, Actual: 3\n",
      "Image 4174 - Predicted: 7, Actual: 7\n",
      "Image 4175 - Predicted: 9, Actual: 9\n",
      "Image 4176 - Predicted: 9, Actual: 9\n",
      "Image 4177 - Predicted: 3, Actual: 3\n",
      "Image 4178 - Predicted: 4, Actual: 4\n",
      "Image 4179 - Predicted: 6, Actual: 6\n",
      "Image 4180 - Predicted: 7, Actual: 7\n",
      "Image 4181 - Predicted: 6, Actual: 6\n",
      "Image 4182 - Predicted: 5, Actual: 5\n",
      "Image 4183 - Predicted: 8, Actual: 4\n",
      "Image 4184 - Predicted: 9, Actual: 9\n",
      "Image 4185 - Predicted: 0, Actual: 0\n",
      "Image 4186 - Predicted: 8, Actual: 7\n",
      "Image 4187 - Predicted: 8, Actual: 8\n",
      "Image 4188 - Predicted: 5, Actual: 8\n",
      "Image 4189 - Predicted: 8, Actual: 4\n",
      "Image 4190 - Predicted: 6, Actual: 2\n",
      "Image 4191 - Predicted: 3, Actual: 3\n",
      "Image 4192 - Predicted: 8, Actual: 8\n",
      "Image 4193 - Predicted: 0, Actual: 0\n",
      "Image 4194 - Predicted: 7, Actual: 9\n",
      "Image 4195 - Predicted: 7, Actual: 7\n",
      "Image 4196 - Predicted: 5, Actual: 5\n",
      "Image 4197 - Predicted: 5, Actual: 9\n",
      "Image 4198 - Predicted: 2, Actual: 2\n",
      "Image 4199 - Predicted: 5, Actual: 5\n",
      "Image 4200 - Predicted: 2, Actual: 2\n",
      "Image 4201 - Predicted: 5, Actual: 5\n",
      "Image 4202 - Predicted: 6, Actual: 6\n",
      "Image 4203 - Predicted: 1, Actual: 1\n",
      "Image 4204 - Predicted: 8, Actual: 4\n",
      "Image 4205 - Predicted: 8, Actual: 8\n",
      "Image 4206 - Predicted: 6, Actual: 2\n",
      "Image 4207 - Predicted: 9, Actual: 9\n",
      "Image 4208 - Predicted: 5, Actual: 5\n",
      "Image 4209 - Predicted: 1, Actual: 1\n",
      "Image 4210 - Predicted: 9, Actual: 8\n",
      "Image 4211 - Predicted: 1, Actual: 1\n",
      "Image 4212 - Predicted: 8, Actual: 8\n",
      "Image 4213 - Predicted: 6, Actual: 6\n",
      "Image 4214 - Predicted: 0, Actual: 0\n",
      "Image 4215 - Predicted: 9, Actual: 9\n",
      "Image 4216 - Predicted: 3, Actual: 3\n",
      "Image 4217 - Predicted: 9, Actual: 5\n",
      "Image 4218 - Predicted: 6, Actual: 6\n",
      "Image 4219 - Predicted: 5, Actual: 5\n",
      "Image 4220 - Predicted: 9, Actual: 9\n",
      "Image 4221 - Predicted: 7, Actual: 9\n",
      "Image 4222 - Predicted: 9, Actual: 9\n",
      "Image 4223 - Predicted: 4, Actual: 4\n",
      "Image 4224 - Predicted: 9, Actual: 9\n",
      "Image 4225 - Predicted: 5, Actual: 5\n",
      "Image 4226 - Predicted: 2, Actual: 2\n",
      "Image 4227 - Predicted: 9, Actual: 9\n",
      "Image 4228 - Predicted: 1, Actual: 1\n",
      "Image 4229 - Predicted: 3, Actual: 3\n",
      "Image 4230 - Predicted: 2, Actual: 2\n",
      "Image 4231 - Predicted: 5, Actual: 5\n",
      "Image 4232 - Predicted: 9, Actual: 9\n",
      "Image 4233 - Predicted: 1, Actual: 1\n",
      "Image 4234 - Predicted: 5, Actual: 5\n",
      "Image 4235 - Predicted: 1, Actual: 1\n",
      "Image 4236 - Predicted: 5, Actual: 5\n",
      "Image 4237 - Predicted: 8, Actual: 4\n",
      "Image 4238 - Predicted: 2, Actual: 2\n",
      "Image 4239 - Predicted: 4, Actual: 8\n",
      "Image 4240 - Predicted: 4, Actual: 4\n",
      "Image 4241 - Predicted: 2, Actual: 2\n",
      "Image 4242 - Predicted: 1, Actual: 1\n",
      "Image 4243 - Predicted: 7, Actual: 7\n",
      "Image 4244 - Predicted: 1, Actual: 1\n",
      "Image 4245 - Predicted: 4, Actual: 4\n",
      "Image 4246 - Predicted: 9, Actual: 5\n",
      "Image 4247 - Predicted: 3, Actual: 3\n",
      "Image 4248 - Predicted: 5, Actual: 5\n",
      "Image 4249 - Predicted: 3, Actual: 3\n",
      "Image 4250 - Predicted: 6, Actual: 6\n",
      "Image 4251 - Predicted: 5, Actual: 5\n",
      "Image 4252 - Predicted: 2, Actual: 2\n",
      "Image 4253 - Predicted: 3, Actual: 3\n",
      "Image 4254 - Predicted: 6, Actual: 6\n",
      "Image 4255 - Predicted: 3, Actual: 3\n",
      "Image 4256 - Predicted: 8, Actual: 8\n",
      "Image 4257 - Predicted: 9, Actual: 9\n",
      "Image 4258 - Predicted: 9, Actual: 9\n",
      "Image 4259 - Predicted: 2, Actual: 2\n",
      "Image 4260 - Predicted: 5, Actual: 5\n",
      "Image 4261 - Predicted: 5, Actual: 5\n",
      "Image 4262 - Predicted: 0, Actual: 0\n",
      "Image 4263 - Predicted: 7, Actual: 9\n",
      "Image 4264 - Predicted: 6, Actual: 6\n",
      "Image 4265 - Predicted: 0, Actual: 0\n",
      "Image 4266 - Predicted: 0, Actual: 0\n",
      "Image 4267 - Predicted: 4, Actual: 9\n",
      "Image 4268 - Predicted: 9, Actual: 9\n",
      "Image 4269 - Predicted: 4, Actual: 5\n",
      "Image 4270 - Predicted: 0, Actual: 0\n",
      "Image 4271 - Predicted: 6, Actual: 2\n",
      "Image 4272 - Predicted: 2, Actual: 2\n",
      "Image 4273 - Predicted: 6, Actual: 6\n",
      "Image 4274 - Predicted: 8, Actual: 4\n",
      "Image 4275 - Predicted: 2, Actual: 2\n",
      "Image 4276 - Predicted: 4, Actual: 8\n",
      "Image 4277 - Predicted: 8, Actual: 8\n",
      "Image 4278 - Predicted: 0, Actual: 0\n",
      "Image 4279 - Predicted: 9, Actual: 9\n",
      "Image 4280 - Predicted: 5, Actual: 5\n",
      "Image 4281 - Predicted: 1, Actual: 1\n",
      "Image 4282 - Predicted: 9, Actual: 9\n",
      "Image 4283 - Predicted: 1, Actual: 1\n",
      "Image 4284 - Predicted: 8, Actual: 8\n",
      "Image 4285 - Predicted: 9, Actual: 4\n",
      "Image 4286 - Predicted: 3, Actual: 3\n",
      "Image 4287 - Predicted: 9, Actual: 9\n",
      "Image 4288 - Predicted: 5, Actual: 4\n",
      "Image 4289 - Predicted: 1, Actual: 1\n",
      "Image 4290 - Predicted: 7, Actual: 7\n",
      "Image 4291 - Predicted: 9, Actual: 9\n",
      "Image 4292 - Predicted: 5, Actual: 5\n",
      "Image 4293 - Predicted: 6, Actual: 6\n",
      "Image 4294 - Predicted: 7, Actual: 7\n",
      "Image 4295 - Predicted: 0, Actual: 0\n",
      "Image 4296 - Predicted: 4, Actual: 4\n",
      "Image 4297 - Predicted: 6, Actual: 6\n",
      "Image 4298 - Predicted: 9, Actual: 9\n",
      "Image 4299 - Predicted: 5, Actual: 8\n",
      "Image 4300 - Predicted: 4, Actual: 8\n",
      "Image 4301 - Predicted: 7, Actual: 7\n",
      "Image 4302 - Predicted: 4, Actual: 9\n",
      "Image 4303 - Predicted: 4, Actual: 4\n",
      "Image 4304 - Predicted: 9, Actual: 7\n",
      "Image 4305 - Predicted: 5, Actual: 5\n",
      "Image 4306 - Predicted: 5, Actual: 5\n",
      "Image 4307 - Predicted: 8, Actual: 8\n",
      "Image 4308 - Predicted: 7, Actual: 9\n",
      "Image 4309 - Predicted: 2, Actual: 2\n",
      "Image 4310 - Predicted: 8, Actual: 4\n",
      "Image 4311 - Predicted: 7, Actual: 7\n",
      "Image 4312 - Predicted: 1, Actual: 1\n",
      "Image 4313 - Predicted: 6, Actual: 6\n",
      "Image 4314 - Predicted: 8, Actual: 8\n",
      "Image 4315 - Predicted: 4, Actual: 4\n",
      "Image 4316 - Predicted: 4, Actual: 4\n",
      "Image 4317 - Predicted: 9, Actual: 8\n",
      "Image 4318 - Predicted: 7, Actual: 7\n",
      "Image 4319 - Predicted: 4, Actual: 4\n",
      "Image 4320 - Predicted: 2, Actual: 2\n",
      "Image 4321 - Predicted: 1, Actual: 1\n",
      "Image 4322 - Predicted: 5, Actual: 5\n",
      "Image 4323 - Predicted: 2, Actual: 6\n",
      "Image 4324 - Predicted: 3, Actual: 3\n",
      "Image 4325 - Predicted: 5, Actual: 5\n",
      "Image 4326 - Predicted: 6, Actual: 6\n",
      "Image 4327 - Predicted: 1, Actual: 1\n",
      "Image 4328 - Predicted: 2, Actual: 6\n",
      "Image 4329 - Predicted: 8, Actual: 7\n",
      "Image 4330 - Predicted: 8, Actual: 7\n",
      "Image 4331 - Predicted: 0, Actual: 0\n",
      "Image 4332 - Predicted: 1, Actual: 1\n",
      "Image 4333 - Predicted: 2, Actual: 2\n",
      "Image 4334 - Predicted: 7, Actual: 8\n",
      "Image 4335 - Predicted: 6, Actual: 6\n",
      "Image 4336 - Predicted: 6, Actual: 6\n",
      "Image 4337 - Predicted: 0, Actual: 0\n",
      "Image 4338 - Predicted: 8, Actual: 8\n",
      "Image 4339 - Predicted: 6, Actual: 6\n",
      "Image 4340 - Predicted: 2, Actual: 2\n",
      "Image 4341 - Predicted: 3, Actual: 3\n",
      "Image 4342 - Predicted: 0, Actual: 0\n",
      "Image 4343 - Predicted: 3, Actual: 3\n",
      "Image 4344 - Predicted: 5, Actual: 5\n",
      "Image 4345 - Predicted: 2, Actual: 2\n",
      "Image 4346 - Predicted: 0, Actual: 0\n",
      "Image 4347 - Predicted: 3, Actual: 3\n",
      "Image 4348 - Predicted: 0, Actual: 0\n",
      "Image 4349 - Predicted: 7, Actual: 7\n",
      "Image 4350 - Predicted: 5, Actual: 5\n",
      "Image 4351 - Predicted: 2, Actual: 2\n",
      "Image 4352 - Predicted: 2, Actual: 2\n",
      "Image 4353 - Predicted: 5, Actual: 5\n",
      "Image 4354 - Predicted: 5, Actual: 4\n",
      "Image 4355 - Predicted: 7, Actual: 7\n",
      "Image 4356 - Predicted: 9, Actual: 9\n",
      "Image 4357 - Predicted: 8, Actual: 8\n",
      "Image 4358 - Predicted: 1, Actual: 1\n",
      "Image 4359 - Predicted: 3, Actual: 1\n",
      "Image 4360 - Predicted: 7, Actual: 7\n",
      "Image 4361 - Predicted: 9, Actual: 4\n",
      "Image 4362 - Predicted: 0, Actual: 0\n",
      "Image 4363 - Predicted: 1, Actual: 1\n",
      "Image 4364 - Predicted: 5, Actual: 5\n",
      "Image 4365 - Predicted: 9, Actual: 9\n",
      "Image 4366 - Predicted: 2, Actual: 2\n",
      "Image 4367 - Predicted: 5, Actual: 9\n",
      "Image 4368 - Predicted: 0, Actual: 0\n",
      "Image 4369 - Predicted: 1, Actual: 1\n",
      "Image 4370 - Predicted: 8, Actual: 8\n",
      "Image 4371 - Predicted: 7, Actual: 7\n",
      "Image 4372 - Predicted: 3, Actual: 3\n",
      "Image 4373 - Predicted: 7, Actual: 7\n",
      "Image 4374 - Predicted: 5, Actual: 5\n",
      "Image 4375 - Predicted: 3, Actual: 3\n",
      "Image 4376 - Predicted: 5, Actual: 5\n",
      "Image 4377 - Predicted: 3, Actual: 3\n",
      "Image 4378 - Predicted: 1, Actual: 1\n",
      "Image 4379 - Predicted: 5, Actual: 5\n",
      "Image 4380 - Predicted: 9, Actual: 9\n",
      "Image 4381 - Predicted: 6, Actual: 6\n",
      "Image 4382 - Predicted: 4, Actual: 4\n",
      "Image 4383 - Predicted: 3, Actual: 3\n",
      "Image 4384 - Predicted: 2, Actual: 2\n",
      "Image 4385 - Predicted: 4, Actual: 4\n",
      "Image 4386 - Predicted: 0, Actual: 0\n",
      "Image 4387 - Predicted: 0, Actual: 0\n",
      "Image 4388 - Predicted: 5, Actual: 5\n",
      "Image 4389 - Predicted: 0, Actual: 1\n",
      "Image 4390 - Predicted: 4, Actual: 4\n",
      "Image 4391 - Predicted: 1, Actual: 1\n",
      "Image 4392 - Predicted: 7, Actual: 7\n",
      "Image 4393 - Predicted: 4, Actual: 4\n",
      "Image 4394 - Predicted: 1, Actual: 0\n",
      "Image 4395 - Predicted: 3, Actual: 3\n",
      "Image 4396 - Predicted: 9, Actual: 9\n",
      "Image 4397 - Predicted: 2, Actual: 2\n",
      "Image 4398 - Predicted: 4, Actual: 4\n",
      "Image 4399 - Predicted: 0, Actual: 0\n",
      "Image 4400 - Predicted: 6, Actual: 6\n",
      "Image 4401 - Predicted: 2, Actual: 2\n",
      "Image 4402 - Predicted: 0, Actual: 0\n",
      "Image 4403 - Predicted: 9, Actual: 4\n",
      "Image 4404 - Predicted: 7, Actual: 2\n",
      "Image 4405 - Predicted: 2, Actual: 2\n",
      "Image 4406 - Predicted: 6, Actual: 6\n",
      "Image 4407 - Predicted: 1, Actual: 1\n",
      "Image 4408 - Predicted: 9, Actual: 5\n",
      "Image 4409 - Predicted: 1, Actual: 1\n",
      "Image 4410 - Predicted: 0, Actual: 0\n",
      "Image 4411 - Predicted: 2, Actual: 2\n",
      "Image 4412 - Predicted: 2, Actual: 2\n",
      "Image 4413 - Predicted: 9, Actual: 9\n",
      "Image 4414 - Predicted: 0, Actual: 0\n",
      "Image 4415 - Predicted: 6, Actual: 6\n",
      "Image 4416 - Predicted: 0, Actual: 0\n",
      "Image 4417 - Predicted: 2, Actual: 2\n",
      "Image 4418 - Predicted: 3, Actual: 3\n",
      "Image 4419 - Predicted: 2, Actual: 2\n",
      "Image 4420 - Predicted: 2, Actual: 2\n",
      "Image 4421 - Predicted: 9, Actual: 8\n",
      "Image 4422 - Predicted: 3, Actual: 3\n",
      "Image 4423 - Predicted: 6, Actual: 6\n",
      "Image 4424 - Predicted: 0, Actual: 1\n",
      "Image 4425 - Predicted: 4, Actual: 4\n",
      "Image 4426 - Predicted: 6, Actual: 6\n",
      "Image 4427 - Predicted: 5, Actual: 5\n",
      "Image 4428 - Predicted: 5, Actual: 5\n",
      "Image 4429 - Predicted: 9, Actual: 8\n",
      "Image 4430 - Predicted: 9, Actual: 7\n",
      "Image 4431 - Predicted: 0, Actual: 0\n",
      "Image 4432 - Predicted: 7, Actual: 7\n",
      "Image 4433 - Predicted: 1, Actual: 1\n",
      "Image 4434 - Predicted: 8, Actual: 5\n",
      "Image 4435 - Predicted: 6, Actual: 6\n",
      "Image 4436 - Predicted: 9, Actual: 9\n",
      "Image 4437 - Predicted: 3, Actual: 3\n",
      "Image 4438 - Predicted: 3, Actual: 3\n",
      "Image 4439 - Predicted: 3, Actual: 3\n",
      "Image 4440 - Predicted: 5, Actual: 5\n",
      "Image 4441 - Predicted: 7, Actual: 7\n",
      "Image 4442 - Predicted: 9, Actual: 9\n",
      "Image 4443 - Predicted: 4, Actual: 4\n",
      "Image 4444 - Predicted: 9, Actual: 5\n",
      "Image 4445 - Predicted: 0, Actual: 0\n",
      "Image 4446 - Predicted: 6, Actual: 6\n",
      "Image 4447 - Predicted: 2, Actual: 2\n",
      "Image 4448 - Predicted: 6, Actual: 6\n",
      "Image 4449 - Predicted: 3, Actual: 3\n",
      "Image 4450 - Predicted: 1, Actual: 1\n",
      "Image 4451 - Predicted: 6, Actual: 6\n",
      "Image 4452 - Predicted: 2, Actual: 2\n",
      "Image 4453 - Predicted: 1, Actual: 1\n",
      "Image 4454 - Predicted: 5, Actual: 8\n",
      "Image 4455 - Predicted: 2, Actual: 2\n",
      "Image 4456 - Predicted: 8, Actual: 4\n",
      "Image 4457 - Predicted: 1, Actual: 1\n",
      "Image 4458 - Predicted: 6, Actual: 2\n",
      "Image 4459 - Predicted: 4, Actual: 5\n",
      "Image 4460 - Predicted: 1, Actual: 1\n",
      "Image 4461 - Predicted: 8, Actual: 8\n",
      "Image 4462 - Predicted: 9, Actual: 8\n",
      "Image 4463 - Predicted: 6, Actual: 2\n",
      "Image 4464 - Predicted: 4, Actual: 4\n",
      "Image 4465 - Predicted: 7, Actual: 7\n",
      "Image 4466 - Predicted: 4, Actual: 5\n",
      "Image 4467 - Predicted: 6, Actual: 6\n",
      "Image 4468 - Predicted: 7, Actual: 7\n",
      "Image 4469 - Predicted: 5, Actual: 9\n",
      "Image 4470 - Predicted: 1, Actual: 1\n",
      "Image 4471 - Predicted: 6, Actual: 6\n",
      "Image 4472 - Predicted: 4, Actual: 5\n",
      "Image 4473 - Predicted: 2, Actual: 2\n",
      "Image 4474 - Predicted: 5, Actual: 5\n",
      "Image 4475 - Predicted: 6, Actual: 6\n",
      "Image 4476 - Predicted: 5, Actual: 5\n",
      "Image 4477 - Predicted: 0, Actual: 0\n",
      "Image 4478 - Predicted: 5, Actual: 5\n",
      "Image 4479 - Predicted: 3, Actual: 3\n",
      "Image 4480 - Predicted: 6, Actual: 6\n",
      "Image 4481 - Predicted: 5, Actual: 5\n",
      "Image 4482 - Predicted: 4, Actual: 4\n",
      "Image 4483 - Predicted: 9, Actual: 9\n",
      "Image 4484 - Predicted: 3, Actual: 3\n",
      "Image 4485 - Predicted: 9, Actual: 7\n",
      "Image 4486 - Predicted: 9, Actual: 4\n",
      "Image 4487 - Predicted: 1, Actual: 1\n",
      "Image 4488 - Predicted: 3, Actual: 3\n",
      "Image 4489 - Predicted: 1, Actual: 1\n",
      "Image 4490 - Predicted: 8, Actual: 8\n",
      "Image 4491 - Predicted: 6, Actual: 6\n",
      "Image 4492 - Predicted: 2, Actual: 2\n",
      "Image 4493 - Predicted: 0, Actual: 0\n",
      "Image 4494 - Predicted: 0, Actual: 0\n",
      "Image 4495 - Predicted: 6, Actual: 6\n",
      "Image 4496 - Predicted: 4, Actual: 4\n",
      "Image 4497 - Predicted: 6, Actual: 6\n",
      "Image 4498 - Predicted: 3, Actual: 3\n",
      "Image 4499 - Predicted: 2, Actual: 2\n",
      "Image 4500 - Predicted: 7, Actual: 7\n",
      "Image 4501 - Predicted: 9, Actual: 9\n",
      "Image 4502 - Predicted: 7, Actual: 7\n",
      "Image 4503 - Predicted: 7, Actual: 7\n",
      "Image 4504 - Predicted: 5, Actual: 4\n",
      "Image 4505 - Predicted: 6, Actual: 6\n",
      "Image 4506 - Predicted: 9, Actual: 9\n",
      "Image 4507 - Predicted: 2, Actual: 2\n",
      "Image 4508 - Predicted: 2, Actual: 2\n",
      "Image 4509 - Predicted: 9, Actual: 9\n",
      "Image 4510 - Predicted: 2, Actual: 2\n",
      "Image 4511 - Predicted: 7, Actual: 7\n",
      "Image 4512 - Predicted: 5, Actual: 5\n",
      "Image 4513 - Predicted: 2, Actual: 2\n",
      "Image 4514 - Predicted: 2, Actual: 6\n",
      "Image 4515 - Predicted: 3, Actual: 3\n",
      "Image 4516 - Predicted: 8, Actual: 4\n",
      "Image 4517 - Predicted: 5, Actual: 5\n",
      "Image 4518 - Predicted: 7, Actual: 2\n",
      "Image 4519 - Predicted: 9, Actual: 9\n",
      "Image 4520 - Predicted: 1, Actual: 1\n",
      "Image 4521 - Predicted: 4, Actual: 4\n",
      "Image 4522 - Predicted: 5, Actual: 5\n",
      "Image 4523 - Predicted: 2, Actual: 2\n",
      "Image 4524 - Predicted: 3, Actual: 3\n",
      "Image 4525 - Predicted: 1, Actual: 1\n",
      "Image 4526 - Predicted: 6, Actual: 6\n",
      "Image 4527 - Predicted: 1, Actual: 1\n",
      "Image 4528 - Predicted: 0, Actual: 0\n",
      "Image 4529 - Predicted: 4, Actual: 4\n",
      "Image 4530 - Predicted: 0, Actual: 0\n",
      "Image 4531 - Predicted: 4, Actual: 4\n",
      "Image 4532 - Predicted: 0, Actual: 0\n",
      "Image 4533 - Predicted: 2, Actual: 6\n",
      "Image 4534 - Predicted: 8, Actual: 8\n",
      "Image 4535 - Predicted: 1, Actual: 1\n",
      "Image 4536 - Predicted: 7, Actual: 7\n",
      "Image 4537 - Predicted: 3, Actual: 3\n",
      "Image 4538 - Predicted: 1, Actual: 1\n",
      "Image 4539 - Predicted: 8, Actual: 8\n",
      "Image 4540 - Predicted: 9, Actual: 7\n",
      "Image 4541 - Predicted: 7, Actual: 7\n",
      "Image 4542 - Predicted: 0, Actual: 1\n",
      "Image 4543 - Predicted: 7, Actual: 7\n",
      "Image 4544 - Predicted: 4, Actual: 5\n",
      "Image 4545 - Predicted: 5, Actual: 5\n",
      "Image 4546 - Predicted: 0, Actual: 0\n",
      "Image 4547 - Predicted: 1, Actual: 1\n",
      "Image 4548 - Predicted: 4, Actual: 8\n",
      "Image 4549 - Predicted: 2, Actual: 2\n",
      "Image 4550 - Predicted: 2, Actual: 2\n",
      "Image 4551 - Predicted: 6, Actual: 6\n",
      "Image 4552 - Predicted: 6, Actual: 6\n",
      "Image 4553 - Predicted: 3, Actual: 3\n",
      "Image 4554 - Predicted: 4, Actual: 4\n",
      "Image 4555 - Predicted: 0, Actual: 0\n",
      "Image 4556 - Predicted: 2, Actual: 2\n",
      "Image 4557 - Predicted: 1, Actual: 1\n",
      "Image 4558 - Predicted: 3, Actual: 3\n",
      "Image 4559 - Predicted: 3, Actual: 3\n",
      "Image 4560 - Predicted: 5, Actual: 5\n",
      "Image 4561 - Predicted: 1, Actual: 1\n",
      "Image 4562 - Predicted: 9, Actual: 9\n",
      "Image 4563 - Predicted: 6, Actual: 6\n",
      "Image 4564 - Predicted: 0, Actual: 0\n",
      "Image 4565 - Predicted: 1, Actual: 1\n",
      "Image 4566 - Predicted: 6, Actual: 6\n",
      "Image 4567 - Predicted: 7, Actual: 7\n",
      "Image 4568 - Predicted: 3, Actual: 3\n",
      "Image 4569 - Predicted: 9, Actual: 9\n",
      "Image 4570 - Predicted: 5, Actual: 5\n",
      "Image 4571 - Predicted: 5, Actual: 5\n",
      "Image 4572 - Predicted: 2, Actual: 2\n",
      "Image 4573 - Predicted: 0, Actual: 1\n",
      "Image 4574 - Predicted: 6, Actual: 6\n",
      "Image 4575 - Predicted: 4, Actual: 4\n",
      "Image 4576 - Predicted: 1, Actual: 1\n",
      "Image 4577 - Predicted: 6, Actual: 6\n",
      "Image 4578 - Predicted: 4, Actual: 5\n",
      "Image 4579 - Predicted: 6, Actual: 2\n",
      "Image 4580 - Predicted: 1, Actual: 1\n",
      "Image 4581 - Predicted: 4, Actual: 5\n",
      "Image 4582 - Predicted: 1, Actual: 0\n",
      "Image 4583 - Predicted: 6, Actual: 6\n",
      "Image 4584 - Predicted: 8, Actual: 8\n",
      "Image 4585 - Predicted: 2, Actual: 2\n",
      "Image 4586 - Predicted: 9, Actual: 4\n",
      "Image 4587 - Predicted: 6, Actual: 6\n",
      "Image 4588 - Predicted: 8, Actual: 7\n",
      "Image 4589 - Predicted: 3, Actual: 3\n",
      "Image 4590 - Predicted: 6, Actual: 6\n",
      "Image 4591 - Predicted: 4, Actual: 4\n",
      "Image 4592 - Predicted: 1, Actual: 1\n",
      "Image 4593 - Predicted: 5, Actual: 4\n",
      "Image 4594 - Predicted: 9, Actual: 9\n",
      "Image 4595 - Predicted: 8, Actual: 8\n",
      "Image 4596 - Predicted: 2, Actual: 2\n",
      "Image 4597 - Predicted: 2, Actual: 2\n",
      "Image 4598 - Predicted: 3, Actual: 7\n",
      "Image 4599 - Predicted: 2, Actual: 2\n",
      "Image 4600 - Predicted: 7, Actual: 0\n",
      "Image 4601 - Predicted: 9, Actual: 9\n",
      "Image 4602 - Predicted: 5, Actual: 8\n",
      "Image 4603 - Predicted: 0, Actual: 0\n",
      "Image 4604 - Predicted: 1, Actual: 1\n",
      "Image 4605 - Predicted: 7, Actual: 7\n",
      "Image 4606 - Predicted: 1, Actual: 1\n",
      "Image 4607 - Predicted: 5, Actual: 5\n",
      "Image 4608 - Predicted: 4, Actual: 8\n",
      "Image 4609 - Predicted: 3, Actual: 3\n",
      "Image 4610 - Predicted: 7, Actual: 7\n",
      "Image 4611 - Predicted: 0, Actual: 0\n",
      "Image 4612 - Predicted: 6, Actual: 6\n",
      "Image 4613 - Predicted: 7, Actual: 7\n",
      "Image 4614 - Predicted: 0, Actual: 0\n",
      "Image 4615 - Predicted: 6, Actual: 6\n",
      "Image 4616 - Predicted: 9, Actual: 9\n",
      "Image 4617 - Predicted: 0, Actual: 0\n",
      "Image 4618 - Predicted: 7, Actual: 8\n",
      "Image 4619 - Predicted: 2, Actual: 2\n",
      "Image 4620 - Predicted: 1, Actual: 1\n",
      "Image 4621 - Predicted: 2, Actual: 2\n",
      "Image 4622 - Predicted: 7, Actual: 7\n",
      "Image 4623 - Predicted: 1, Actual: 1\n",
      "Image 4624 - Predicted: 8, Actual: 8\n",
      "Image 4625 - Predicted: 2, Actual: 2\n",
      "Image 4626 - Predicted: 6, Actual: 6\n",
      "Image 4627 - Predicted: 1, Actual: 1\n",
      "Image 4628 - Predicted: 3, Actual: 3\n",
      "Image 4629 - Predicted: 9, Actual: 4\n",
      "Image 4630 - Predicted: 0, Actual: 0\n",
      "Image 4631 - Predicted: 3, Actual: 3\n",
      "Image 4632 - Predicted: 7, Actual: 7\n",
      "Image 4633 - Predicted: 2, Actual: 1\n",
      "Image 4634 - Predicted: 7, Actual: 7\n",
      "Image 4635 - Predicted: 6, Actual: 6\n",
      "Image 4636 - Predicted: 7, Actual: 7\n",
      "Image 4637 - Predicted: 7, Actual: 7\n",
      "Image 4638 - Predicted: 0, Actual: 0\n",
      "Image 4639 - Predicted: 4, Actual: 4\n",
      "Image 4640 - Predicted: 5, Actual: 5\n",
      "Image 4641 - Predicted: 0, Actual: 0\n",
      "Image 4642 - Predicted: 7, Actual: 7\n",
      "Image 4643 - Predicted: 9, Actual: 8\n",
      "Image 4644 - Predicted: 5, Actual: 5\n",
      "Image 4645 - Predicted: 1, Actual: 1\n",
      "Image 4646 - Predicted: 3, Actual: 3\n",
      "Image 4647 - Predicted: 6, Actual: 2\n",
      "Image 4648 - Predicted: 6, Actual: 6\n",
      "Image 4649 - Predicted: 3, Actual: 3\n",
      "Image 4650 - Predicted: 7, Actual: 7\n",
      "Image 4651 - Predicted: 2, Actual: 2\n",
      "Image 4652 - Predicted: 9, Actual: 9\n",
      "Image 4653 - Predicted: 8, Actual: 8\n",
      "Image 4654 - Predicted: 6, Actual: 6\n",
      "Image 4655 - Predicted: 2, Actual: 2\n",
      "Image 4656 - Predicted: 8, Actual: 8\n",
      "Image 4657 - Predicted: 5, Actual: 5\n",
      "Image 4658 - Predicted: 5, Actual: 5\n",
      "Image 4659 - Predicted: 0, Actual: 0\n",
      "Image 4660 - Predicted: 4, Actual: 4\n",
      "Image 4661 - Predicted: 5, Actual: 5\n",
      "Image 4662 - Predicted: 9, Actual: 4\n",
      "Image 4663 - Predicted: 2, Actual: 2\n",
      "Image 4664 - Predicted: 7, Actual: 7\n",
      "Image 4665 - Predicted: 0, Actual: 0\n",
      "Image 4666 - Predicted: 9, Actual: 9\n",
      "Image 4667 - Predicted: 4, Actual: 4\n",
      "Image 4668 - Predicted: 2, Actual: 2\n",
      "Image 4669 - Predicted: 9, Actual: 9\n",
      "Image 4670 - Predicted: 5, Actual: 5\n",
      "Image 4671 - Predicted: 6, Actual: 2\n",
      "Image 4672 - Predicted: 8, Actual: 7\n",
      "Image 4673 - Predicted: 3, Actual: 3\n",
      "Image 4674 - Predicted: 7, Actual: 9\n",
      "Image 4675 - Predicted: 3, Actual: 3\n",
      "Image 4676 - Predicted: 9, Actual: 7\n",
      "Image 4677 - Predicted: 1, Actual: 1\n",
      "Image 4678 - Predicted: 4, Actual: 8\n",
      "Image 4679 - Predicted: 2, Actual: 2\n",
      "Image 4680 - Predicted: 4, Actual: 5\n",
      "Image 4681 - Predicted: 4, Actual: 4\n",
      "Image 4682 - Predicted: 9, Actual: 7\n",
      "Image 4683 - Predicted: 9, Actual: 9\n",
      "Image 4684 - Predicted: 8, Actual: 9\n",
      "Image 4685 - Predicted: 2, Actual: 2\n",
      "Image 4686 - Predicted: 1, Actual: 1\n",
      "Image 4687 - Predicted: 4, Actual: 4\n",
      "Image 4688 - Predicted: 4, Actual: 4\n",
      "Image 4689 - Predicted: 6, Actual: 6\n",
      "Image 4690 - Predicted: 2, Actual: 2\n",
      "Image 4691 - Predicted: 8, Actual: 8\n",
      "Image 4692 - Predicted: 9, Actual: 9\n",
      "Image 4693 - Predicted: 0, Actual: 0\n",
      "Image 4694 - Predicted: 7, Actual: 6\n",
      "Image 4695 - Predicted: 1, Actual: 1\n",
      "Image 4696 - Predicted: 4, Actual: 4\n",
      "Image 4697 - Predicted: 6, Actual: 6\n",
      "Image 4698 - Predicted: 2, Actual: 2\n",
      "Image 4699 - Predicted: 5, Actual: 5\n",
      "Image 4700 - Predicted: 0, Actual: 0\n",
      "Image 4701 - Predicted: 6, Actual: 2\n",
      "Image 4702 - Predicted: 1, Actual: 1\n",
      "Image 4703 - Predicted: 3, Actual: 3\n",
      "Image 4704 - Predicted: 4, Actual: 4\n",
      "Image 4705 - Predicted: 8, Actual: 8\n",
      "Image 4706 - Predicted: 2, Actual: 2\n",
      "Image 4707 - Predicted: 9, Actual: 9\n",
      "Image 4708 - Predicted: 5, Actual: 5\n",
      "Image 4709 - Predicted: 1, Actual: 1\n",
      "Image 4710 - Predicted: 7, Actual: 7\n",
      "Image 4711 - Predicted: 5, Actual: 5\n",
      "Image 4712 - Predicted: 6, Actual: 2\n",
      "Image 4713 - Predicted: 8, Actual: 8\n",
      "Image 4714 - Predicted: 3, Actual: 3\n",
      "Image 4715 - Predicted: 8, Actual: 4\n",
      "Image 4716 - Predicted: 6, Actual: 6\n",
      "Image 4717 - Predicted: 4, Actual: 4\n",
      "Image 4718 - Predicted: 3, Actual: 3\n",
      "Image 4719 - Predicted: 1, Actual: 1\n",
      "Image 4720 - Predicted: 5, Actual: 5\n",
      "Image 4721 - Predicted: 9, Actual: 9\n",
      "Image 4722 - Predicted: 1, Actual: 1\n",
      "Image 4723 - Predicted: 7, Actual: 7\n",
      "Image 4724 - Predicted: 2, Actual: 2\n",
      "Image 4725 - Predicted: 3, Actual: 3\n",
      "Image 4726 - Predicted: 9, Actual: 4\n",
      "Image 4727 - Predicted: 3, Actual: 3\n",
      "Image 4728 - Predicted: 3, Actual: 3\n",
      "Image 4729 - Predicted: 0, Actual: 0\n",
      "Image 4730 - Predicted: 7, Actual: 7\n",
      "Image 4731 - Predicted: 4, Actual: 4\n",
      "Image 4732 - Predicted: 3, Actual: 3\n",
      "Image 4733 - Predicted: 2, Actual: 2\n",
      "Image 4734 - Predicted: 7, Actual: 7\n",
      "Image 4735 - Predicted: 3, Actual: 3\n",
      "Image 4736 - Predicted: 3, Actual: 3\n",
      "Image 4737 - Predicted: 9, Actual: 9\n",
      "Image 4738 - Predicted: 1, Actual: 1\n",
      "Image 4739 - Predicted: 3, Actual: 3\n",
      "Image 4740 - Predicted: 7, Actual: 7\n",
      "Image 4741 - Predicted: 1, Actual: 1\n",
      "Image 4742 - Predicted: 0, Actual: 0\n",
      "Image 4743 - Predicted: 6, Actual: 6\n",
      "Image 4744 - Predicted: 5, Actual: 5\n",
      "Image 4745 - Predicted: 0, Actual: 0\n",
      "Image 4746 - Predicted: 9, Actual: 9\n",
      "Image 4747 - Predicted: 2, Actual: 2\n",
      "Image 4748 - Predicted: 3, Actual: 9\n",
      "Image 4749 - Predicted: 4, Actual: 4\n",
      "Image 4750 - Predicted: 8, Actual: 4\n",
      "Image 4751 - Predicted: 7, Actual: 7\n",
      "Image 4752 - Predicted: 7, Actual: 7\n",
      "Image 4753 - Predicted: 3, Actual: 3\n",
      "Image 4754 - Predicted: 6, Actual: 6\n",
      "Image 4755 - Predicted: 2, Actual: 2\n",
      "Image 4756 - Predicted: 4, Actual: 4\n",
      "Image 4757 - Predicted: 6, Actual: 6\n",
      "Image 4758 - Predicted: 6, Actual: 6\n",
      "Image 4759 - Predicted: 2, Actual: 2\n",
      "Image 4760 - Predicted: 2, Actual: 2\n",
      "Image 4761 - Predicted: 4, Actual: 4\n",
      "Image 4762 - Predicted: 0, Actual: 0\n",
      "Image 4763 - Predicted: 7, Actual: 7\n",
      "Image 4764 - Predicted: 2, Actual: 2\n",
      "Image 4765 - Predicted: 9, Actual: 4\n",
      "Image 4766 - Predicted: 8, Actual: 4\n",
      "Image 4767 - Predicted: 4, Actual: 5\n",
      "Image 4768 - Predicted: 1, Actual: 1\n",
      "Image 4769 - Predicted: 6, Actual: 6\n",
      "Image 4770 - Predicted: 1, Actual: 1\n",
      "Image 4771 - Predicted: 4, Actual: 4\n",
      "Image 4772 - Predicted: 4, Actual: 8\n",
      "Image 4773 - Predicted: 7, Actual: 7\n",
      "Image 4774 - Predicted: 1, Actual: 1\n",
      "Image 4775 - Predicted: 7, Actual: 7\n",
      "Image 4776 - Predicted: 1, Actual: 1\n",
      "Image 4777 - Predicted: 0, Actual: 0\n",
      "Image 4778 - Predicted: 3, Actual: 3\n",
      "Image 4779 - Predicted: 6, Actual: 6\n",
      "Image 4780 - Predicted: 4, Actual: 4\n",
      "Image 4781 - Predicted: 6, Actual: 6\n",
      "Image 4782 - Predicted: 6, Actual: 6\n",
      "Image 4783 - Predicted: 1, Actual: 1\n",
      "Image 4784 - Predicted: 3, Actual: 3\n",
      "Image 4785 - Predicted: 6, Actual: 6\n",
      "Image 4786 - Predicted: 9, Actual: 7\n",
      "Image 4787 - Predicted: 2, Actual: 2\n",
      "Image 4788 - Predicted: 8, Actual: 8\n",
      "Image 4789 - Predicted: 2, Actual: 2\n",
      "Image 4790 - Predicted: 8, Actual: 9\n",
      "Image 4791 - Predicted: 3, Actual: 3\n",
      "Image 4792 - Predicted: 9, Actual: 7\n",
      "Image 4793 - Predicted: 9, Actual: 7\n",
      "Image 4794 - Predicted: 8, Actual: 8\n",
      "Image 4795 - Predicted: 4, Actual: 4\n",
      "Image 4796 - Predicted: 6, Actual: 6\n",
      "Image 4797 - Predicted: 9, Actual: 9\n",
      "Image 4798 - Predicted: 6, Actual: 6\n",
      "Image 4799 - Predicted: 8, Actual: 8\n",
      "Image 4800 - Predicted: 3, Actual: 3\n",
      "Image 4801 - Predicted: 9, Actual: 9\n",
      "Image 4802 - Predicted: 0, Actual: 1\n",
      "Image 4803 - Predicted: 7, Actual: 8\n",
      "Image 4804 - Predicted: 5, Actual: 5\n",
      "Image 4805 - Predicted: 0, Actual: 0\n",
      "Image 4806 - Predicted: 1, Actual: 1\n",
      "Image 4807 - Predicted: 8, Actual: 8\n",
      "Image 4808 - Predicted: 0, Actual: 0\n",
      "Image 4809 - Predicted: 1, Actual: 0\n",
      "Image 4810 - Predicted: 3, Actual: 3\n",
      "Image 4811 - Predicted: 9, Actual: 7\n",
      "Image 4812 - Predicted: 2, Actual: 2\n",
      "Image 4813 - Predicted: 0, Actual: 0\n",
      "Image 4814 - Predicted: 8, Actual: 9\n",
      "Image 4815 - Predicted: 5, Actual: 5\n",
      "Image 4816 - Predicted: 8, Actual: 8\n",
      "Image 4817 - Predicted: 9, Actual: 7\n",
      "Image 4818 - Predicted: 8, Actual: 4\n",
      "Image 4819 - Predicted: 3, Actual: 3\n",
      "Image 4820 - Predicted: 6, Actual: 6\n",
      "Image 4821 - Predicted: 3, Actual: 3\n",
      "Image 4822 - Predicted: 0, Actual: 0\n",
      "Image 4823 - Predicted: 2, Actual: 2\n",
      "Image 4824 - Predicted: 5, Actual: 5\n",
      "Image 4825 - Predicted: 8, Actual: 4\n",
      "Image 4826 - Predicted: 9, Actual: 9\n",
      "Image 4827 - Predicted: 2, Actual: 2\n",
      "Image 4828 - Predicted: 8, Actual: 8\n",
      "Image 4829 - Predicted: 5, Actual: 5\n",
      "Image 4830 - Predicted: 0, Actual: 1\n",
      "Image 4831 - Predicted: 6, Actual: 6\n",
      "Image 4832 - Predicted: 6, Actual: 4\n",
      "Image 4833 - Predicted: 0, Actual: 0\n",
      "Image 4834 - Predicted: 6, Actual: 6\n",
      "Image 4835 - Predicted: 6, Actual: 6\n",
      "Image 4836 - Predicted: 9, Actual: 9\n",
      "Image 4837 - Predicted: 8, Actual: 8\n",
      "Image 4838 - Predicted: 9, Actual: 9\n",
      "Image 4839 - Predicted: 9, Actual: 9\n",
      "Image 4840 - Predicted: 1, Actual: 1\n",
      "Image 4841 - Predicted: 9, Actual: 9\n",
      "Image 4842 - Predicted: 6, Actual: 6\n",
      "Image 4843 - Predicted: 1, Actual: 1\n",
      "Image 4844 - Predicted: 3, Actual: 3\n",
      "Image 4845 - Predicted: 3, Actual: 3\n",
      "Image 4846 - Predicted: 2, Actual: 2\n",
      "Image 4847 - Predicted: 4, Actual: 4\n",
      "Image 4848 - Predicted: 0, Actual: 0\n",
      "Image 4849 - Predicted: 4, Actual: 4\n",
      "Image 4850 - Predicted: 7, Actual: 7\n",
      "Image 4851 - Predicted: 3, Actual: 2\n",
      "Image 4852 - Predicted: 6, Actual: 6\n",
      "Image 4853 - Predicted: 8, Actual: 8\n",
      "Image 4854 - Predicted: 7, Actual: 4\n",
      "Image 4855 - Predicted: 1, Actual: 1\n",
      "Image 4856 - Predicted: 6, Actual: 6\n",
      "Image 4857 - Predicted: 9, Actual: 9\n",
      "Image 4858 - Predicted: 6, Actual: 6\n",
      "Image 4859 - Predicted: 0, Actual: 0\n",
      "Image 4860 - Predicted: 8, Actual: 8\n",
      "Image 4861 - Predicted: 6, Actual: 6\n",
      "Image 4862 - Predicted: 7, Actual: 7\n",
      "Image 4863 - Predicted: 5, Actual: 5\n",
      "Image 4864 - Predicted: 6, Actual: 2\n",
      "Image 4865 - Predicted: 7, Actual: 7\n",
      "Image 4866 - Predicted: 0, Actual: 0\n",
      "Image 4867 - Predicted: 9, Actual: 9\n",
      "Image 4868 - Predicted: 4, Actual: 9\n",
      "Image 4869 - Predicted: 0, Actual: 1\n",
      "Image 4870 - Predicted: 8, Actual: 4\n",
      "Image 4871 - Predicted: 8, Actual: 8\n",
      "Image 4872 - Predicted: 6, Actual: 6\n",
      "Image 4873 - Predicted: 2, Actual: 2\n",
      "Image 4874 - Predicted: 3, Actual: 3\n",
      "Image 4875 - Predicted: 7, Actual: 7\n",
      "Image 4876 - Predicted: 8, Actual: 8\n",
      "Image 4877 - Predicted: 6, Actual: 2\n",
      "Image 4878 - Predicted: 1, Actual: 3\n",
      "Image 4879 - Predicted: 1, Actual: 1\n",
      "Accuracy: 0.8217213114754098\n"
     ]
    }
   ],
   "source": [
    "current_datasets = train_datasets.copy()\n",
    "current_x_test = x_train.copy()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(current_datasets)\n",
    "\n",
    "for i, row in current_datasets.iterrows():\n",
    "    result = NN.forward(current_x_test[i].reshape(784, 1).T)\n",
    "    # print(result)\n",
    "    predicted_label = result.argmax()  # Get the index of the max value (predicted class)\n",
    "    actual_label = row['label']\n",
    "    \n",
    "    print(f\"Image {i} - Predicted: {predicted_label}, Actual: {actual_label}\")\n",
    "    \n",
    "    if predicted_label == actual_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open(\"model.pk\", \"wb\") as f:\n",
    "    pickle.dump(NN, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a model loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.52111060e-06, 7.39898703e-06, 4.88526727e-04, 1.16688070e-03,\n",
       "        2.33216792e-02, 1.19896471e-02, 7.02679826e-06, 2.57336973e-01,\n",
       "        8.13942365e-02, 6.24281110e-01]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pickle.load(open(\"model.pk\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
